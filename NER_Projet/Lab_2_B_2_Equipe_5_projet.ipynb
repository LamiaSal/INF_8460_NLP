{"nbformat":4,"nbformat_minor":5,"metadata":{"accelerator":"GPU","colab":{"name":"BERT SOUMISSION.ipynb","provenance":[],"collapsed_sections":["SOEYbh2y8w0B","Y4kpbbarJ2M8","znXh_IlLoxVm","3eHhFVFW9KVE","P91MWhUos35e","CwyvsxLIKroi","l-Eiis34o4-h","uLfmtsFD848-","suWHQgQetICM","fBMx7U-oJ5_A","p9JN2A0MBnO9","1fIhXc9kCFHg","2avuUzlpCKYZ","Wx5xWdEHCSEr","0aFM8_0SDU9E","vud2L5uMfrS1","ibfNeGD7CkXf","7mSobTNUsnq2","JFWQNhELtHQu","JuI-p8_xtW4o","yJ1ne16LyRXm","h7LwyydixzWT","K5ktDqWdWdhv","3PXTzQNlcvIy","Ee9_zHgqF8a4","oBU1j2vBbFMw","Z5sJPq9S3Spk","PgWWT3QlI8qv","eSrGXfXnf-qk","itX9_FIRf3u1","5wIOsqXjGHwB"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"widgets":{"application/vnd.jupyter.widget-state+json":{"24dee2ed298e466b8b9cdc1afe66b729":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_dc4ba2d02a344bb29427e44a81dd3485","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_add506e0d3a94f1b9244f78fee3098b3","IPY_MODEL_0f36735185f74b92bd5f093fdfc2fa84","IPY_MODEL_c357de13e50a473e942884cab6393f37"]}},"dc4ba2d02a344bb29427e44a81dd3485":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"add506e0d3a94f1b9244f78fee3098b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_800640c0e2e945be9eb7bf64c751079c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b908e8268d194f7cba5e3ad848301fa0"}},"0f36735185f74b92bd5f093fdfc2fa84":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_dab07a1b62e84ebaae993735b5ab4516","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6a6da3cd54314b0a8dde1200fb2ba6a4"}},"c357de13e50a473e942884cab6393f37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bf16cec6e7de49dda8d258405b7eae02","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 420M/420M [00:16&lt;00:00, 31.7MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_63c546fa1d2b48dbb3f73ef19348d80e"}},"800640c0e2e945be9eb7bf64c751079c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b908e8268d194f7cba5e3ad848301fa0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dab07a1b62e84ebaae993735b5ab4516":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6a6da3cd54314b0a8dde1200fb2ba6a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bf16cec6e7de49dda8d258405b7eae02":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"63c546fa1d2b48dbb3f73ef19348d80e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"59b3b425-cbcd-4ffc-ba91-b959ec81d563"},"source":["<center> École Polytechnique de Montréal <br> Département Génie Informatique et Génie Logiciel <br>  INF8460 – Traitement automatique de la langue naturelle <br> </center>\n","<center> TP3 INF8460 <br>  Automne 2021 </center>"],"id":"59b3b425-cbcd-4ffc-ba91-b959ec81d563"},{"cell_type":"code","metadata":{"id":"b15be814-e347-4fcd-bf4e-51cec64317c7","executionInfo":{"status":"ok","timestamp":1638765858720,"user_tz":300,"elapsed":239,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}}},"source":["# PATH = \"drive/MyDrive/TPS/TP3\"\n","PATH = \"drive/MyDrive/POLY/INF8460/TPS/TP3\"\n","# PATH = \"drive/MyDrive/POLY/Traitement Auto de la Langue Naturelle/TPS/TP3\"\n"],"id":"b15be814-e347-4fcd-bf4e-51cec64317c7","execution_count":87,"outputs":[]},{"cell_type":"code","metadata":{"id":"sxQYbXlI7net","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"error","timestamp":1638765860380,"user_tz":300,"elapsed":236,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"6fe6481e-63ab-400f-ad8c-eb7eeedf27a2"},"source":["import os\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","os.chdir(PATH)"],"id":"sxQYbXlI7net","execution_count":88,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-88-1dee07b8ed35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/MyDrive/POLY/INF8460/TP/TP3'"]}]},{"cell_type":"markdown","metadata":{"id":"bXOdzZAX7hCk"},"source":["# Importations des différentes librairies"],"id":"bXOdzZAX7hCk"},{"cell_type":"code","metadata":{"id":"hOfDToCaOeHs","executionInfo":{"status":"ok","timestamp":1638765865983,"user_tz":300,"elapsed":176,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}}},"source":["from nltk.tag.util import untag\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import nltk\n","import re\n","from nltk import word_tokenize\n","\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import nltk\n","import re\n","from nltk import word_tokenize\n","from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","import matplotlib.pyplot as plt\n","from keras.models import Sequential\n","from keras.layers import Input,Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n","from tensorflow.keras.optimizers import Adam, SGD,RMSprop,Adagrad,Adadelta\n","from keras.models import load_model\n","#from tensorflow.keras.models import create_model\n","\n","\n","from sklearn.metrics import confusion_matrix, plot_confusion_matrix, recall_score, precision_score, recall_score\n","from itertools import chain\n","\n","import pandas as pd\n","\n","import nltk\n","import sklearn\n","import scipy.stats\n","from sklearn.metrics import make_scorer\n","\n","\n","from sklearn.model_selection import RandomizedSearchCV,cross_val_score\n","from sklearn.metrics import f1_score\n","\n","import tensorflow as tf\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import layers\n","from keras.models import Sequential, load_model,Model\n","from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Input, GRU, Activation, Masking\n","\n","from keras.layers import Dense, Input, GRU, Embedding, Dropout, Activation, Masking\n","from keras.layers import BatchNormalization\n","\n","#defining the checkpoint\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","from keras import backend as K\n","\n","from sklearn.metrics import f1_score\n","\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n","from tensorflow.keras.optimizers import Adam, SGD\n","\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import classification_report\n"],"id":"hOfDToCaOeHs","execution_count":89,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tNz6qN-r3Zb9","executionInfo":{"status":"ok","timestamp":1638765868646,"user_tz":300,"elapsed":162,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"e6988583-4515-417b-8dc5-3a263f55e123"},"source":["nltk.download(\"punkt\")"],"id":"tNz6qN-r3Zb9","execution_count":90,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":90}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ulRnPsR79hS7","executionInfo":{"status":"ok","timestamp":1638765870211,"user_tz":300,"elapsed":792,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"dc1589bb-ac55-4879-ca98-8c15e3cb75ed"},"source":["nltk.download('all')"],"id":"ulRnPsR79hS7","execution_count":91,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Package abc is already up-to-date!\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Package alpino is already up-to-date!\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Package brown is already up-to-date!\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Package brown_tei is already up-to-date!\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Package cess_cat is already up-to-date!\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Package cess_esp is already up-to-date!\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Package chat80 is already up-to-date!\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package city_database is already up-to-date!\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Package cmudict is already up-to-date!\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package comparative_sentences is already up-to-\n","[nltk_data]    |       date!\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    |   Package comtrans is already up-to-date!\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Package conll2000 is already up-to-date!\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Package conll2002 is already up-to-date!\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    |   Package conll2007 is already up-to-date!\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Package crubadan is already up-to-date!\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package dependency_treebank is already up-to-date!\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Package dolch is already up-to-date!\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package europarl_raw is already up-to-date!\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Package floresta is already up-to-date!\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package framenet_v15 is already up-to-date!\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package framenet_v17 is already up-to-date!\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Package gazetteers is already up-to-date!\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Package genesis is already up-to-date!\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Package gutenberg is already up-to-date!\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Package ieer is already up-to-date!\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Package indian is already up-to-date!\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    |   Package jeita is already up-to-date!\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Package kimmo is already up-to-date!\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    |   Package knbc is already up-to-date!\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Package mac_morpho is already up-to-date!\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    |   Package machado is already up-to-date!\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    |   Package masc_tagged is already up-to-date!\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package moses_sample is already up-to-date!\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package movie_reviews is already up-to-date!\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Package names is already up-to-date!\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Package nps_chat is already up-to-date!\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Package omw is already up-to-date!\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Package paradigms is already up-to-date!\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Package pil is already up-to-date!\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Package pl196x is already up-to-date!\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Package ppattach is already up-to-date!\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package problem_reports is already up-to-date!\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    |   Package propbank is already up-to-date!\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Package ptb is already up-to-date!\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Package pros_cons is already up-to-date!\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Package qc is already up-to-date!\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    |   Package reuters is already up-to-date!\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Package rte is already up-to-date!\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    |   Package semcor is already up-to-date!\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Package senseval is already up-to-date!\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sentiwordnet is already up-to-date!\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sentence_polarity is already up-to-date!\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Package shakespeare is already up-to-date!\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sinica_treebank is already up-to-date!\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Package smultron is already up-to-date!\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Package state_union is already up-to-date!\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Package stopwords is already up-to-date!\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package subjectivity is already up-to-date!\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Package swadesh is already up-to-date!\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Package switchboard is already up-to-date!\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Package timit is already up-to-date!\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Package toolbox is already up-to-date!\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Package treebank is already up-to-date!\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package twitter_samples is already up-to-date!\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Package udhr is already up-to-date!\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Package udhr2 is already up-to-date!\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package unicode_samples is already up-to-date!\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n","[nltk_data]    |       date!\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Package verbnet is already up-to-date!\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Package verbnet3 is already up-to-date!\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Package webtext is already up-to-date!\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Package wordnet is already up-to-date!\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Package wordnet_ic is already up-to-date!\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Package words is already up-to-date!\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Package ycoe is already up-to-date!\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Package rslp is already up-to-date!\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package universal_tagset is already up-to-date!\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Package punkt is already up-to-date!\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package book_grammars is already up-to-date!\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sample_grammars is already up-to-date!\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package spanish_grammars is already up-to-date!\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package basque_grammars is already up-to-date!\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package large_grammars is already up-to-date!\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Package tagsets is already up-to-date!\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package snowball_data is already up-to-date!\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package word2vec_sample is already up-to-date!\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Package mte_teip5 is already up-to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n","[nltk_data]    |       up-to-date!\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package perluniprops is already up-to-date!\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package vader_lexicon is already up-to-date!\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Package porter_test is already up-to-date!\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Package wmt15_eval is already up-to-date!\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":91}]},{"cell_type":"code","metadata":{"id":"v0_Dd-Cj7yo1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638765875799,"user_tz":300,"elapsed":2533,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"c0e75655-6c93-4f60-c23b-e925c8bcd167"},"source":[" #download crf library for crf layer usage\n"," !pip install tf2crf"],"id":"v0_Dd-Cj7yo1","execution_count":92,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tf2crf in /usr/local/lib/python3.7/dist-packages (0.1.33)\n","Requirement already satisfied: tensorflow-addons>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from tf2crf) (0.15.0)\n","Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tf2crf) (2.7.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.42.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.1.2)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (0.22.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (3.17.3)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (3.1.0)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (0.12.0)\n","Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (0.4.0)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.19.5)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.6.3)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (3.3.0)\n","Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (2.7.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.15.0)\n","Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (2.0)\n","Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (0.37.0)\n","Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (2.7.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (3.10.0.2)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (12.0.0)\n","Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (2.7.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.13.3)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.1.0->tf2crf) (1.5.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (2.23.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (1.8.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (3.3.6)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (0.4.6)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (57.4.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (0.6.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (1.35.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (1.3.0)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (4.8.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (3.6.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.1.0->tf2crf) (3.1.1)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons>=0.8.2->tf2crf) (2.7.1)\n"]}]},{"cell_type":"code","metadata":{"id":"anSz5OCk8HQ1"},"source":["from tf2crf import CRF, ModelWithCRFLoss,ModelWithCRFLossDSCLoss"],"id":"anSz5OCk8HQ1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nO93SIoX8KAL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638765884861,"user_tz":300,"elapsed":5289,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"9414d531-38e4-424b-e7a5-4e1b4b578164"},"source":["#download sklearn-crfsuite for crf implementation\n","!pip install sklearn-crfsuite\n","!pip install -U 'scikit-learn<0.24'"],"id":"nO93SIoX8KAL","execution_count":93,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.7/dist-packages (0.3.6)\n","Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (0.9.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (1.15.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (0.8.9)\n","Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (4.62.3)\n","Requirement already satisfied: scikit-learn<0.24 in /usr/local/lib/python3.7/dist-packages (0.23.2)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.24) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.24) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.24) (3.0.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.24) (1.19.5)\n"]}]},{"cell_type":"code","metadata":{"id":"XcP9XVOC8uV0","executionInfo":{"status":"ok","timestamp":1638765891996,"user_tz":300,"elapsed":111,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}}},"source":["import sklearn_crfsuite\n","from sklearn_crfsuite import scorers\n","from sklearn_crfsuite import metrics\n","from sklearn_crfsuite import CRF"],"id":"XcP9XVOC8uV0","execution_count":94,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SOEYbh2y8w0B"},"source":["## Importation de Glove"],"id":"SOEYbh2y8w0B"},{"cell_type":"code","metadata":{"id":"vcQlXPXZ8vyO"},"source":["import requests, zipfile, io\n","zip_file_url = \"http://nlp.stanford.edu/data/glove.840B.300d.zip\"\n","r = requests.get(zip_file_url)\n","z = zipfile.ZipFile(io.BytesIO(r.content))\n","z.extractall()"],"id":"vcQlXPXZ8vyO","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_FRodkgc88P6"},"source":["import codecs\n","print('loading word embeddings...')\n","\n","\n","embeddings_index = {}\n","f = codecs.open('glove.840B.300d.txt', encoding='utf-8')\n","\n","for line in tqdm(f):\n","    values = line.rstrip().rsplit(' ')\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('found %s word vectors' % len(embeddings_index))"],"id":"_FRodkgc88P6","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hzmXMBAS9vsr"},"source":["# Fonctions utiles "],"id":"hzmXMBAS9vsr"},{"cell_type":"code","metadata":{"id":"fRswAUwiXwel"},"source":["def show_distrib(all_tokens):\n","  max_len, mean_len = 0, 0\n","  tok_count = {}\n","  nb_tags = 0\n","  for tags in all_tokens:\n","    nb_tags += len(tags)\n","    for tag in tags:\n","      if tok_count.get(tag) is None:\n","        tok_count[tag] = 1\n","      else:\n","        tok_count[tag] += 1\n","  tok_freq = {tag: value / nb_tags for (tag, value) in tok_count.items()}\n","  plt.bar(tok_freq.keys(), tok_freq.values())\n","  plt.title('Distribution des tag')\n","  plt.xlabel('Tags')\n","  plt.ylabel('Fréquence')\n","  plt.show()"],"id":"fRswAUwiXwel","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AIA8RYyj92_Z"},"source":["# we get data in the form of a list of tokens and tags given the their dataframe\n","\n","def vectorize_tagged_sentence(df, folder):\n","  sentences = []\n","  ids = []\n","  tags = []\n","  for doc_id in tqdm(doc_ids[folder]):\n","    doc = df[df[\"DocID\"] == doc_id]\n","    sentences.append(doc[\"Token\"].values.astype(str))\n","    tags.append(doc[\"Tag\"].values)\n","    ids.append(doc[\"TokenID\"].values.astype(str))\n","  return sentences,tags, ids\n","\n","def vectorize_test_sentence(df):\n","  sentences = []\n","  ids = []\n","  for doc_id in tqdm(doc_ids['test']):\n","    doc = df[df[\"DocID\"] == doc_id]\n","    sentences.append(doc[\"Token\"].values)\n","    ids.append(doc[\"TokenID\"].values.astype(str))\n","  return sentences, ids\n"],"id":"AIA8RYyj92_Z","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Avh4SZm-OoY"},"source":["def check_training_preproccessing():\n","  \"\"\"Cette fonction sert à vérifier que les données d'entrainements possèdent les propriétés attendues.\n","  Notemment on vérifie la proportion de chaque tag ce qui permet d'avoir un estimation de si on en a oulié.\n","  Ensuite on applique \"\"\"\n","  max_len, mean_len = 0, 0\n","  tok_count = {}\n","  for sentence in train_sentences:\n","    max_len = max(max_len, len(sentence))\n","    mean_len += len(sentence)\n","  nb_tags = 0\n","  for tags in train_tags:\n","    nb_tags += len(tags)\n","    for tag in tags:\n","      if tok_count.get(tag) is None:\n","        tok_count[tag] = 1\n","      else:\n","        tok_count[tag] += 1\n","  tok_freq = {tag: value / nb_tags for (tag, value) in tok_count.items()}\n","  print(f\"max_len={max_len}, mean_len={mean_len / len(train_sentences):.1f}\")\n","  print(f\"tok_freq={tok_freq}\")\n","\n","  bad = 0\n","  for k, tags in enumerate(train_tags):\n","    for i in range(len(tags)):\n","      if \"B\" in tags[i] and not (i == 0 or 'L' in tags[i-1] or 'U' in tags[i-1] or 'O' in tags[i-1]):\n","        bad += 1\n","        # print(tags)\n","      if \"L\" in tags[i] and not (i == len(train_tags)-1 or 'B' in tags[i+1] or 'U' in tags[i+1] or 'O' in tags[i+1]):\n","        bad += 1\n","        # print(tags)\n","  print(\"nb of badly formed taggings:\",bad)\n","\n","  nb_entity = 0\n","  for doc in os.listdir(\"data/train\"):\n","    if \".ann\" in doc:\n","      ANN = pd.read_csv(f\"data/train/{doc}\", delimiter=\"\\t\", names=[\"Type\", \"Annotation\", \"Tokens\"])\n","      ANN = ANN.drop(ANN[ANN['Type'].map(lambda x: x[0]) != \"T\"].index)\n","      ANN[\"Tokens begining\"] = ANN[\"Annotation\"].apply(lambda x: int(x.split()[1]))\n","      ANN = ANN.to_numpy()\n","      ANN = ANN[np.argsort(ANN[:, 3])]\n","      p = 0\n","      for i in range(ANN.shape[0]):\n","        if i > 0 and int(re.split(\" |;\", ANN[i,1])[1]) <= int(re.split(\" |;\", ANN[p,1])[2]):\n","          continue\n","        p = i\n","        nb_entity += 1\n","\n","  nb_found = 0\n","  for tags in train_tags:\n","    for tag in tags:\n","      if \"B\" in tag or \"U\" in tag:\n","        nb_found += 1\n","  print(\"Nb of labeled entity:\", nb_entity, \"\\ nb of found entity:\", nb_found)\n","  print(f\"Prop of possibly missing tags: {(nb_entity-nb_found)/nb_entity:.4f}%\")\n","# check_training_preproccessing()"],"id":"2Avh4SZm-OoY","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E1YuNIfm-9BY"},"source":["def weighted_categorical_crossentropy(weights):\n","    \"\"\"\n","    A weighted version of keras.objectives.categorical_crossentropy\n","    \n","    Variables:\n","        weights: numpy array of shape (C,) where C is the number of classes\n","    \n","    Usage:\n","        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n","        loss = weighted_categorical_crossentropy(weights)\n","        model.compile(loss=loss,optimizer='adam')\n","    \"\"\"\n","    \n","    weights = K.variable(weights)\n","        \n","    def loss(y_true, y_pred):\n","        # scale predictions so that the class probas of each sample sum to 1\n","        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n","        # clip to prevent NaN's and Inf's\n","        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n","        # calc\n","        loss = y_true * K.log(y_pred) * weights\n","        loss = -K.sum(loss, -1)\n","        return loss\n","    \n","    return loss"],"id":"E1YuNIfm-9BY","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hAjmUBH5ABKI"},"source":["def get_seq(array):\n","  return np.argmax(array, axis=2).flatten()\n","def get_f1_micro(y_true, y_pred): \n","  return f1_score(get_seq(y_true), get_seq(y_pred), average='micro') \n","def get_f1_macro(y_true, y_pred):\n","  return f1_score(get_seq(y_true), get_seq(y_pred), average='macro')"],"id":"hAjmUBH5ABKI","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"arz1i-UyBex4"},"source":["# printing results of the benchmark done for bilstm model task A\n","def print_training(histories, metric_names, params, size=(20,30)):\n","  nb_models = len(params)\n","  nb_params = len(metric_names)\n","  f, axes = plt.subplots(nb_models, nb_params)\n","  f.set_size_inches(size[0], size[1])\n","  for i, history in enumerate(histories):\n","    for j, metric_name in enumerate(metric_names):\n","      if len(axes.shape) == 1:\n","        ax = axes[i]\n","      else:\n","        ax = axes[i,j]\n","\n","      ax.plot(history.history[metric_name])\n","      ax.plot(history.history[\"val_\"+metric_name])\n","      ax.set_title(params[i])\n","      ax.set_ylabel(metric_name)\n","      ax.set_xlabel('epoch')\n","      ax.legend(['train set', 'validation set'], loc='upper left')\n","  f.tight_layout(pad=3.0)\n","  plt.show()"],"id":"arz1i-UyBex4","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yio7Aigx8Tyb"},"source":["# importations du dataset et visualisation"],"id":"yio7Aigx8Tyb"},{"cell_type":"code","metadata":{"id":"1rNdqG3ZPqfQ"},"source":["# On charge les fichier .csv dans lesquels se trouve la liste des jetons à annoter. \n","# On utilise `keep_default_na=False` pour éviter de transformer le mot 'null' en la valeur 'na'\n","\n","train_df = pd.read_csv(\"data/train.csv\", keep_default_na=False)\n","val_df = pd.read_csv(\"data/val.csv\", keep_default_na=False)\n","test_df = pd.read_csv(\"data/test.csv\", keep_default_na=False)"],"id":"1rNdqG3ZPqfQ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dHqwai0_mFtX","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1638764131130,"user_tz":300,"elapsed":101,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"071cd80c-880b-47ca-81d6-913acfed9d76"},"source":["train_df.head()"],"id":"dHqwai0_mFtX","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>DocID</th>\n","      <th>TokenID</th>\n","      <th>Token</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>S0022311514001640</td>\n","      <td>S0022311514001640-0</td>\n","      <td>The</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>S0022311514001640</td>\n","      <td>S0022311514001640-1</td>\n","      <td>vapour</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>S0022311514001640</td>\n","      <td>S0022311514001640-2</td>\n","      <td>phase</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>S0022311514001640</td>\n","      <td>S0022311514001640-3</td>\n","      <td>consists</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>S0022311514001640</td>\n","      <td>S0022311514001640-4</td>\n","      <td>of</td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               DocID              TokenID     Token Tag\n","0  S0022311514001640  S0022311514001640-0       The    \n","1  S0022311514001640  S0022311514001640-1    vapour    \n","2  S0022311514001640  S0022311514001640-2     phase    \n","3  S0022311514001640  S0022311514001640-3  consists    \n","4  S0022311514001640  S0022311514001640-4        of    "]},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"HldL-qgYoZSH","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1638764131132,"user_tz":300,"elapsed":96,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"e448ace7-f9b9-4c8c-9c95-fd8073c8e3af"},"source":["val_df.head()"],"id":"HldL-qgYoZSH","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>DocID</th>\n","      <th>TokenID</th>\n","      <th>Token</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>S0301010415300355</td>\n","      <td>S0301010415300355-0</td>\n","      <td>Alternatively</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>S0301010415300355</td>\n","      <td>S0301010415300355-1</td>\n","      <td>to</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>S0301010415300355</td>\n","      <td>S0301010415300355-2</td>\n","      <td>H-atom</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>S0301010415300355</td>\n","      <td>S0301010415300355-3</td>\n","      <td>photodetachment</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>S0301010415300355</td>\n","      <td>S0301010415300355-4</td>\n","      <td>from</td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               DocID              TokenID            Token Tag\n","0  S0301010415300355  S0301010415300355-0    Alternatively    \n","1  S0301010415300355  S0301010415300355-1               to    \n","2  S0301010415300355  S0301010415300355-2           H-atom    \n","3  S0301010415300355  S0301010415300355-3  photodetachment    \n","4  S0301010415300355  S0301010415300355-4             from    "]},"metadata":{},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"67kv8OsHZTC7"},"source":["# On sauvegarde les documents dans une variable afin de pouvoir retrouver la provenance du document si on voit des problème pendant le prétraitement\n","\n","doc_ids = {'train':list(set(train_df[\"DocID\"].values)), 'val':list(set(val_df[\"DocID\"].values)), 'test':list(set(test_df[\"DocID\"].values))}\n","N_doc = len(doc_ids['train']) + len(doc_ids['val'])"],"id":"67kv8OsHZTC7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hC1vqR_Q9Hd4"},"source":["# 1. Etat de l’art (10%)\n","\n","Décrivez en deux paragraphes, dans une cellule du notebook, l’état de l’art pour la reconnaissance de mots clé et leur annotation. Utilisez le service Google Scholar. Voici quelques mots-clé (non exhaustifs) : Named Entity recognition, NER, entity typing.  \n","\n","Quelles sont les meilleures techniques de l’état de l’art ?\n"],"id":"hC1vqR_Q9Hd4"},{"cell_type":"markdown","metadata":{"id":"gJprGRkJ9SO0"},"source":["L’état de l’art pour la reconnaissance de mots-clés et leur annotation s’est appuyée sur des réseaux de neurones et notamment des LSTM jusqu’en 2017 environ avant d’être dépassé par les modèles à base de transformers, et notamment grâce au modèle BERT.\n","\n","Parmi les meilleurs modèles pour la reconnaissance d’entités nommées (en les évaluant sur le corpus CoNLL 2003) on trouve :\n","-\tAutomated Concatenation of Embeddings for Structured Prediction [1] (Wang et al., 2021) qui consiste en la représentation de mots par une concaténation de différents types d’embeddings, en utilisant de l’apprentissage par renforcement pour déterminer quel concaténation choisir.\n","-\tLUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention [2] (Yamada et al., 2020) qui se base sur des représentations pré-entrainées de mots basées sur des transformers bidirectionnels et sur le modèle de BERT. La méthode propose aussi un mécanisme d’attention propre qui soit « entity-aware ».\n","-\tImproving Named Entity Recognition by External Context Retrieving and Cooperative Learning [3] (Wang et al., 2021) qui considère pour les mots d’une phrase des contextes extérieurs à la phrase elle-même, en recherchant un ensemble de textes sémantiquement proches de la phrase d’origine dans un moteur de recherche.\n","\n","[1] https://arxiv.org/abs/2010.05006\n","\n","[2] https://arxiv.org/abs/2010.01057\n","\n","[3] https://arxiv.org/abs/2105.03654\n"],"id":"gJprGRkJ9SO0"},{"cell_type":"markdown","metadata":{"id":"7AVUmSBL9TJe"},"source":["# 2. Sous-tâche A : Identification des mots-clés (65%)"],"id":"7AVUmSBL9TJe"},{"cell_type":"markdown","metadata":{"id":"v9Di3d08s7ZJ"},"source":["## a. Prétraitement des données\n","Premières étapes de prétraitement communes à tous les modèles. Nous récupérons les informations des fichiers .ann pour associé les tags aux jetons."],"id":"v9Di3d08s7ZJ"},{"cell_type":"code","metadata":{"id":"HcNejl67SBMQ"},"source":["show_id = 18\n","def set_tag_A(df, sub_data):\n","  \"\"\"\n","  Fonction pour annoter les jetons présents dans les fichiers .csv\n","  On parcours toutes les lignes du fichier et parallèlement on avance dans les fichiers .ann pour récupérer les tags associés.\n","  \"\"\"\n","  df[\"Tag\"] = df[\"Tag\"].astype(str)\n","  df[\"Token\"] = df[\"Token\"].astype(str)\n","  doc_id = None\n","  for i in tqdm(range(len(df))):\n","    if df[\"DocID\"][i] != doc_id:\n","      i0 = i\n","      doc_id = df[\"DocID\"][i]\n","      ANN = pd.read_csv(f\"data/{sub_data}/{doc_id}.ann\", delimiter=\"\\t\", names=[\"Type\", \"Annotation\", \"Tokens\"])\n","      ANN = ANN.drop(ANN[ANN['Type'].map(lambda x: x[0]) != \"T\"].index)\n","      ANN[\"Tokens begining\"] = ANN[\"Annotation\"].apply(lambda x: int(x.split()[1]))\n","      ANN = ANN.to_numpy()\n","      ANN = ANN[np.argsort(ANN[:, 3])]\n","      tokens_id = 0\n","      match_id = 0\n","      entity_ids = []\n","    if tokens_id >= len(ANN): \n","      df.at[i, 'Tag'] = \"O\"\n","      continue\n","    tokens = nltk.word_tokenize(ANN[tokens_id][2])\n","    token = df['Token'][i]\n","    if tokens[match_id] in token:\n","      if match_id == 0 and len(tokens) == 1:\n","        df.at[i, 'Tag'] = \"U\"\n","        previous_tokens_id = tokens_id\n","        tokens_id += 1\n","        while tokens_id < len(ANN) and int(re.split(\" |;\", ANN[tokens_id][1])[1]) < int(re.split(\" |;\", ANN[previous_tokens_id][1])[2]):\n","          tokens_id += 1\n","      elif match_id == 0:\n","        entity_ids = [i]\n","        match_id += 1\n","      elif len(tokens) == match_id + 1:\n","        df.at[entity_ids[0], 'Tag'] = \"B\"\n","        for k in entity_ids[1:]:\n","          df.at[k, 'Tag'] = \"I\"\n","        df.at[i, 'Tag'] = \"L\"\n","        match_id = 0\n","        previous_tokens_id = tokens_id\n","        tokens_id += 1\n","        while tokens_id < len(ANN) and int(re.split(\" |;\", ANN[tokens_id][1])[1]) < int(re.split(\" |;\", ANN[previous_tokens_id][1])[2]):\n","          tokens_id += 1\n","        entity_ids = []\n","      else:\n","        entity_ids.append(i)\n","        match_id += 1\n","    else:\n","      df.at[i, 'Tag'] = \"O\"\n","      for j in entity_ids:\n","        df.at[j, 'Tag'] = \"O\"\n","      entity_ids = []\n","      match_id = 0\n","  return df\n"],"id":"HcNejl67SBMQ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"URbRNmkZmtDr"},"source":["# Comme la fonction `set_tag` met un certain temps à être exécutée, on sauvegarde le résultat pour pouvoir le réutiliser plus tard\n","\n","if not \"train_A_bilou.csv\" in os.listdir():\n","  train_A_df = set_tag_A(train_df, \"train\")\n","  train_A_df.to_csv(\"train_A_bilou.csv\", index = False, header = True)\n","else:\n","  train_A_df = pd.read_csv(\"train_A_bilou.csv\")\n","\n","if not \"val_A_bilou.csv\" in os.listdir():\n","  val_A_df = set_tag_A(val_df, \"val\")\n","  val_A_df.to_csv(\"val_A_bilou.csv\", index = False, header = True)\n","else:\n","  val_A_df = pd.read_csv(\"val_A_bilou.csv\")\n"],"id":"URbRNmkZmtDr","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Flk3hvY8MX0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638764135348,"user_tz":300,"elapsed":25,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"908fb445-3aef-4b8e-b016-ed22d65243a1"},"source":["train_A_df.head()"],"id":"7Flk3hvY8MX0","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>DocID</th>\n","      <th>TokenID</th>\n","      <th>Token</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>S0022311514001640</td>\n","      <td>S0022311514001640-0</td>\n","      <td>The</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>S0022311514001640</td>\n","      <td>S0022311514001640-1</td>\n","      <td>vapour</td>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>S0022311514001640</td>\n","      <td>S0022311514001640-2</td>\n","      <td>phase</td>\n","      <td>L</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>S0022311514001640</td>\n","      <td>S0022311514001640-3</td>\n","      <td>consists</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>S0022311514001640</td>\n","      <td>S0022311514001640-4</td>\n","      <td>of</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               DocID              TokenID     Token Tag\n","0  S0022311514001640  S0022311514001640-0       The   O\n","1  S0022311514001640  S0022311514001640-1    vapour   B\n","2  S0022311514001640  S0022311514001640-2     phase   L\n","3  S0022311514001640  S0022311514001640-3  consists   O\n","4  S0022311514001640  S0022311514001640-4        of   O"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"6804da3a-e448-4644-bc95-20a7332ccb50","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638764136985,"user_tz":300,"elapsed":1657,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"c335d887-4d6d-4983-fec6-83ad4acf74ec"},"source":["# TODO : \n","train_sentences, train_tags, train_ids = vectorize_tagged_sentence(train_A_df, 'train')\n","val_sentences, val_tags, val_ids = vectorize_tagged_sentence(val_A_df, 'val')\n","test_sentences, test_ids = vectorize_test_sentence(test_df)"],"id":"6804da3a-e448-4644-bc95-20a7332ccb50","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 350/350 [00:01<00:00, 239.68it/s]\n","100%|██████████| 50/50 [00:00<00:00, 729.22it/s]\n","100%|██████████| 100/100 [00:00<00:00, 600.45it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"daS6H845-EOM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638764136986,"user_tz":300,"elapsed":15,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"d6df306e-5544-46af-d192-c54d3c4eabc5"},"source":["print(train_sentences[show_id])\n","print(train_tags[show_id])"],"id":"daS6H845-EOM","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['The' 'final' 'contribution' 'to' 'the' 'force' 'is' 'the' 'van' 'der'\n"," 'Waals' 'interaction' '.' 'It' 'includes' 'the' 'following'\n"," 'contributions' ':' '(' 'i' ')' 'between' 'the' 'macroscopic' 'Si' 'tip'\n"," 'of' 'conical' 'shape' 'with' 'the' 'sphere' 'of' 'radius' 'R' 'at' 'the'\n"," 'end' '[' '27' ']' 'and' 'semi-infinite' 'substrate' ';' '(' 'ii' ')'\n"," 'the' 'dispersion' 'forces' 'between' 'the' 'atoms' 'in' 'the' 'sample'\n"," 'treated' 'atomistically' ';' 'and' '(' 'iii' ')' 'the' 'interaction'\n"," 'between' 'the' 'macroscopic' 'part' 'of' 'the' 'tip' 'and' 'the'\n"," 'sample' 'atoms' '.' 'The' 'first' 'contribution' 'is' 'calculated'\n"," 'analytically' '[' '27' ']' '.' 'In' 'fact' ',' 'the' 'macroscopic'\n"," 'contribution' 'to' 'the' 'van' 'der' 'Waals' 'force' 'is' 'the' 'same'\n"," 'in' 'each' 'of' 'the' 'three' 'systems' 'described' 'below' ',' 'as'\n"," 'it' 'depends' 'only' 'on' 'the' 'tip–surface' 'separation' ','\n"," 'macroscopic' 'sphere' 'radius' ',' 'cone-angle' 'and' 'Hamaker'\n"," 'constant' 'of' 'the' 'system' '[' '27' ']' '.' 'All' 'these'\n"," 'quantities' 'are' 'identical' 'in' 'each' 'system' 'we' 'look' 'at' ','\n"," 'so' 'that' 'the' 'van' 'der' 'Waals' 'force' 'acts' 'as' 'a'\n"," 'background' 'attractive' 'force' 'independent' 'of' 'the' 'microscopic'\n"," 'properties' 'of' 'the' 'system' '[' '8' ']' '.' 'The' 'Hamaker'\n"," 'constant' 'needed' 'for' 'the' 'calculation' 'of' 'the' 'macroscopic'\n"," 'van' 'der' 'Waals' 'force' 'is' 'estimated' 'to' 'be' '0.5eV' '[' '32'\n"," ']' '.']\n","['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'I' 'I' 'L' 'O' 'O' 'O' 'O' 'O' 'O'\n"," 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'I' 'L' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n"," 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'L' 'O' 'O' 'O' 'O' 'O' 'B' 'L' 'O' 'O'\n"," 'U' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'I' 'I'\n"," 'I' 'L' 'O' 'O' 'B' 'L' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n"," 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'I' 'I' 'L' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n"," 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'L' 'O' 'O' 'O' 'O' 'O'\n"," 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n"," 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B' 'I' 'I' 'L' 'O' 'O' 'O' 'O' 'O' 'O'\n"," 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O'\n"," 'U' 'O' 'O' 'B' 'I' 'I' 'I' 'L' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n"]}]},{"cell_type":"code","metadata":{"id":"iBg545_qw0LY","colab":{"base_uri":"https://localhost:8080/","height":294},"executionInfo":{"status":"ok","timestamp":1638754598232,"user_tz":300,"elapsed":647,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"7bf4840e-f38e-4bee-ebd0-d61b63574af4"},"source":["show_distrib(train_tags)"],"id":"iBg545_qw0LY","execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"4r0sjGoqCqbO"},"source":["***REMARQUE***\n","on remarque que le jeu de données est très déséquilibré."],"id":"4r0sjGoqCqbO"},{"cell_type":"markdown","metadata":{"id":"368f9b8c-b54d-44f9-9249-e91021f50ded"},"source":["## b.&c. Models et Evaluations"],"id":"368f9b8c-b54d-44f9-9249-e91021f50ded"},{"cell_type":"markdown","metadata":{"id":"Y4kpbbarJ2M8"},"source":["### CRF simple"],"id":"Y4kpbbarJ2M8"},{"cell_type":"markdown","metadata":{"id":"csDaPBQhem6U"},"source":["On utilise le modèle proposé par le tutoriel en ligne CRF avec l'algorithme L-BFGS et Elastic Net avec les régulariseur (L1 + L2).\n","\n","https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html#training"],"id":"csDaPBQhem6U"},{"cell_type":"markdown","metadata":{"id":"znXh_IlLoxVm"},"source":["#### Prétraitement"],"id":"znXh_IlLoxVm"},{"cell_type":"code","metadata":{"id":"mUur7CeX_BR1"},"source":["# we define the input of our CRF by addind the POS tag for each sentence, \n","# we need them for the features of our CRF\n","X_crf_train= nltk.pos_tag_sents(train_sentences)\n","X_crf_val = nltk.pos_tag_sents(val_sentences)\n","X_crf_test = nltk.pos_tag_sents(test_sentences)"],"id":"mUur7CeX_BR1","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3eHhFVFW9KVE"},"source":["#### Definition du Model"],"id":"3eHhFVFW9KVE"},{"cell_type":"code","metadata":{"id":"GI_1ZJbiqJrg"},"source":["# Features have been chosen based on what has been done on other models found on the internet and logical sense\n","def features(sentence, index):\n","    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n","    word = sentence[index][0]\n","    postag = sentence[index][1]\n","\n","    features = {\n","        'bias': 1.0,\n","        'word': word,\n","        'is_first': index == 0,\n","        'is_last': index == len(sentence) - 1,\n","        'word.lower()': word.lower(),\n","        'word[-3:]': word[-3:],\n","        'word[-2:]': word[-2:],\n","        'prev_word': '' if index == 0 else sentence[index - 1][0],\n","        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1][0],\n","        'word.isupper()': word.isupper(),\n","        'word.istitle()': word.istitle(),\n","        'word.isdigit()': word.isdigit(),\n","        'postag': postag,\n","        'postag[:2]': postag[:2],\n","        'postag[:-2]': postag[:-2],\n","    }\n","    if index > 0:\n","        word1 = sentence[index-1][0]\n","        postag1 = sentence[index-1][1]\n","        features.update({\n","            '-1:word.lower()': word1.lower(),\n","            '-1:word.istitle()': word1.istitle(),\n","            '-1:word.isupper()': word1.isupper(),\n","            '-1:postag': postag1,\n","            '-1:postag[:2]': postag1[:2],\n","        })\n","\n","    if index < len(word)-1:\n","        word1 = sentence[index+1][0]\n","        postag1 = sentence[index+1][1]\n","        features.update({\n","            '+1:word.lower()': word1.lower(),\n","            '+1:word.istitle()': word1.istitle(),\n","            '+1:word.isupper()': word1.isupper(),\n","            '+1:postag': postag1,\n","            '+1:postag[:2]': postag1[:2],\n","        })\n","    return features"],"id":"GI_1ZJbiqJrg","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g1f0CdAprZ34"},"source":["def transform_to_dataset(tagged_sentences):\n","    X= []\n","    for tagged in tagged_sentences:\n","      X.append([features(tagged, index) for index in range(len(tagged))])\n"," \n","    return X\n","CRF_train_sentences = transform_to_dataset(X_crf_train)"],"id":"g1f0CdAprZ34","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P91MWhUos35e"},"source":["#### Entrainement d'un model"],"id":"P91MWhUos35e"},{"cell_type":"code","metadata":{"id":"9oLewH00s2q3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638764304136,"user_tz":300,"elapsed":6107,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"f0796030-86df-4442-88c3-f7ac6a1d902d"},"source":[" model_CRF_tacheA_ = CRF(\n","    algorithm='lbfgs',\n","    c1=0.1,\n","    c2=0.1,\n","    max_iterations=100,\n","    all_possible_transitions=True\n","    )\n","\n","model_CRF_tacheA_.fit(CRF_train_sentences, train_tags)"],"id":"9oLewH00s2q3","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/base.py:213: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n","  FutureWarning)\n"]},{"output_type":"execute_result","data":{"text/plain":["CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n","    keep_tempfiles=None, max_iterations=100)"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"kesUBfhQLe8C"},"source":["model_CRF_tacheA = model_CRF_tacheA_"],"id":"kesUBfhQLe8C","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CwyvsxLIKroi"},"source":["#### Grid search"],"id":"CwyvsxLIKroi"},{"cell_type":"code","metadata":{"id":"xWaMzA45Bzi3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638764304139,"user_tz":300,"elapsed":34,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"0e516920-f28b-41d3-d254-a6cb03ea5bd8"},"source":["'''\n","model_CRF_tacheA = CRF(\n","    algorithm='lbfgs',\n","    max_iterations=100,\n","    all_possible_transitions=True\n","    )\n","\n","params_space = {\n","    'c1': scipy.stats.expon(scale=0.5),\n","    'c2': scipy.stats.expon(scale=0.05),\n","}\n","\n","labels = list(model_CRF_tacheA_.classes_)\n","labels.remove('O')\n","\n","# use the same metric for evaluation\n","f1_scorer = make_scorer(metrics.flat_f1_score,\n","                        average='weighted', labels=labels)\n","\n","# search\n","rs = RandomizedSearchCV(model_CRF_tacheA, params_space,\n","                        cv=3,\n","                        verbose=1,\n","                        n_jobs=-1,\n","                        n_iter=50,\n","                        scoring=f1_scorer)\n","rs.fit(CRF_train_sentences, train_tags)\n","'''"],"id":"xWaMzA45Bzi3","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nmodel_CRF_tacheA = CRF(\\n    algorithm='lbfgs',\\n    max_iterations=100,\\n    all_possible_transitions=True\\n    )\\n\\nparams_space = {\\n    'c1': scipy.stats.expon(scale=0.5),\\n    'c2': scipy.stats.expon(scale=0.05),\\n}\\n\\nlabels = list(model_CRF_tacheA_.classes_)\\nlabels.remove('O')\\n\\n# use the same metric for evaluation\\nf1_scorer = make_scorer(metrics.flat_f1_score,\\n                        average='weighted', labels=labels)\\n\\n# search\\nrs = RandomizedSearchCV(model_CRF_tacheA, params_space,\\n                        cv=3,\\n                        verbose=1,\\n                        n_jobs=-1,\\n                        n_iter=50,\\n                        scoring=f1_scorer)\\nrs.fit(CRF_train_sentences, train_tags)\\n\""]},"metadata":{},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"ZZ0Obo03CnYD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638764304140,"user_tz":300,"elapsed":33,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"5aaa3135-1168-4aec-b836-28e04d7e7a78"},"source":["'''\n","print('best params:', rs.best_params_)\n","print('best CV score:', rs.best_score_)\n","print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))\n","'''"],"id":"ZZ0Obo03CnYD","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nprint('best params:', rs.best_params_)\\nprint('best CV score:', rs.best_score_)\\nprint('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))\\n\""]},"metadata":{},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"te-d-IoMCv2N"},"source":["#model_CRF_tacheA = rs.best_estimator_"],"id":"te-d-IoMCv2N","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l-Eiis34o4-h"},"source":["#### prediction"],"id":"l-Eiis34o4-h"},{"cell_type":"code","metadata":{"id":"Ig47kjNi37ln"},"source":["def tag_prediction(sentences, model):\n","  X= []\n","  Y= []\n","  \n","  for tagged in sentences:\n","    sentence_features=[features(tagged, index) for index in range(len(tagged))]\n","    X.append(sentence_features)\n","    y_pred = model.predict([sentence_features])[0]\n","    Y.append(y_pred)\n","  return X, Y\n"," \n","CRF_train_sentences, y_train_pred = tag_prediction(X_crf_train,model_CRF_tacheA)\n","CRF_val_sentences, y_val_pred = tag_prediction(X_crf_val,model_CRF_tacheA)\n","CRF_test_sentences, y_test_pred = tag_prediction(X_crf_test,model_CRF_tacheA)\n"],"id":"Ig47kjNi37ln","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KAY0wVxGpn3O"},"source":["#### Evaluation on train and val set"],"id":"KAY0wVxGpn3O"},{"cell_type":"code","metadata":{"id":"82G6ZKVjB1be"},"source":["def CRF_show_confusion(y_true, y_pred):\n","  print(f\"f1 micro:{f1_score(y_true, y_pred, average='micro'):.3f}\")\n","  print(f\"f1 micro:{f1_score(y_true, y_pred, average='macro'):.3f}\")\n","  print(f\"recall macro:{recall_score(y_true, y_pred, average='macro'):.3f}\")\n","  print(\"Confusion Matrix:\")\n","  print(confusion_matrix(y_true, y_pred))"],"id":"82G6ZKVjB1be","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-9Rwsf0aBa9m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638764305524,"user_tz":300,"elapsed":11,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"0f823064-165c-4ca7-c216-ba709132bd9c"},"source":["labels = list(model_CRF_tacheA.classes_)\n","labels.remove('O')\n","print(metrics.flat_f1_score(list(np.concatenate(val_tags)), list(np.concatenate(y_val_pred)),\n","      average='weighted', labels=labels))"],"id":"-9Rwsf0aBa9m","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.47435081824155995\n"]}]},{"cell_type":"code","metadata":{"id":"_-_2eRrbAsb1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638764305524,"user_tz":300,"elapsed":10,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"f48da103-66c7-4578-d11d-14fed0d88af2"},"source":["print(metrics.flat_accuracy_score(val_tags, y_val_pred))"],"id":"_-_2eRrbAsb1","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.7748409640713197\n"]}]},{"cell_type":"code","metadata":{"id":"mU9Awjy1B9ke","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638764305690,"user_tz":300,"elapsed":174,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"5e12bdb3-7460-412b-c20d-5896fd11dc7f"},"source":["CRF_show_confusion(list(np.concatenate(val_tags)), list(np.concatenate(y_val_pred)))"],"id":"mU9Awjy1B9ke","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["f1 micro:0.775\n","f1 micro:0.559\n","recall macro:0.539\n","Confusion Matrix:\n","[[ 324  109    1  248   12]\n"," [  71  462   43  482   14]\n"," [   0   54  407  224    9]\n"," [ 241  544  193 7335   27]\n"," [  37   29   29  146  120]]\n"]}]},{"cell_type":"code","metadata":{"id":"CCah_pdrrzGF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638764407802,"user_tz":300,"elapsed":121,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"ffc71b0e-5263-46fd-c773-6b74ef93bae8"},"source":["# labels results\n","labels = model_CRF_tacheA.classes_\n","print(labels)\n","print(metrics.flat_classification_report(\n","    y_val_pred, val_tags, labels=labels, digits=3\n","))"],"id":"CCah_pdrrzGF","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['O', 'B', 'L', 'U', 'I']\n","              precision    recall  f1-score   support\n","\n","           O      0.879     0.870     0.875      8435\n","           B      0.467     0.481     0.474       673\n","           L      0.586     0.605     0.595       673\n","           U      0.332     0.659     0.442       182\n","           I      0.431     0.386     0.407      1198\n","\n","    accuracy                          0.775     11161\n","   macro avg      0.539     0.600     0.559     11161\n","weighted avg      0.780     0.775     0.776     11161\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:70: FutureWarning: Pass labels=['O', 'B', 'L', 'U', 'I'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n","  FutureWarning)\n"]}]},{"cell_type":"markdown","metadata":{"id":"7zq64K3ALtpt"},"source":["***Remarque***\n","\n","Le CRF simple nous donne des résultats assez médiocres, on ne s'attendait pas à ce qu'il dépasse les state of the art mais nous l'avons implémenté pour évaluer le modèle et le tester tout simplement. On remarque que le déséquilibre du jeu de données se ressent dans les résultats avec plus de 'O' prédis. Par contre le CRF propose des résultats qui restent correctes."],"id":"7zq64K3ALtpt"},{"cell_type":"markdown","metadata":{"id":"uLfmtsFD848-"},"source":["#### Submission file for CRF"],"id":"uLfmtsFD848-"},{"cell_type":"code","metadata":{"id":"8x79yLn084t6","executionInfo":{"status":"ok","timestamp":1638766076099,"user_tz":300,"elapsed":115,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}}},"source":["\n","y_val_csv=list(np.concatenate(y_val_pred))\n","\n","CRF_val_df_soumission=val_df.drop(['DocID', 'Token'], 1)\n","CRF_val_df_soumission['Tag']=y_val_csv\n","CRF_val_df_soumission\n","\n","CRF_val_df_soumission.to_csv('CRF_submission_val_A.csv', index=False, encoding='utf-8')\n"],"id":"8x79yLn084t6","execution_count":102,"outputs":[]},{"cell_type":"code","metadata":{"id":"XsY0Kq-PpKff","executionInfo":{"status":"ok","timestamp":1638766077895,"user_tz":300,"elapsed":148,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}}},"source":["y_test_csv=list(np.concatenate(y_test_pred))\n","\n","CRF_test_df_soumission=test_df.drop(['DocID', 'Token'], 1)\n","CRF_test_df_soumission['Tag']=y_test_csv\n","CRF_test_df_soumission\n","\n","CRF_test_df_soumission.to_csv('CRF_submission_test_A.csv', index=False, encoding='utf-8')"],"id":"XsY0Kq-PpKff","execution_count":103,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"suWHQgQetICM"},"source":["### BI LSTM et Glove-Bilstm-CRF"],"id":"suWHQgQetICM"},{"cell_type":"markdown","metadata":{"id":"fBMx7U-oJ5_A"},"source":["#### Prétraitement"],"id":"fBMx7U-oJ5_A"},{"cell_type":"code","metadata":{"id":"IxSJ9OAv-MKa"},"source":["# we define the words through numbers/index to save space\n","words, tags = set(), set()\n"," \n","for s in train_sentences:\n","  words = words.union(set(s))\n"," \n","for ts in train_tags:\n","  tags = tags.union(set(ts))\n"," \n","word2index = {w: i + 2 for i, w in enumerate(list(words))}\n","word2index['-PAD-'] = 0  # The special value used for padding\n","word2index['-OOV-'] = 1  # The special value used for OOVs\n"," \n","# defining tag to index mapping\n","tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n","tag2index['-PAD-'] = 0  # The special value used to padding\n","index2tag={value: key for (key, value) in tag2index.items()}\n","\n","print(tag2index)\n","print(index2tag)"],"id":"IxSJ9OAv-MKa","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VxgvjHZcCMRW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638559970143,"user_tz":300,"elapsed":28,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"68c8c589-ac83-4560-a22f-f470ed041ef0"},"source":["def get_sentence_X(sentences):\n","  sentences_X = []\n","  for s in sentences:\n","    s_int = []\n","    for w in s:\n","        try:\n","            s_int.append(word2index[w])\n","        except KeyError:\n","            s_int.append(word2index['-OOV-'])\n"," \n","    sentences_X.append(s_int)\n","  return sentences_X\n","  \n","train_sentences_X_no_pad = get_sentence_X(train_sentences)\n","val_sentences_X_no_pad   = get_sentence_X(val_sentences)\n","test_sentences_X_no_pad  = get_sentence_X(test_sentences)\n","\n","train_tags_y_no_pad = [[tag2index[t] for t in s] for s in train_tags]\n","val_tags_y_no_pad   = [[tag2index[t] for t in s] for s in val_tags]\n"," \n","print(train_sentences_X_no_pad[0])\n","print(val_sentences_X_no_pad[0])\n","print(train_tags_y_no_pad[0])\n","print(val_tags_y_no_pad[0])\n","\n","print('\\n',test_sentences_X_no_pad[0])"],"id":"VxgvjHZcCMRW","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[5612, 1238, 1447, 8464, 4901, 5266, 3634, 3814, 6457, 4901, 5266, 7145, 5148, 6393, 7371, 8672, 849, 6328, 5282, 5222, 4902, 4903, 2228, 2304, 8268, 6421, 1447, 3318, 8989, 1291, 8672, 2408, 7447, 7931, 7625, 2614, 4901, 454, 2637, 7931, 806, 2614, 4267, 8528, 6941, 2304, 8268, 6421, 1447, 3886, 5148, 8189, 8984, 6602, 6941, 8989, 1291, 8672, 3718, 1087, 4563, 7931, 4831, 2614, 4901, 5675, 8953, 7081, 7931, 4611, 2614, 4267, 8672, 746, 4657, 6838, 1238, 5489, 4903, 6941, 2304, 5664, 4903, 2902, 1733, 8984, 6602, 6941, 2721, 9001, 5466, 2228, 4901, 8984, 3415, 1896, 8672, 3494, 4656, 4901, 4678, 4052, 8127, 8856, 4360, 4901, 192, 8044, 3830, 7082, 8672, 4393, 1292, 949, 140, 6941, 2304, 8268, 6602, 6941, 1923, 3886, 5148, 6602, 4077, 4985, 192, 2073, 4901, 1291, 8672, 6754, 5148, 3453, 4901, 5148, 8268, 3546, 408, 5282, 5222, 153, 8888, 8672, 2843, 6941, 192, 7275, 5148, 5282, 9001, 4550, 4286, 5148, 1195, 8672, 1538, 2304, 8285, 1089, 9034, 3419, 8942, 8463, 3569, 1956, 8989, 8528, 4589, 4267, 7413, 8528, 7289, 408, 8674, 1630, 7931, 4611, 2614, 8672, 4690, 4901, 6328, 7214, 259, 5222, 2385, 8127, 2435, 5148, 1470, 8127, 2601, 5266, 1616, 1500, 7931, 3522, 2614, 8528, 361, 755, 7931, 2057, 2614, 8672]\n","[440, 8127, 1, 1, 2169, 6602, 4872, 1, 4901, 6602, 4457, 5282, 5445, 4903, 4475, 8711, 8672, 1, 1448, 1089, 2580, 5266, 3802, 6235, 3353, 6602, 1, 8011, 8989, 1, 4267, 192, 8044, 1, 109, 4475, 3816, 2016, 259, 1533, 6765, 5259, 8127, 5526, 4901, 1, 7110, 8528, 4569, 3419, 8229, 1, 7931, 1, 2614, 4901, 3605, 6602, 8723, 2391, 6328, 919, 1447, 6673, 606, 3023, 7931, 1, 2614, 8672, 2843, 1018, 810, 4594, 1, 2391, 6602, 1, 4901, 1, 5148, 1, 1, 1447, 7928, 1, 4901, 1, 5148, 1, 4901, 8140, 8989, 2105, 2302, 8672, 3071, 4267, 4901, 5033, 6602, 810, 4594, 7572, 2391, 6602, 1, 8011, 192, 4189, 88, 4901, 7928, 1, 7931, 575, 2614, 8672, 1, 192, 6814, 5190, 4651, 1, 1, 1491, 1, 5148, 1639, 1, 8672, 3146, 192, 7026, 5154, 5077, 3353, 6602, 4457, 7900, 5222, 4986, 8127, 6765, 4118, 7295, 5266, 1, 919, 8672]\n","[1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 4, 4, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 4, 2, 5, 5, 5, 1, 2, 5, 5, 5, 5, 1, 2, 5, 5, 5, 5, 5, 1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 4, 2, 5, 5, 5, 5, 1, 4, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 2, 5, 5, 5, 5, 1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 4, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 4, 2, 5, 5, 5, 5, 5, 5, 5, 1, 2, 5, 5, 5, 5, 5, 5, 3, 5, 5, 1, 4, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n","[5, 5, 1, 2, 5, 5, 1, 2, 5, 5, 5, 5, 5, 5, 1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 2, 5, 3, 5, 5, 5, 5, 5, 1, 2, 5, 5, 5, 5, 3, 5, 3, 5, 1, 2, 5, 3, 5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 4, 4, 4, 4, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 5, 3, 5, 3, 5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 4, 4, 4, 4, 2, 5]\n","\n"," [1, 6859, 6521, 1, 8989, 1, 4267, 2794, 2169, 1, 1, 1, 6246, 5113, 8984, 3415, 3546, 8579, 8516, 8127, 1, 5266, 2659, 1, 8672, 2843, 1, 5185, 2391, 1, 5266, 5190, 1, 1, 6246, 1959, 4819, 3621, 5190, 4429, 5642, 2391, 6602, 3282, 2446, 8638, 8672, 888, 5185, 6246, 1838, 8127, 7790, 1, 2650, 8989, 1, 4267, 1616, 5185, 4903, 8044, 1362, 2391, 8393, 1, 3344, 3569, 4429, 8891, 5148, 5697, 3253, 8989, 1, 750, 5393, 4901, 950, 4901, 1, 4901, 7889, 4901, 3460, 4267, 8672, 1279, 7357, 3703, 6287, 4673, 34, 841, 2169, 6602, 1, 1616, 8672, 1, 5148, 7477, 1762, 4903, 1525, 2203, 8672, 2843, 1, 1, 34, 8400, 6271, 8127, 5190, 4429, 1, 8989, 1, 1, 6199, 1, 1, 4901, 1, 1, 4901, 1, 4901, 3460, 4267, 8672, 2843, 2644, 2391, 6602, 5430, 6230, 34, 1, 4901, 5148, 6602, 4429, 1, 2446, 5642, 34, 7436, 3569, 1865, 3813, 8989, 1, 4267, 8672, 8993, 1, 5266, 3453, 4819, 5190, 6632, 8793, 4901, 6602, 1616, 2391, 6602, 4429, 1, 5642, 34, 1, 3622, 8816, 3888, 1, 8672]\n"]}]},{"cell_type":"code","metadata":{"id":"6K1Zj_yhCiLW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638559970430,"user_tz":300,"elapsed":308,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"0e341264-3145-49eb-8d28-51f8a29ef420"},"source":["# defininf the length of the longest sentence in the whole dataset\n","MAX_LENGTH_train = len(max(train_sentences_X_no_pad, key=len))\n","MAX_LENGTH_val = len(max(val_sentences_X_no_pad, key=len))\n","MAX_LENGTH_test = len(max(test_sentences_X_no_pad, key=len))\n","MAX_LENGTH=max(MAX_LENGTH_train , MAX_LENGTH_val,MAX_LENGTH_test)\n","print(MAX_LENGTH)"],"id":"6K1Zj_yhCiLW","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["350\n"]}]},{"cell_type":"code","metadata":{"id":"5qKoQmy9Cm8w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638559972035,"user_tz":300,"elapsed":1611,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"bc0bd681-49aa-4c5e-fc7e-29da2d7e5933"},"source":["# Pad the sentences for bilstm entry\n"," \n","train_sentences_X = pad_sequences(train_sentences_X_no_pad, maxlen=MAX_LENGTH, padding='post')\n","val_sentences_X = pad_sequences(val_sentences_X_no_pad, maxlen=MAX_LENGTH, padding='post')\n","test_sentences_X = pad_sequences(test_sentences_X_no_pad, maxlen=MAX_LENGTH, padding='post')\n","\n","train_tags_y = pad_sequences(train_tags_y_no_pad, maxlen=MAX_LENGTH, padding='post')\n","val_tags_y = pad_sequences(val_tags_y_no_pad, maxlen=MAX_LENGTH, padding='post')\n","\n","train_tags_y = to_categorical(train_tags_y)\n","val_tags_y = to_categorical(val_tags_y)\n","\n","print(train_sentences_X[0])\n","print(train_tags_y[0])"],"id":"5qKoQmy9Cm8w","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[5612 1238 1447 8464 4901 5266 3634 3814 6457 4901 5266 7145 5148 6393\n"," 7371 8672  849 6328 5282 5222 4902 4903 2228 2304 8268 6421 1447 3318\n"," 8989 1291 8672 2408 7447 7931 7625 2614 4901  454 2637 7931  806 2614\n"," 4267 8528 6941 2304 8268 6421 1447 3886 5148 8189 8984 6602 6941 8989\n"," 1291 8672 3718 1087 4563 7931 4831 2614 4901 5675 8953 7081 7931 4611\n"," 2614 4267 8672  746 4657 6838 1238 5489 4903 6941 2304 5664 4903 2902\n"," 1733 8984 6602 6941 2721 9001 5466 2228 4901 8984 3415 1896 8672 3494\n"," 4656 4901 4678 4052 8127 8856 4360 4901  192 8044 3830 7082 8672 4393\n"," 1292  949  140 6941 2304 8268 6602 6941 1923 3886 5148 6602 4077 4985\n","  192 2073 4901 1291 8672 6754 5148 3453 4901 5148 8268 3546  408 5282\n"," 5222  153 8888 8672 2843 6941  192 7275 5148 5282 9001 4550 4286 5148\n"," 1195 8672 1538 2304 8285 1089 9034 3419 8942 8463 3569 1956 8989 8528\n"," 4589 4267 7413 8528 7289  408 8674 1630 7931 4611 2614 8672 4690 4901\n"," 6328 7214  259 5222 2385 8127 2435 5148 1470 8127 2601 5266 1616 1500\n"," 7931 3522 2614 8528  361  755 7931 2057 2614 8672    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n","[[0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1.]\n"," ...\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"p9JN2A0MBnO9"},"source":["#### Simple Bilstm"],"id":"p9JN2A0MBnO9"},{"cell_type":"markdown","metadata":{"id":"1fIhXc9kCFHg"},"source":["##### architecture du model"],"id":"1fIhXc9kCFHg"},{"cell_type":"code","metadata":{"id":"a-8hvaKGzvaN"},"source":["def create_model(weights, optimizer):\n","\n","  model = Sequential()\n","  model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n","  model.add(Embedding(len(word2index), 128))\n","  model.add(Bidirectional(LSTM(256, return_sequences=True)))\n","  model.add(TimeDistributed(Dense(len(tag2index))))\n","  model.add(Activation('softmax'))\n","  \n","  model.compile(loss=weighted_categorical_crossentropy(weights),\n","                optimizer=optimizer,\n","                metrics=['accuracy', get_f1_micro, get_f1_macro],\n","                run_eagerly=True)\n","  return model"],"id":"a-8hvaKGzvaN","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2avuUzlpCKYZ"},"source":["##### benchmark/grid search"],"id":"2avuUzlpCKYZ"},{"cell_type":"code","metadata":{"id":"FzCAdgOaEb31"},"source":["'''\n","tag2weights_list = [\n","                    {\"-PAD-\":1, 'O':1, 'U':1, 'I':1, 'B':1, 'L':1},\n","                    {\"-PAD-\":1, 'O':1, 'U':2, 'I':2, 'B':2, 'L':2},\n","                    {\"-PAD-\":1, 'O':1, 'U':10, 'I':10, 'B':10, 'L':10}\n","]\n","all_weights = [np.array([tag2weights[index2tag[i]] for i in range(len(tag2index))]) for tag2weights in tag2weights_list]\n","optimizers = [Adam(), SGD()]\n","models = [create_model(weights, optimizer) for weights in all_weights for optimizer in optimizers]\n","histories = []\n","for model in tqdm(models):\n","  histories.append(model.fit(train_sentences_X, train_tags_y, batch_size=32, epochs=15, validation_data=(val_sentences_X,val_tags_y), verbose=0))\n","metric_names = ['loss', 'accuracy']\n","params = [([tag2weights[index2tag[i]] for i in range(len(tag2index))], optimizer) for tag2weights in tag2weights_list for optimizer in [\"adam\", 'sgd']]\n","print_training(histories, metric_names, params)\n","#predictions_test = mymodel.predict(test_sentences_X)\n","'''"],"id":"FzCAdgOaEb31","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wx5xWdEHCSEr"},"source":["#### Bilstm-CRF avec glove embedding"],"id":"Wx5xWdEHCSEr"},{"cell_type":"code","metadata":{"id":"a69iq6kJmIuy"},"source":["# fex parameters\n","MAX_LEN = MAX_LENGTH  # Max length of words\n","WORD_EMBEDDING_OUT_DIM = 300\n","\n","tags_A = tag2index\n","n_tags_A = len(tags_A)\n","words_voc_A=list(set(train_df[\"Token\"].values))\n","n_words_A = len(words_voc_A) +2\n","max_word_len_A = len(max(words_voc_A[1:], key=len))"],"id":"a69iq6kJmIuy","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0aFM8_0SDU9E"},"source":["##### Definition de la matrice d'embedding Glove"],"id":"0aFM8_0SDU9E"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IATEKOA4mOjm","executionInfo":{"status":"ok","timestamp":1638743693215,"user_tz":300,"elapsed":288,"user":{"displayName":"lamia2 salhi2","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14440555040870778846"}},"outputId":"2a2c88c4-a0b7-4f96-a595-babb52b9796f"},"source":["# definition of the embedding matrix\n","words_not_found = []\n","\n","embedding_matrix = np.zeros((n_words_A, WORD_EMBEDDING_OUT_DIM))\n","\n","for word, i in word2index.items():\n","    if i >= n_words_A:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if (embedding_vector is not None) and len(embedding_vector) > 0:\n","        # words not found in embedding index will be all-zeros.\n","        embedding_matrix[i] = embedding_vector\n","    else:\n","        words_not_found.append(word)\n","print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"],"id":"IATEKOA4mOjm","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of null word embeddings: 1302\n"]}]},{"cell_type":"markdown","metadata":{"id":"vud2L5uMfrS1"},"source":["##### Définition du modéle Bilstm+Crf"],"id":"vud2L5uMfrS1"},{"cell_type":"markdown","metadata":{"id":"ENrZe-txzwuN"},"source":["On utilise la librairie tf2crf de pypi, qui est est un module permettant d'implémenter un layer crf sur tensorflow 2. Ici on utilise la fonction  ModelWithCRFLossDSCLoss qui permet d'uitiliser la fonction de cout DSC (Dice similarity coefficient) qui est trés pratique dans le cadre des données qui sont déséquilibrée. On utilise également de l'early stopping et un checkpoint conservant le meilleur modèle en terme d'accuracy sur le validation set. Finalement, un embedding glove est utilisé pour augementer nos performances. On compare également le GRU et le LSTM comme layers."],"id":"ENrZe-txzwuN"},{"cell_type":"code","metadata":{"id":"jT3T7xkcfrS1"},"source":["def create_model_Bilstm_crf_A( optimizer,embedding_name,layer,dropout, unit):\n","  inputs = Input(shape=(MAX_LEN, ))\n","  if embedding_name == 'Glove':\n","    x = Embedding(input_dim=n_words_A, output_dim=WORD_EMBEDDING_OUT_DIM, input_length=MAX_LEN,weights=[embedding_matrix])(inputs)\n","  else:\n","    x = Embedding(input_dim=n_words_A, output_dim=WORD_EMBEDDING_OUT_DIM, input_length=MAX_LEN)(inputs)\n","\n","  if layer == 'GRU':\n","    x = Bidirectional(GRU(units=unit, return_sequences=True, dropout=dropout))(x)\n","  else:\n","    x = Bidirectional(LSTM(units=unit, return_sequences=True, dropout=dropout))(x)\n","  crf = CRF(units=n_tags_A)\n","  output = crf(x)\n","  base_model = Model(inputs, output)\n","  model_crf = ModelWithCRFLossDSCLoss(base_model,sparse_target=False)\n","  model_crf.compile(optimizer=optimizer)\n","  return model_crf"],"id":"jT3T7xkcfrS1","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ibfNeGD7CkXf"},"source":["##### Grid search sur plusieurs données "],"id":"ibfNeGD7CkXf"},{"cell_type":"markdown","metadata":{"id":"6dflOCpjzpR5"},"source":["Ici on lance un grid search avec plusieurs paramétres différents "],"id":"6dflOCpjzpR5"},{"cell_type":"code","metadata":{"id":"nURFEtANfrS2"},"source":["optimizers = ['adam'] #['adam','rmsprop','SGD']#['rmsprop','adam','SGD','adadelta']#['Adam()', 'SGD()', 'RMSprop()','Adagrad()','Adadelta()']\n","all_dropouts = [0.3]\n","units = [10,64] #[10, 64, 150, 250]\n","embeddin_meth = ['Glove', None]#['Glove', None] \n","models_bilstm_crf_param_A = [[optimizer,embedding_name,layer, dropout, unit] for unit in units for layer in ['GRU', 'LSTM'] for dropout in all_dropouts for optimizer in optimizers for embedding_name in  embeddin_meth ]\n","models_bilstm_crf_A= [create_model_Bilstm_crf_A( optimizer,embedding_name,layer, dropout, unit) for unit in units for layer in ['GRU', 'LSTM'] for dropout in all_dropouts for optimizer in optimizers for embedding_name in  embeddin_meth ]\n"],"id":"nURFEtANfrS2","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cnd8ILU7y-A3","executionInfo":{"status":"ok","timestamp":1638755871584,"user_tz":300,"elapsed":1907876,"user":{"displayName":"lamia2 salhi2","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14440555040870778846"}},"outputId":"0e061044-50fe-4b47-e359-d95d899a99b0"},"source":["callback2_A = EarlyStopping(monitor='val_loss_val', patience=3)\n","histories_A_Bilstm_crf = []\n","for i,model in tqdm(enumerate(models_bilstm_crf_A)):\n","  checkpoint2_A = ModelCheckpoint(\"./models/Bilstm_crf/best_model_bilstm_A\"+str(i)+\".hdf5\", monitor='val_val_accuracy', verbose=1,save_best_only=True, mode='auto', save_weights_only=True)\n","  histories_A_Bilstm_crf.append(model.fit(train_sentences_X, train_tags_y, batch_size=32, epochs=50, validation_data=(val_sentences_X,val_tags_y),callbacks=[checkpoint2_A,callback2_A],verbose=1))\n"],"id":"Cnd8ILU7y-A3","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r0it [00:00, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","11/11 [==============================] - ETA: 0s - loss: 408.1556 - accuracy: 0.8245\n","Epoch 00001: val_val_accuracy improved from -inf to 0.85314, saving model to ./models/Bilstm_crf/best_model_bilstm_A0.hdf5\n","11/11 [==============================] - 27s 872ms/step - loss: 408.1556 - accuracy: 0.8245 - val_loss_val: 319.8029 - val_val_accuracy: 0.8531\n","Epoch 2/50\n","11/11 [==============================] - ETA: 0s - loss: 270.2895 - accuracy: 0.8632\n","Epoch 00002: val_val_accuracy improved from 0.85314 to 0.85428, saving model to ./models/Bilstm_crf/best_model_bilstm_A0.hdf5\n","11/11 [==============================] - 8s 717ms/step - loss: 270.2895 - accuracy: 0.8632 - val_loss_val: 220.0158 - val_val_accuracy: 0.8543\n","Epoch 3/50\n","11/11 [==============================] - ETA: 0s - loss: 185.2820 - accuracy: 0.8650\n","Epoch 00003: val_val_accuracy improved from 0.85428 to 0.85495, saving model to ./models/Bilstm_crf/best_model_bilstm_A0.hdf5\n","11/11 [==============================] - 8s 715ms/step - loss: 185.2820 - accuracy: 0.8650 - val_loss_val: 173.3734 - val_val_accuracy: 0.8549\n","Epoch 4/50\n","11/11 [==============================] - ETA: 0s - loss: 149.2076 - accuracy: 0.8662\n","Epoch 00004: val_val_accuracy improved from 0.85495 to 0.85655, saving model to ./models/Bilstm_crf/best_model_bilstm_A0.hdf5\n","11/11 [==============================] - 8s 709ms/step - loss: 149.2076 - accuracy: 0.8662 - val_loss_val: 157.4285 - val_val_accuracy: 0.8565\n","Epoch 5/50\n","11/11 [==============================] - ETA: 0s - loss: 134.7961 - accuracy: 0.8685\n","Epoch 00005: val_val_accuracy improved from 0.85655 to 0.85768, saving model to ./models/Bilstm_crf/best_model_bilstm_A0.hdf5\n","11/11 [==============================] - 8s 714ms/step - loss: 134.7961 - accuracy: 0.8685 - val_loss_val: 147.2958 - val_val_accuracy: 0.8577\n","Epoch 6/50\n","11/11 [==============================] - ETA: 0s - loss: 125.6658 - accuracy: 0.8711\n","Epoch 00006: val_val_accuracy improved from 0.85768 to 0.85933, saving model to ./models/Bilstm_crf/best_model_bilstm_A0.hdf5\n","11/11 [==============================] - 8s 708ms/step - loss: 125.6658 - accuracy: 0.8711 - val_loss_val: 140.4553 - val_val_accuracy: 0.8593\n","Epoch 7/50\n","11/11 [==============================] - ETA: 0s - loss: 119.0472 - accuracy: 0.8732\n","Epoch 00007: val_val_accuracy improved from 0.85933 to 0.86067, saving model to ./models/Bilstm_crf/best_model_bilstm_A0.hdf5\n","11/11 [==============================] - 8s 705ms/step - loss: 119.0472 - accuracy: 0.8732 - val_loss_val: 135.0703 - val_val_accuracy: 0.8607\n","Epoch 8/50\n","11/11 [==============================] - ETA: 0s - loss: 112.8316 - accuracy: 0.8778\n","Epoch 00008: val_val_accuracy did not improve from 0.86067\n","11/11 [==============================] - 7s 678ms/step - loss: 112.8316 - accuracy: 0.8778 - val_loss_val: 131.3474 - val_val_accuracy: 0.8599\n","Epoch 9/50\n","11/11 [==============================] - ETA: 0s - loss: 107.3018 - accuracy: 0.8819\n","Epoch 00009: val_val_accuracy improved from 0.86067 to 0.86361, saving model to ./models/Bilstm_crf/best_model_bilstm_A0.hdf5\n","11/11 [==============================] - 8s 709ms/step - loss: 107.3018 - accuracy: 0.8819 - val_loss_val: 127.4926 - val_val_accuracy: 0.8636\n","Epoch 10/50\n","11/11 [==============================] - ETA: 0s - loss: 102.4262 - accuracy: 0.8852\n","Epoch 00010: val_val_accuracy improved from 0.86361 to 0.86515, saving model to ./models/Bilstm_crf/best_model_bilstm_A0.hdf5\n","11/11 [==============================] - 11s 1s/step - loss: 102.4262 - accuracy: 0.8852 - val_loss_val: 123.3809 - val_val_accuracy: 0.8652\n","Epoch 11/50\n","11/11 [==============================] - ETA: 0s - loss: 97.5829 - accuracy: 0.8904\n","Epoch 00011: val_val_accuracy improved from 0.86515 to 0.86691, saving model to ./models/Bilstm_crf/best_model_bilstm_A0.hdf5\n","11/11 [==============================] - 8s 712ms/step - loss: 97.5829 - accuracy: 0.8904 - val_loss_val: 121.0134 - val_val_accuracy: 0.8669\n","Epoch 12/50\n","11/11 [==============================] - ETA: 0s - loss: 93.2918 - accuracy: 0.8953\n","Epoch 00012: val_val_accuracy improved from 0.86691 to 0.86753, saving model to ./models/Bilstm_crf/best_model_bilstm_A0.hdf5\n","11/11 [==============================] - 8s 711ms/step - loss: 93.2918 - accuracy: 0.8953 - val_loss_val: 118.9325 - val_val_accuracy: 0.8675\n","Epoch 13/50\n","11/11 [==============================] - ETA: 0s - loss: 88.7594 - accuracy: 0.9018\n","Epoch 00013: val_val_accuracy improved from 0.86753 to 0.86840, saving model to ./models/Bilstm_crf/best_model_bilstm_A0.hdf5\n","11/11 [==============================] - 8s 719ms/step - loss: 88.7594 - accuracy: 0.9018 - val_loss_val: 116.6680 - val_val_accuracy: 0.8684\n","Epoch 14/50\n","11/11 [==============================] - ETA: 0s - loss: 84.7835 - accuracy: 0.9064\n","Epoch 00014: val_val_accuracy improved from 0.86840 to 0.87299, saving model to ./models/Bilstm_crf/best_model_bilstm_A0.hdf5\n","11/11 [==============================] - 8s 708ms/step - loss: 84.7835 - accuracy: 0.9064 - val_loss_val: 114.5204 - val_val_accuracy: 0.8730\n","Epoch 15/50\n","11/11 [==============================] - ETA: 0s - loss: 81.2092 - accuracy: 0.9106\n","Epoch 00015: val_val_accuracy did not improve from 0.87299\n","11/11 [==============================] - 8s 689ms/step - loss: 81.2092 - accuracy: 0.9106 - val_loss_val: 112.0115 - val_val_accuracy: 0.8713\n","Epoch 16/50\n","11/11 [==============================] - ETA: 0s - loss: 77.1020 - accuracy: 0.9165\n","Epoch 00016: val_val_accuracy did not improve from 0.87299\n","11/11 [==============================] - 7s 680ms/step - loss: 77.1020 - accuracy: 0.9165 - val_loss_val: 111.9755 - val_val_accuracy: 0.8712\n","Epoch 17/50\n","11/11 [==============================] - ETA: 0s - loss: 73.2263 - accuracy: 0.9214\n","Epoch 00017: val_val_accuracy did not improve from 0.87299\n","11/11 [==============================] - 7s 676ms/step - loss: 73.2263 - accuracy: 0.9214 - val_loss_val: 110.0395 - val_val_accuracy: 0.8729\n","Epoch 18/50\n","11/11 [==============================] - ETA: 0s - loss: 70.1145 - accuracy: 0.9259\n","Epoch 00018: val_val_accuracy did not improve from 0.87299\n","11/11 [==============================] - 7s 676ms/step - loss: 70.1145 - accuracy: 0.9259 - val_loss_val: 108.2439 - val_val_accuracy: 0.8726\n","Epoch 19/50\n","11/11 [==============================] - ETA: 0s - loss: 67.0302 - accuracy: 0.9302\n","Epoch 00019: val_val_accuracy improved from 0.87299 to 0.87376, saving model to ./models/Bilstm_crf/best_model_bilstm_A0.hdf5\n","11/11 [==============================] - 9s 814ms/step - loss: 67.0302 - accuracy: 0.9302 - val_loss_val: 107.6265 - val_val_accuracy: 0.8738\n","Epoch 20/50\n","11/11 [==============================] - ETA: 0s - loss: 64.1392 - accuracy: 0.9339\n","Epoch 00020: val_val_accuracy improved from 0.87376 to 0.87428, saving model to ./models/Bilstm_crf/best_model_bilstm_A0.hdf5\n","11/11 [==============================] - 8s 709ms/step - loss: 64.1392 - accuracy: 0.9339 - val_loss_val: 106.4183 - val_val_accuracy: 0.8743\n","Epoch 21/50\n","11/11 [==============================] - ETA: 0s - loss: 61.3323 - accuracy: 0.9365\n","Epoch 00021: val_val_accuracy did not improve from 0.87428\n","11/11 [==============================] - 8s 685ms/step - loss: 61.3323 - accuracy: 0.9365 - val_loss_val: 105.2593 - val_val_accuracy: 0.8731\n","Epoch 22/50\n","11/11 [==============================] - ETA: 0s - loss: 58.5908 - accuracy: 0.9409\n","Epoch 00022: val_val_accuracy did not improve from 0.87428\n","11/11 [==============================] - 7s 676ms/step - loss: 58.5908 - accuracy: 0.9409 - val_loss_val: 105.1700 - val_val_accuracy: 0.8741\n","Epoch 23/50\n","11/11 [==============================] - ETA: 0s - loss: 55.9540 - accuracy: 0.9433\n","Epoch 00023: val_val_accuracy did not improve from 0.87428\n","11/11 [==============================] - 7s 679ms/step - loss: 55.9540 - accuracy: 0.9433 - val_loss_val: 105.2442 - val_val_accuracy: 0.8742\n","Epoch 24/50\n","11/11 [==============================] - ETA: 0s - loss: 53.9516 - accuracy: 0.9466\n","Epoch 00024: val_val_accuracy improved from 0.87428 to 0.87479, saving model to ./models/Bilstm_crf/best_model_bilstm_A0.hdf5\n","11/11 [==============================] - 8s 716ms/step - loss: 53.9516 - accuracy: 0.9466 - val_loss_val: 105.0789 - val_val_accuracy: 0.8748\n","Epoch 25/50\n","11/11 [==============================] - ETA: 0s - loss: 51.4262 - accuracy: 0.9487\n","Epoch 00025: val_val_accuracy did not improve from 0.87479\n","11/11 [==============================] - 8s 686ms/step - loss: 51.4262 - accuracy: 0.9487 - val_loss_val: 105.1767 - val_val_accuracy: 0.8711\n","Epoch 26/50\n","11/11 [==============================] - ETA: 0s - loss: 49.4750 - accuracy: 0.9525\n","Epoch 00026: val_val_accuracy did not improve from 0.87479\n","11/11 [==============================] - 7s 673ms/step - loss: 49.4750 - accuracy: 0.9525 - val_loss_val: 105.2492 - val_val_accuracy: 0.8686\n","Epoch 27/50\n","11/11 [==============================] - ETA: 0s - loss: 47.8141 - accuracy: 0.9531\n","Epoch 00027: val_val_accuracy did not improve from 0.87479\n","11/11 [==============================] - 7s 680ms/step - loss: 47.8141 - accuracy: 0.9531 - val_loss_val: 104.1259 - val_val_accuracy: 0.8716\n","Epoch 28/50\n","11/11 [==============================] - ETA: 0s - loss: 45.5654 - accuracy: 0.9564\n","Epoch 00028: val_val_accuracy did not improve from 0.87479\n","11/11 [==============================] - 7s 676ms/step - loss: 45.5654 - accuracy: 0.9564 - val_loss_val: 105.2187 - val_val_accuracy: 0.8710\n","Epoch 29/50\n","11/11 [==============================] - ETA: 0s - loss: 43.6988 - accuracy: 0.9582\n","Epoch 00029: val_val_accuracy did not improve from 0.87479\n","11/11 [==============================] - 7s 675ms/step - loss: 43.6988 - accuracy: 0.9582 - val_loss_val: 105.1298 - val_val_accuracy: 0.8694\n","Epoch 30/50\n","11/11 [==============================] - ETA: 0s - loss: 42.5566 - accuracy: 0.9589\n","Epoch 00030: val_val_accuracy did not improve from 0.87479\n","11/11 [==============================] - 7s 678ms/step - loss: 42.5566 - accuracy: 0.9589 - val_loss_val: 106.9442 - val_val_accuracy: 0.8698\n"]},{"output_type":"stream","name":"stderr","text":["\r1it [04:12, 252.82s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","11/11 [==============================] - ETA: 0s - loss: 503.0184 - accuracy: 0.4878\n","Epoch 00001: val_val_accuracy improved from -inf to 0.42469, saving model to ./models/Bilstm_crf/best_model_bilstm_A1.hdf5\n","11/11 [==============================] - 17s 872ms/step - loss: 503.0184 - accuracy: 0.4878 - val_loss_val: 478.4823 - val_val_accuracy: 0.4247\n","Epoch 2/50\n","11/11 [==============================] - ETA: 0s - loss: 375.8264 - accuracy: 0.5276\n","Epoch 00002: val_val_accuracy improved from 0.42469 to 0.42572, saving model to ./models/Bilstm_crf/best_model_bilstm_A1.hdf5\n","11/11 [==============================] - 8s 691ms/step - loss: 375.8264 - accuracy: 0.5276 - val_loss_val: 379.1693 - val_val_accuracy: 0.4257\n","Epoch 3/50\n","11/11 [==============================] - ETA: 0s - loss: 288.1357 - accuracy: 0.5999\n","Epoch 00003: val_val_accuracy improved from 0.42572 to 0.76902, saving model to ./models/Bilstm_crf/best_model_bilstm_A1.hdf5\n","11/11 [==============================] - 8s 716ms/step - loss: 288.1357 - accuracy: 0.5999 - val_loss_val: 296.6234 - val_val_accuracy: 0.7690\n","Epoch 4/50\n","11/11 [==============================] - ETA: 0s - loss: 218.9512 - accuracy: 0.8637\n","Epoch 00004: val_val_accuracy improved from 0.76902 to 0.85082, saving model to ./models/Bilstm_crf/best_model_bilstm_A1.hdf5\n","11/11 [==============================] - 8s 713ms/step - loss: 218.9512 - accuracy: 0.8637 - val_loss_val: 222.0821 - val_val_accuracy: 0.8508\n","Epoch 5/50\n","11/11 [==============================] - ETA: 0s - loss: 169.0762 - accuracy: 0.8656\n","Epoch 00005: val_val_accuracy improved from 0.85082 to 0.85428, saving model to ./models/Bilstm_crf/best_model_bilstm_A1.hdf5\n","11/11 [==============================] - 8s 716ms/step - loss: 169.0762 - accuracy: 0.8656 - val_loss_val: 179.5650 - val_val_accuracy: 0.8543\n","Epoch 6/50\n","11/11 [==============================] - ETA: 0s - loss: 145.7827 - accuracy: 0.8653\n","Epoch 00006: val_val_accuracy improved from 0.85428 to 0.85464, saving model to ./models/Bilstm_crf/best_model_bilstm_A1.hdf5\n","11/11 [==============================] - 8s 714ms/step - loss: 145.7827 - accuracy: 0.8653 - val_loss_val: 163.2454 - val_val_accuracy: 0.8546\n","Epoch 7/50\n","11/11 [==============================] - ETA: 0s - loss: 132.0676 - accuracy: 0.8684\n","Epoch 00007: val_val_accuracy improved from 0.85464 to 0.85716, saving model to ./models/Bilstm_crf/best_model_bilstm_A1.hdf5\n","11/11 [==============================] - 8s 718ms/step - loss: 132.0676 - accuracy: 0.8684 - val_loss_val: 154.8576 - val_val_accuracy: 0.8572\n","Epoch 8/50\n","11/11 [==============================] - ETA: 0s - loss: 121.2176 - accuracy: 0.8807\n","Epoch 00008: val_val_accuracy improved from 0.85716 to 0.85923, saving model to ./models/Bilstm_crf/best_model_bilstm_A1.hdf5\n","11/11 [==============================] - 8s 719ms/step - loss: 121.2176 - accuracy: 0.8807 - val_loss_val: 149.8229 - val_val_accuracy: 0.8592\n","Epoch 9/50\n","11/11 [==============================] - ETA: 0s - loss: 112.6191 - accuracy: 0.8923\n","Epoch 00009: val_val_accuracy improved from 0.85923 to 0.86186, saving model to ./models/Bilstm_crf/best_model_bilstm_A1.hdf5\n","11/11 [==============================] - 8s 715ms/step - loss: 112.6191 - accuracy: 0.8923 - val_loss_val: 144.9431 - val_val_accuracy: 0.8619\n","Epoch 10/50\n","11/11 [==============================] - ETA: 0s - loss: 105.0566 - accuracy: 0.9000\n","Epoch 00010: val_val_accuracy improved from 0.86186 to 0.86418, saving model to ./models/Bilstm_crf/best_model_bilstm_A1.hdf5\n","11/11 [==============================] - 8s 712ms/step - loss: 105.0566 - accuracy: 0.9000 - val_loss_val: 141.0876 - val_val_accuracy: 0.8642\n","Epoch 11/50\n","11/11 [==============================] - ETA: 0s - loss: 98.2960 - accuracy: 0.9067\n","Epoch 00011: val_val_accuracy improved from 0.86418 to 0.86567, saving model to ./models/Bilstm_crf/best_model_bilstm_A1.hdf5\n","11/11 [==============================] - 8s 717ms/step - loss: 98.2960 - accuracy: 0.9067 - val_loss_val: 137.9512 - val_val_accuracy: 0.8657\n","Epoch 12/50\n","11/11 [==============================] - ETA: 0s - loss: 91.8609 - accuracy: 0.9144\n","Epoch 00012: val_val_accuracy improved from 0.86567 to 0.86742, saving model to ./models/Bilstm_crf/best_model_bilstm_A1.hdf5\n","11/11 [==============================] - 8s 712ms/step - loss: 91.8609 - accuracy: 0.9144 - val_loss_val: 135.9150 - val_val_accuracy: 0.8674\n","Epoch 13/50\n","11/11 [==============================] - ETA: 0s - loss: 86.0059 - accuracy: 0.9237\n","Epoch 00013: val_val_accuracy improved from 0.86742 to 0.86876, saving model to ./models/Bilstm_crf/best_model_bilstm_A1.hdf5\n","11/11 [==============================] - 8s 713ms/step - loss: 86.0059 - accuracy: 0.9237 - val_loss_val: 133.9197 - val_val_accuracy: 0.8688\n","Epoch 14/50\n","11/11 [==============================] - ETA: 0s - loss: 80.5370 - accuracy: 0.9306\n","Epoch 00014: val_val_accuracy improved from 0.86876 to 0.86964, saving model to ./models/Bilstm_crf/best_model_bilstm_A1.hdf5\n","11/11 [==============================] - 8s 713ms/step - loss: 80.5370 - accuracy: 0.9306 - val_loss_val: 132.2146 - val_val_accuracy: 0.8696\n","Epoch 15/50\n","11/11 [==============================] - ETA: 0s - loss: 75.3754 - accuracy: 0.9349\n","Epoch 00015: val_val_accuracy improved from 0.86964 to 0.87021, saving model to ./models/Bilstm_crf/best_model_bilstm_A1.hdf5\n","11/11 [==============================] - 8s 719ms/step - loss: 75.3754 - accuracy: 0.9349 - val_loss_val: 130.8894 - val_val_accuracy: 0.8702\n","Epoch 16/50\n","11/11 [==============================] - ETA: 0s - loss: 70.7845 - accuracy: 0.9399\n","Epoch 00016: val_val_accuracy did not improve from 0.87021\n","11/11 [==============================] - 8s 689ms/step - loss: 70.7845 - accuracy: 0.9399 - val_loss_val: 130.0584 - val_val_accuracy: 0.8688\n","Epoch 17/50\n","11/11 [==============================] - ETA: 0s - loss: 66.4935 - accuracy: 0.9439\n","Epoch 00017: val_val_accuracy improved from 0.87021 to 0.87041, saving model to ./models/Bilstm_crf/best_model_bilstm_A1.hdf5\n","11/11 [==============================] - 8s 723ms/step - loss: 66.4935 - accuracy: 0.9439 - val_loss_val: 129.1717 - val_val_accuracy: 0.8704\n","Epoch 18/50\n","11/11 [==============================] - ETA: 0s - loss: 62.5794 - accuracy: 0.9467\n","Epoch 00018: val_val_accuracy did not improve from 0.87041\n","11/11 [==============================] - 8s 687ms/step - loss: 62.5794 - accuracy: 0.9467 - val_loss_val: 128.8878 - val_val_accuracy: 0.8691\n","Epoch 19/50\n","11/11 [==============================] - ETA: 0s - loss: 59.1227 - accuracy: 0.9503\n","Epoch 00019: val_val_accuracy did not improve from 0.87041\n","11/11 [==============================] - 7s 681ms/step - loss: 59.1227 - accuracy: 0.9503 - val_loss_val: 128.1987 - val_val_accuracy: 0.8682\n","Epoch 20/50\n","11/11 [==============================] - ETA: 0s - loss: 56.0452 - accuracy: 0.9531\n","Epoch 00020: val_val_accuracy did not improve from 0.87041\n","11/11 [==============================] - 7s 681ms/step - loss: 56.0452 - accuracy: 0.9531 - val_loss_val: 128.3723 - val_val_accuracy: 0.8678\n","Epoch 21/50\n","11/11 [==============================] - ETA: 0s - loss: 53.0496 - accuracy: 0.9553\n","Epoch 00021: val_val_accuracy did not improve from 0.87041\n","11/11 [==============================] - 7s 678ms/step - loss: 53.0496 - accuracy: 0.9553 - val_loss_val: 128.2354 - val_val_accuracy: 0.8681\n","Epoch 22/50\n","11/11 [==============================] - ETA: 0s - loss: 50.4047 - accuracy: 0.9583\n","Epoch 00022: val_val_accuracy did not improve from 0.87041\n","11/11 [==============================] - 7s 679ms/step - loss: 50.4047 - accuracy: 0.9583 - val_loss_val: 128.3351 - val_val_accuracy: 0.8681\n"]},{"output_type":"stream","name":"stderr","text":["\r2it [07:38, 225.00s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","11/11 [==============================] - ETA: 0s - loss: 742.4615 - accuracy: 0.1056\n","Epoch 00001: val_val_accuracy improved from -inf to 0.41778, saving model to ./models/Bilstm_crf/best_model_bilstm_A2.hdf5\n","11/11 [==============================] - 17s 863ms/step - loss: 742.4615 - accuracy: 0.1056 - val_loss_val: 617.0889 - val_val_accuracy: 0.4178\n","Epoch 2/50\n","11/11 [==============================] - ETA: 0s - loss: 615.9537 - accuracy: 0.3342\n","Epoch 00002: val_val_accuracy improved from 0.41778 to 0.43000, saving model to ./models/Bilstm_crf/best_model_bilstm_A2.hdf5\n","11/11 [==============================] - 7s 674ms/step - loss: 615.9537 - accuracy: 0.3342 - val_loss_val: 503.7560 - val_val_accuracy: 0.4300\n","Epoch 3/50\n","11/11 [==============================] - ETA: 0s - loss: 482.8935 - accuracy: 0.4193\n","Epoch 00003: val_val_accuracy improved from 0.43000 to 0.83572, saving model to ./models/Bilstm_crf/best_model_bilstm_A2.hdf5\n","11/11 [==============================] - 7s 674ms/step - loss: 482.8935 - accuracy: 0.4193 - val_loss_val: 378.2153 - val_val_accuracy: 0.8357\n","Epoch 4/50\n","11/11 [==============================] - ETA: 0s - loss: 339.3403 - accuracy: 0.8564\n","Epoch 00004: val_val_accuracy improved from 0.83572 to 0.85201, saving model to ./models/Bilstm_crf/best_model_bilstm_A2.hdf5\n","11/11 [==============================] - 8s 697ms/step - loss: 339.3403 - accuracy: 0.8564 - val_loss_val: 277.2865 - val_val_accuracy: 0.8520\n","Epoch 5/50\n","11/11 [==============================] - ETA: 0s - loss: 243.4981 - accuracy: 0.8629\n","Epoch 00005: val_val_accuracy improved from 0.85201 to 0.85356, saving model to ./models/Bilstm_crf/best_model_bilstm_A2.hdf5\n","11/11 [==============================] - 8s 698ms/step - loss: 243.4981 - accuracy: 0.8629 - val_loss_val: 221.0386 - val_val_accuracy: 0.8536\n","Epoch 6/50\n","11/11 [==============================] - ETA: 0s - loss: 193.4892 - accuracy: 0.8646\n","Epoch 00006: val_val_accuracy improved from 0.85356 to 0.85464, saving model to ./models/Bilstm_crf/best_model_bilstm_A2.hdf5\n","11/11 [==============================] - 8s 701ms/step - loss: 193.4892 - accuracy: 0.8646 - val_loss_val: 192.9861 - val_val_accuracy: 0.8546\n","Epoch 7/50\n","11/11 [==============================] - ETA: 0s - loss: 168.1801 - accuracy: 0.8657\n","Epoch 00007: val_val_accuracy improved from 0.85464 to 0.85510, saving model to ./models/Bilstm_crf/best_model_bilstm_A2.hdf5\n","11/11 [==============================] - 8s 698ms/step - loss: 168.1801 - accuracy: 0.8657 - val_loss_val: 177.8651 - val_val_accuracy: 0.8551\n","Epoch 8/50\n","11/11 [==============================] - ETA: 0s - loss: 153.9174 - accuracy: 0.8673\n","Epoch 00008: val_val_accuracy improved from 0.85510 to 0.85526, saving model to ./models/Bilstm_crf/best_model_bilstm_A2.hdf5\n","11/11 [==============================] - 8s 695ms/step - loss: 153.9174 - accuracy: 0.8673 - val_loss_val: 167.9852 - val_val_accuracy: 0.8553\n","Epoch 9/50\n","11/11 [==============================] - ETA: 0s - loss: 144.0385 - accuracy: 0.8692\n","Epoch 00009: val_val_accuracy improved from 0.85526 to 0.85680, saving model to ./models/Bilstm_crf/best_model_bilstm_A2.hdf5\n","11/11 [==============================] - 8s 697ms/step - loss: 144.0385 - accuracy: 0.8692 - val_loss_val: 161.0492 - val_val_accuracy: 0.8568\n","Epoch 10/50\n","11/11 [==============================] - ETA: 0s - loss: 136.6276 - accuracy: 0.8721\n","Epoch 00010: val_val_accuracy improved from 0.85680 to 0.85856, saving model to ./models/Bilstm_crf/best_model_bilstm_A2.hdf5\n","11/11 [==============================] - 8s 694ms/step - loss: 136.6276 - accuracy: 0.8721 - val_loss_val: 155.1065 - val_val_accuracy: 0.8586\n","Epoch 11/50\n","11/11 [==============================] - ETA: 0s - loss: 130.4489 - accuracy: 0.8751\n","Epoch 00011: val_val_accuracy improved from 0.85856 to 0.86005, saving model to ./models/Bilstm_crf/best_model_bilstm_A2.hdf5\n","11/11 [==============================] - 8s 707ms/step - loss: 130.4489 - accuracy: 0.8751 - val_loss_val: 150.6991 - val_val_accuracy: 0.8601\n","Epoch 12/50\n","11/11 [==============================] - ETA: 0s - loss: 124.9412 - accuracy: 0.8791\n","Epoch 00012: val_val_accuracy improved from 0.86005 to 0.86201, saving model to ./models/Bilstm_crf/best_model_bilstm_A2.hdf5\n","11/11 [==============================] - 8s 700ms/step - loss: 124.9412 - accuracy: 0.8791 - val_loss_val: 146.8780 - val_val_accuracy: 0.8620\n","Epoch 13/50\n","11/11 [==============================] - ETA: 0s - loss: 120.5122 - accuracy: 0.8815\n","Epoch 00013: val_val_accuracy did not improve from 0.86201\n","11/11 [==============================] - 7s 671ms/step - loss: 120.5122 - accuracy: 0.8815 - val_loss_val: 143.6225 - val_val_accuracy: 0.8614\n","Epoch 14/50\n","11/11 [==============================] - ETA: 0s - loss: 115.8515 - accuracy: 0.8863\n","Epoch 00014: val_val_accuracy improved from 0.86201 to 0.86459, saving model to ./models/Bilstm_crf/best_model_bilstm_A2.hdf5\n","11/11 [==============================] - 8s 700ms/step - loss: 115.8515 - accuracy: 0.8863 - val_loss_val: 140.9211 - val_val_accuracy: 0.8646\n","Epoch 15/50\n","11/11 [==============================] - ETA: 0s - loss: 111.7748 - accuracy: 0.8897\n","Epoch 00015: val_val_accuracy improved from 0.86459 to 0.86716, saving model to ./models/Bilstm_crf/best_model_bilstm_A2.hdf5\n","11/11 [==============================] - 8s 704ms/step - loss: 111.7748 - accuracy: 0.8897 - val_loss_val: 137.8863 - val_val_accuracy: 0.8672\n","Epoch 16/50\n","11/11 [==============================] - ETA: 0s - loss: 107.9929 - accuracy: 0.8934\n","Epoch 00016: val_val_accuracy did not improve from 0.86716\n","11/11 [==============================] - 7s 669ms/step - loss: 107.9929 - accuracy: 0.8934 - val_loss_val: 136.5431 - val_val_accuracy: 0.8655\n","Epoch 17/50\n","11/11 [==============================] - ETA: 0s - loss: 104.3382 - accuracy: 0.8977\n","Epoch 00017: val_val_accuracy did not improve from 0.86716\n","11/11 [==============================] - 7s 670ms/step - loss: 104.3382 - accuracy: 0.8977 - val_loss_val: 134.5343 - val_val_accuracy: 0.8658\n","Epoch 18/50\n","11/11 [==============================] - ETA: 0s - loss: 100.9835 - accuracy: 0.9002\n","Epoch 00018: val_val_accuracy did not improve from 0.86716\n","11/11 [==============================] - 7s 666ms/step - loss: 100.9835 - accuracy: 0.9002 - val_loss_val: 132.5215 - val_val_accuracy: 0.8645\n","Epoch 19/50\n","11/11 [==============================] - ETA: 0s - loss: 97.6702 - accuracy: 0.9069\n","Epoch 00019: val_val_accuracy did not improve from 0.86716\n","11/11 [==============================] - 7s 664ms/step - loss: 97.6702 - accuracy: 0.9069 - val_loss_val: 132.1455 - val_val_accuracy: 0.8648\n","Epoch 20/50\n","11/11 [==============================] - ETA: 0s - loss: 94.5155 - accuracy: 0.9079\n","Epoch 00020: val_val_accuracy did not improve from 0.86716\n","11/11 [==============================] - 7s 670ms/step - loss: 94.5155 - accuracy: 0.9079 - val_loss_val: 130.4512 - val_val_accuracy: 0.8645\n","Epoch 21/50\n","11/11 [==============================] - ETA: 0s - loss: 91.2579 - accuracy: 0.9135\n","Epoch 00021: val_val_accuracy did not improve from 0.86716\n","11/11 [==============================] - 7s 666ms/step - loss: 91.2579 - accuracy: 0.9135 - val_loss_val: 129.1151 - val_val_accuracy: 0.8662\n","Epoch 22/50\n","11/11 [==============================] - ETA: 0s - loss: 88.1726 - accuracy: 0.9172\n","Epoch 00022: val_val_accuracy did not improve from 0.86716\n","11/11 [==============================] - 7s 667ms/step - loss: 88.1726 - accuracy: 0.9172 - val_loss_val: 128.4799 - val_val_accuracy: 0.8654\n","Epoch 23/50\n","11/11 [==============================] - ETA: 0s - loss: 85.3496 - accuracy: 0.9204\n","Epoch 00023: val_val_accuracy did not improve from 0.86716\n","11/11 [==============================] - 7s 677ms/step - loss: 85.3496 - accuracy: 0.9204 - val_loss_val: 127.5776 - val_val_accuracy: 0.8638\n","Epoch 24/50\n","11/11 [==============================] - ETA: 0s - loss: 82.7516 - accuracy: 0.9238\n","Epoch 00024: val_val_accuracy did not improve from 0.86716\n","11/11 [==============================] - 7s 677ms/step - loss: 82.7516 - accuracy: 0.9238 - val_loss_val: 126.9635 - val_val_accuracy: 0.8665\n","Epoch 25/50\n","11/11 [==============================] - ETA: 0s - loss: 79.8203 - accuracy: 0.9270\n","Epoch 00025: val_val_accuracy did not improve from 0.86716\n","11/11 [==============================] - 7s 664ms/step - loss: 79.8203 - accuracy: 0.9270 - val_loss_val: 126.7819 - val_val_accuracy: 0.8634\n","Epoch 26/50\n","11/11 [==============================] - ETA: 0s - loss: 77.4260 - accuracy: 0.9298\n","Epoch 00026: val_val_accuracy did not improve from 0.86716\n","11/11 [==============================] - 7s 662ms/step - loss: 77.4260 - accuracy: 0.9298 - val_loss_val: 127.7902 - val_val_accuracy: 0.8617\n","Epoch 27/50\n","11/11 [==============================] - ETA: 0s - loss: 74.6847 - accuracy: 0.9331\n","Epoch 00027: val_val_accuracy did not improve from 0.86716\n","11/11 [==============================] - 7s 665ms/step - loss: 74.6847 - accuracy: 0.9331 - val_loss_val: 126.3179 - val_val_accuracy: 0.8632\n","Epoch 28/50\n","11/11 [==============================] - ETA: 0s - loss: 72.5325 - accuracy: 0.9350\n","Epoch 00028: val_val_accuracy did not improve from 0.86716\n","11/11 [==============================] - 7s 663ms/step - loss: 72.5325 - accuracy: 0.9350 - val_loss_val: 127.2918 - val_val_accuracy: 0.8631\n","Epoch 29/50\n","11/11 [==============================] - ETA: 0s - loss: 70.2593 - accuracy: 0.9371\n","Epoch 00029: val_val_accuracy did not improve from 0.86716\n","11/11 [==============================] - 7s 665ms/step - loss: 70.2593 - accuracy: 0.9371 - val_loss_val: 125.4094 - val_val_accuracy: 0.8652\n","Epoch 30/50\n","11/11 [==============================] - ETA: 0s - loss: 68.0090 - accuracy: 0.9401\n","Epoch 00030: val_val_accuracy did not improve from 0.86716\n","11/11 [==============================] - 7s 669ms/step - loss: 68.0090 - accuracy: 0.9401 - val_loss_val: 126.7594 - val_val_accuracy: 0.8634\n","Epoch 31/50\n","11/11 [==============================] - ETA: 0s - loss: 65.9041 - accuracy: 0.9422\n","Epoch 00031: val_val_accuracy did not improve from 0.86716\n","11/11 [==============================] - 7s 658ms/step - loss: 65.9041 - accuracy: 0.9422 - val_loss_val: 127.8074 - val_val_accuracy: 0.8632\n","Epoch 32/50\n","11/11 [==============================] - ETA: 0s - loss: 63.7840 - accuracy: 0.9445\n","Epoch 00032: val_val_accuracy did not improve from 0.86716\n","11/11 [==============================] - 7s 664ms/step - loss: 63.7840 - accuracy: 0.9445 - val_loss_val: 127.1277 - val_val_accuracy: 0.8616\n"]},{"output_type":"stream","name":"stderr","text":["\r3it [12:16, 249.10s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","11/11 [==============================] - ETA: 0s - loss: 813.2711 - accuracy: 0.2275\n","Epoch 00001: val_val_accuracy improved from -inf to 0.26753, saving model to ./models/Bilstm_crf/best_model_bilstm_A3.hdf5\n","11/11 [==============================] - 17s 856ms/step - loss: 813.2711 - accuracy: 0.2275 - val_loss_val: 723.2390 - val_val_accuracy: 0.2675\n","Epoch 2/50\n","11/11 [==============================] - ETA: 0s - loss: 590.4734 - accuracy: 0.4268\n","Epoch 00002: val_val_accuracy improved from 0.26753 to 0.47923, saving model to ./models/Bilstm_crf/best_model_bilstm_A3.hdf5\n","11/11 [==============================] - 9s 811ms/step - loss: 590.4734 - accuracy: 0.4268 - val_loss_val: 533.1318 - val_val_accuracy: 0.4792\n","Epoch 3/50\n","11/11 [==============================] - ETA: 0s - loss: 412.7198 - accuracy: 0.6604\n","Epoch 00003: val_val_accuracy improved from 0.47923 to 0.66510, saving model to ./models/Bilstm_crf/best_model_bilstm_A3.hdf5\n","11/11 [==============================] - 10s 715ms/step - loss: 412.7198 - accuracy: 0.6604 - val_loss_val: 391.2334 - val_val_accuracy: 0.6651\n","Epoch 4/50\n","11/11 [==============================] - ETA: 0s - loss: 302.7714 - accuracy: 0.7334\n","Epoch 00004: val_val_accuracy improved from 0.66510 to 0.76067, saving model to ./models/Bilstm_crf/best_model_bilstm_A3.hdf5\n","11/11 [==============================] - 8s 696ms/step - loss: 302.7714 - accuracy: 0.7334 - val_loss_val: 298.5576 - val_val_accuracy: 0.7607\n","Epoch 5/50\n","11/11 [==============================] - ETA: 0s - loss: 238.1145 - accuracy: 0.8461\n","Epoch 00005: val_val_accuracy improved from 0.76067 to 0.84809, saving model to ./models/Bilstm_crf/best_model_bilstm_A3.hdf5\n","11/11 [==============================] - 7s 687ms/step - loss: 238.1145 - accuracy: 0.8461 - val_loss_val: 240.5229 - val_val_accuracy: 0.8481\n","Epoch 6/50\n","11/11 [==============================] - ETA: 0s - loss: 201.9463 - accuracy: 0.8604\n","Epoch 00006: val_val_accuracy improved from 0.84809 to 0.85098, saving model to ./models/Bilstm_crf/best_model_bilstm_A3.hdf5\n","11/11 [==============================] - 8s 691ms/step - loss: 201.9463 - accuracy: 0.8604 - val_loss_val: 211.4399 - val_val_accuracy: 0.8510\n","Epoch 7/50\n","11/11 [==============================] - ETA: 0s - loss: 182.7852 - accuracy: 0.8621\n","Epoch 00007: val_val_accuracy improved from 0.85098 to 0.85206, saving model to ./models/Bilstm_crf/best_model_bilstm_A3.hdf5\n","11/11 [==============================] - 8s 693ms/step - loss: 182.7852 - accuracy: 0.8621 - val_loss_val: 196.4420 - val_val_accuracy: 0.8521\n","Epoch 8/50\n","11/11 [==============================] - ETA: 0s - loss: 169.6782 - accuracy: 0.8633\n","Epoch 00008: val_val_accuracy improved from 0.85206 to 0.85263, saving model to ./models/Bilstm_crf/best_model_bilstm_A3.hdf5\n","11/11 [==============================] - 8s 694ms/step - loss: 169.6782 - accuracy: 0.8633 - val_loss_val: 186.5994 - val_val_accuracy: 0.8526\n","Epoch 9/50\n","11/11 [==============================] - ETA: 0s - loss: 158.8791 - accuracy: 0.8649\n","Epoch 00009: val_val_accuracy improved from 0.85263 to 0.85284, saving model to ./models/Bilstm_crf/best_model_bilstm_A3.hdf5\n","11/11 [==============================] - 8s 700ms/step - loss: 158.8791 - accuracy: 0.8649 - val_loss_val: 179.2210 - val_val_accuracy: 0.8528\n","Epoch 10/50\n","11/11 [==============================] - ETA: 0s - loss: 150.0727 - accuracy: 0.8677\n","Epoch 00010: val_val_accuracy improved from 0.85284 to 0.85356, saving model to ./models/Bilstm_crf/best_model_bilstm_A3.hdf5\n","11/11 [==============================] - 8s 691ms/step - loss: 150.0727 - accuracy: 0.8677 - val_loss_val: 172.8640 - val_val_accuracy: 0.8536\n","Epoch 11/50\n","11/11 [==============================] - ETA: 0s - loss: 142.3945 - accuracy: 0.8707\n","Epoch 00011: val_val_accuracy improved from 0.85356 to 0.85505, saving model to ./models/Bilstm_crf/best_model_bilstm_A3.hdf5\n","11/11 [==============================] - 8s 696ms/step - loss: 142.3945 - accuracy: 0.8707 - val_loss_val: 166.6411 - val_val_accuracy: 0.8551\n","Epoch 12/50\n","11/11 [==============================] - ETA: 0s - loss: 135.6338 - accuracy: 0.8742\n","Epoch 00012: val_val_accuracy improved from 0.85505 to 0.85582, saving model to ./models/Bilstm_crf/best_model_bilstm_A3.hdf5\n","11/11 [==============================] - 8s 697ms/step - loss: 135.6338 - accuracy: 0.8742 - val_loss_val: 162.0122 - val_val_accuracy: 0.8558\n","Epoch 13/50\n","11/11 [==============================] - ETA: 0s - loss: 129.5919 - accuracy: 0.8792\n","Epoch 00013: val_val_accuracy improved from 0.85582 to 0.85794, saving model to ./models/Bilstm_crf/best_model_bilstm_A3.hdf5\n","11/11 [==============================] - 8s 692ms/step - loss: 129.5919 - accuracy: 0.8792 - val_loss_val: 158.0872 - val_val_accuracy: 0.8579\n","Epoch 14/50\n","11/11 [==============================] - ETA: 0s - loss: 124.1356 - accuracy: 0.8842\n","Epoch 00014: val_val_accuracy improved from 0.85794 to 0.85964, saving model to ./models/Bilstm_crf/best_model_bilstm_A3.hdf5\n","11/11 [==============================] - 9s 850ms/step - loss: 124.1356 - accuracy: 0.8842 - val_loss_val: 154.6490 - val_val_accuracy: 0.8596\n","Epoch 15/50\n","11/11 [==============================] - ETA: 0s - loss: 119.2670 - accuracy: 0.8895\n","Epoch 00015: val_val_accuracy improved from 0.85964 to 0.85990, saving model to ./models/Bilstm_crf/best_model_bilstm_A3.hdf5\n","11/11 [==============================] - 8s 694ms/step - loss: 119.2670 - accuracy: 0.8895 - val_loss_val: 152.0501 - val_val_accuracy: 0.8599\n","Epoch 16/50\n","11/11 [==============================] - ETA: 0s - loss: 114.5187 - accuracy: 0.8954\n","Epoch 00016: val_val_accuracy improved from 0.85990 to 0.86124, saving model to ./models/Bilstm_crf/best_model_bilstm_A3.hdf5\n","11/11 [==============================] - 8s 699ms/step - loss: 114.5187 - accuracy: 0.8954 - val_loss_val: 149.5993 - val_val_accuracy: 0.8612\n","Epoch 17/50\n","11/11 [==============================] - ETA: 0s - loss: 110.1428 - accuracy: 0.9004\n","Epoch 00017: val_val_accuracy did not improve from 0.86124\n","11/11 [==============================] - 7s 666ms/step - loss: 110.1428 - accuracy: 0.9004 - val_loss_val: 147.4149 - val_val_accuracy: 0.8606\n","Epoch 18/50\n","11/11 [==============================] - ETA: 0s - loss: 105.8828 - accuracy: 0.9055\n","Epoch 00018: val_val_accuracy did not improve from 0.86124\n","11/11 [==============================] - 7s 667ms/step - loss: 105.8828 - accuracy: 0.9055 - val_loss_val: 145.4979 - val_val_accuracy: 0.8610\n","Epoch 19/50\n","11/11 [==============================] - ETA: 0s - loss: 101.9472 - accuracy: 0.9092\n","Epoch 00019: val_val_accuracy did not improve from 0.86124\n","11/11 [==============================] - 7s 666ms/step - loss: 101.9472 - accuracy: 0.9092 - val_loss_val: 143.7287 - val_val_accuracy: 0.8610\n","Epoch 20/50\n","11/11 [==============================] - ETA: 0s - loss: 98.1668 - accuracy: 0.9130\n","Epoch 00020: val_val_accuracy did not improve from 0.86124\n","11/11 [==============================] - 7s 664ms/step - loss: 98.1668 - accuracy: 0.9130 - val_loss_val: 142.4571 - val_val_accuracy: 0.8589\n","Epoch 21/50\n","11/11 [==============================] - ETA: 0s - loss: 94.4084 - accuracy: 0.9173\n","Epoch 00021: val_val_accuracy did not improve from 0.86124\n","11/11 [==============================] - 7s 674ms/step - loss: 94.4084 - accuracy: 0.9173 - val_loss_val: 141.1472 - val_val_accuracy: 0.8599\n","Epoch 22/50\n","11/11 [==============================] - ETA: 0s - loss: 90.8987 - accuracy: 0.9208\n","Epoch 00022: val_val_accuracy did not improve from 0.86124\n","11/11 [==============================] - 7s 659ms/step - loss: 90.8987 - accuracy: 0.9208 - val_loss_val: 139.9941 - val_val_accuracy: 0.8584\n","Epoch 23/50\n","11/11 [==============================] - ETA: 0s - loss: 87.5356 - accuracy: 0.9236\n","Epoch 00023: val_val_accuracy did not improve from 0.86124\n","11/11 [==============================] - 7s 663ms/step - loss: 87.5356 - accuracy: 0.9236 - val_loss_val: 139.7140 - val_val_accuracy: 0.8578\n","Epoch 24/50\n","11/11 [==============================] - ETA: 0s - loss: 84.3048 - accuracy: 0.9266\n","Epoch 00024: val_val_accuracy did not improve from 0.86124\n","11/11 [==============================] - 7s 661ms/step - loss: 84.3048 - accuracy: 0.9266 - val_loss_val: 138.6302 - val_val_accuracy: 0.8598\n","Epoch 25/50\n","11/11 [==============================] - ETA: 0s - loss: 81.2186 - accuracy: 0.9296\n","Epoch 00025: val_val_accuracy did not improve from 0.86124\n","11/11 [==============================] - 7s 662ms/step - loss: 81.2186 - accuracy: 0.9296 - val_loss_val: 138.1147 - val_val_accuracy: 0.8598\n","Epoch 26/50\n","11/11 [==============================] - ETA: 0s - loss: 78.1885 - accuracy: 0.9318\n","Epoch 00026: val_val_accuracy did not improve from 0.86124\n","11/11 [==============================] - 7s 660ms/step - loss: 78.1885 - accuracy: 0.9318 - val_loss_val: 137.4277 - val_val_accuracy: 0.8593\n","Epoch 27/50\n","11/11 [==============================] - ETA: 0s - loss: 75.3621 - accuracy: 0.9352\n","Epoch 00027: val_val_accuracy did not improve from 0.86124\n","11/11 [==============================] - 7s 662ms/step - loss: 75.3621 - accuracy: 0.9352 - val_loss_val: 136.9212 - val_val_accuracy: 0.8601\n","Epoch 28/50\n","11/11 [==============================] - ETA: 0s - loss: 72.5772 - accuracy: 0.9380\n","Epoch 00028: val_val_accuracy improved from 0.86124 to 0.86314, saving model to ./models/Bilstm_crf/best_model_bilstm_A3.hdf5\n","11/11 [==============================] - 8s 692ms/step - loss: 72.5772 - accuracy: 0.9380 - val_loss_val: 137.0233 - val_val_accuracy: 0.8631\n","Epoch 29/50\n","11/11 [==============================] - ETA: 0s - loss: 69.9634 - accuracy: 0.9415\n","Epoch 00029: val_val_accuracy did not improve from 0.86314\n","11/11 [==============================] - 7s 669ms/step - loss: 69.9634 - accuracy: 0.9415 - val_loss_val: 136.3283 - val_val_accuracy: 0.8627\n","Epoch 30/50\n","11/11 [==============================] - ETA: 0s - loss: 67.4495 - accuracy: 0.9452\n","Epoch 00030: val_val_accuracy did not improve from 0.86314\n","11/11 [==============================] - 7s 660ms/step - loss: 67.4495 - accuracy: 0.9452 - val_loss_val: 136.2452 - val_val_accuracy: 0.8613\n","Epoch 31/50\n","11/11 [==============================] - ETA: 0s - loss: 65.0133 - accuracy: 0.9477\n","Epoch 00031: val_val_accuracy did not improve from 0.86314\n","11/11 [==============================] - 7s 664ms/step - loss: 65.0133 - accuracy: 0.9477 - val_loss_val: 136.2579 - val_val_accuracy: 0.8605\n","Epoch 32/50\n","11/11 [==============================] - ETA: 0s - loss: 62.6481 - accuracy: 0.9511\n","Epoch 00032: val_val_accuracy did not improve from 0.86314\n","11/11 [==============================] - 7s 668ms/step - loss: 62.6481 - accuracy: 0.9511 - val_loss_val: 136.5500 - val_val_accuracy: 0.8606\n","Epoch 33/50\n","11/11 [==============================] - ETA: 0s - loss: 60.3572 - accuracy: 0.9538\n","Epoch 00033: val_val_accuracy did not improve from 0.86314\n","11/11 [==============================] - 7s 662ms/step - loss: 60.3572 - accuracy: 0.9538 - val_loss_val: 136.6698 - val_val_accuracy: 0.8594\n"]},{"output_type":"stream","name":"stderr","text":["\r4it [16:57, 261.79s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","11/11 [==============================] - ETA: 0s - loss: 576.6807 - accuracy: 0.3084\n","Epoch 00001: val_val_accuracy improved from -inf to 0.57799, saving model to ./models/Bilstm_crf/best_model_bilstm_A4.hdf5\n","11/11 [==============================] - 21s 1s/step - loss: 576.6807 - accuracy: 0.3084 - val_loss_val: 424.4284 - val_val_accuracy: 0.5780\n","Epoch 2/50\n","11/11 [==============================] - ETA: 0s - loss: 325.2778 - accuracy: 0.7910\n","Epoch 00002: val_val_accuracy improved from 0.57799 to 0.85485, saving model to ./models/Bilstm_crf/best_model_bilstm_A4.hdf5\n","11/11 [==============================] - 11s 1s/step - loss: 325.2778 - accuracy: 0.7910 - val_loss_val: 177.6390 - val_val_accuracy: 0.8548\n","Epoch 3/50\n","11/11 [==============================] - ETA: 0s - loss: 136.8391 - accuracy: 0.8692\n","Epoch 00003: val_val_accuracy improved from 0.85485 to 0.85629, saving model to ./models/Bilstm_crf/best_model_bilstm_A4.hdf5\n","11/11 [==============================] - 11s 1s/step - loss: 136.8391 - accuracy: 0.8692 - val_loss_val: 143.7660 - val_val_accuracy: 0.8563\n","Epoch 4/50\n","11/11 [==============================] - ETA: 0s - loss: 121.9745 - accuracy: 0.8720\n","Epoch 00004: val_val_accuracy improved from 0.85629 to 0.86041, saving model to ./models/Bilstm_crf/best_model_bilstm_A4.hdf5\n","11/11 [==============================] - 11s 1s/step - loss: 121.9745 - accuracy: 0.8720 - val_loss_val: 135.9795 - val_val_accuracy: 0.8604\n","Epoch 5/50\n","11/11 [==============================] - ETA: 0s - loss: 112.9335 - accuracy: 0.8778\n","Epoch 00005: val_val_accuracy improved from 0.86041 to 0.86356, saving model to ./models/Bilstm_crf/best_model_bilstm_A4.hdf5\n","11/11 [==============================] - 11s 1s/step - loss: 112.9335 - accuracy: 0.8778 - val_loss_val: 127.5509 - val_val_accuracy: 0.8636\n","Epoch 6/50\n","11/11 [==============================] - ETA: 0s - loss: 105.1034 - accuracy: 0.8852\n","Epoch 00006: val_val_accuracy improved from 0.86356 to 0.86510, saving model to ./models/Bilstm_crf/best_model_bilstm_A4.hdf5\n","11/11 [==============================] - 11s 1s/step - loss: 105.1034 - accuracy: 0.8852 - val_loss_val: 121.3885 - val_val_accuracy: 0.8651\n","Epoch 7/50\n","11/11 [==============================] - ETA: 0s - loss: 98.1868 - accuracy: 0.8923\n","Epoch 00007: val_val_accuracy improved from 0.86510 to 0.87077, saving model to ./models/Bilstm_crf/best_model_bilstm_A4.hdf5\n","11/11 [==============================] - 11s 1s/step - loss: 98.1868 - accuracy: 0.8923 - val_loss_val: 114.9686 - val_val_accuracy: 0.8708\n","Epoch 8/50\n","11/11 [==============================] - ETA: 0s - loss: 91.1774 - accuracy: 0.8976\n","Epoch 00008: val_val_accuracy improved from 0.87077 to 0.87160, saving model to ./models/Bilstm_crf/best_model_bilstm_A4.hdf5\n","11/11 [==============================] - 11s 1s/step - loss: 91.1774 - accuracy: 0.8976 - val_loss_val: 110.8770 - val_val_accuracy: 0.8716\n","Epoch 9/50\n","11/11 [==============================] - ETA: 0s - loss: 84.5473 - accuracy: 0.9058\n","Epoch 00009: val_val_accuracy improved from 0.87160 to 0.87479, saving model to ./models/Bilstm_crf/best_model_bilstm_A4.hdf5\n","11/11 [==============================] - 11s 1s/step - loss: 84.5473 - accuracy: 0.9058 - val_loss_val: 105.7032 - val_val_accuracy: 0.8748\n","Epoch 10/50\n","11/11 [==============================] - ETA: 0s - loss: 78.3752 - accuracy: 0.9117\n","Epoch 00010: val_val_accuracy improved from 0.87479 to 0.87943, saving model to ./models/Bilstm_crf/best_model_bilstm_A4.hdf5\n","11/11 [==============================] - 12s 1s/step - loss: 78.3752 - accuracy: 0.9117 - val_loss_val: 101.6577 - val_val_accuracy: 0.8794\n","Epoch 11/50\n","11/11 [==============================] - ETA: 0s - loss: 73.0855 - accuracy: 0.9183\n","Epoch 00011: val_val_accuracy improved from 0.87943 to 0.87954, saving model to ./models/Bilstm_crf/best_model_bilstm_A4.hdf5\n","11/11 [==============================] - 11s 1s/step - loss: 73.0855 - accuracy: 0.9183 - val_loss_val: 99.9656 - val_val_accuracy: 0.8795\n","Epoch 12/50\n","11/11 [==============================] - ETA: 0s - loss: 67.7933 - accuracy: 0.9249\n","Epoch 00012: val_val_accuracy improved from 0.87954 to 0.88077, saving model to ./models/Bilstm_crf/best_model_bilstm_A4.hdf5\n","11/11 [==============================] - 11s 1s/step - loss: 67.7933 - accuracy: 0.9249 - val_loss_val: 97.5033 - val_val_accuracy: 0.8808\n","Epoch 13/50\n","11/11 [==============================] - ETA: 0s - loss: 63.2665 - accuracy: 0.9314\n","Epoch 00013: val_val_accuracy improved from 0.88077 to 0.88108, saving model to ./models/Bilstm_crf/best_model_bilstm_A4.hdf5\n","11/11 [==============================] - 11s 1s/step - loss: 63.2665 - accuracy: 0.9314 - val_loss_val: 96.0012 - val_val_accuracy: 0.8811\n","Epoch 14/50\n","11/11 [==============================] - ETA: 0s - loss: 58.6265 - accuracy: 0.9365\n","Epoch 00014: val_val_accuracy did not improve from 0.88108\n","11/11 [==============================] - 11s 983ms/step - loss: 58.6265 - accuracy: 0.9365 - val_loss_val: 95.5850 - val_val_accuracy: 0.8798\n","Epoch 15/50\n","11/11 [==============================] - ETA: 0s - loss: 54.4825 - accuracy: 0.9409\n","Epoch 00015: val_val_accuracy did not improve from 0.88108\n","11/11 [==============================] - 11s 979ms/step - loss: 54.4825 - accuracy: 0.9409 - val_loss_val: 96.4534 - val_val_accuracy: 0.8803\n","Epoch 16/50\n","11/11 [==============================] - ETA: 0s - loss: 50.3339 - accuracy: 0.9464\n","Epoch 00016: val_val_accuracy did not improve from 0.88108\n","11/11 [==============================] - 11s 980ms/step - loss: 50.3339 - accuracy: 0.9464 - val_loss_val: 98.5796 - val_val_accuracy: 0.8778\n","Epoch 17/50\n","11/11 [==============================] - ETA: 0s - loss: 47.2959 - accuracy: 0.9499\n","Epoch 00017: val_val_accuracy did not improve from 0.88108\n","11/11 [==============================] - 11s 978ms/step - loss: 47.2959 - accuracy: 0.9499 - val_loss_val: 99.5261 - val_val_accuracy: 0.8766\n"]},{"output_type":"stream","name":"stderr","text":["\r5it [20:15, 238.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","11/11 [==============================] - ETA: 0s - loss: 430.8212 - accuracy: 0.6677\n","Epoch 00001: val_val_accuracy improved from -inf to 0.84036, saving model to ./models/Bilstm_crf/best_model_bilstm_A5.hdf5\n","11/11 [==============================] - 20s 1s/step - loss: 430.8212 - accuracy: 0.6677 - val_loss_val: 335.6440 - val_val_accuracy: 0.8404\n","Epoch 2/50\n","11/11 [==============================] - ETA: 0s - loss: 242.3040 - accuracy: 0.8585\n","Epoch 00002: val_val_accuracy improved from 0.84036 to 0.85041, saving model to ./models/Bilstm_crf/best_model_bilstm_A5.hdf5\n","11/11 [==============================] - 11s 988ms/step - loss: 242.3040 - accuracy: 0.8585 - val_loss_val: 223.6375 - val_val_accuracy: 0.8504\n","Epoch 3/50\n","11/11 [==============================] - ETA: 0s - loss: 172.8817 - accuracy: 0.8631\n","Epoch 00003: val_val_accuracy improved from 0.85041 to 0.85454, saving model to ./models/Bilstm_crf/best_model_bilstm_A5.hdf5\n","11/11 [==============================] - 11s 1s/step - loss: 172.8817 - accuracy: 0.8631 - val_loss_val: 179.3337 - val_val_accuracy: 0.8545\n","Epoch 4/50\n","11/11 [==============================] - ETA: 0s - loss: 151.8311 - accuracy: 0.8647\n","Epoch 00004: val_val_accuracy improved from 0.85454 to 0.85459, saving model to ./models/Bilstm_crf/best_model_bilstm_A5.hdf5\n","11/11 [==============================] - 11s 1s/step - loss: 151.8311 - accuracy: 0.8647 - val_loss_val: 170.9669 - val_val_accuracy: 0.8546\n","Epoch 5/50\n","11/11 [==============================] - ETA: 0s - loss: 136.0702 - accuracy: 0.8648\n","Epoch 00005: val_val_accuracy improved from 0.85459 to 0.85479, saving model to ./models/Bilstm_crf/best_model_bilstm_A5.hdf5\n","11/11 [==============================] - 11s 1s/step - loss: 136.0702 - accuracy: 0.8648 - val_loss_val: 153.0976 - val_val_accuracy: 0.8548\n","Epoch 6/50\n","11/11 [==============================] - ETA: 0s - loss: 121.1565 - accuracy: 0.8704\n","Epoch 00006: val_val_accuracy improved from 0.85479 to 0.86005, saving model to ./models/Bilstm_crf/best_model_bilstm_A5.hdf5\n","11/11 [==============================] - 11s 1s/step - loss: 121.1565 - accuracy: 0.8704 - val_loss_val: 144.7955 - val_val_accuracy: 0.8601\n","Epoch 7/50\n","11/11 [==============================] - ETA: 0s - loss: 107.6114 - accuracy: 0.8940\n","Epoch 00007: val_val_accuracy improved from 0.86005 to 0.86943, saving model to ./models/Bilstm_crf/best_model_bilstm_A5.hdf5\n","11/11 [==============================] - 11s 1s/step - loss: 107.6114 - accuracy: 0.8940 - val_loss_val: 137.5357 - val_val_accuracy: 0.8694\n","Epoch 8/50\n","11/11 [==============================] - ETA: 0s - loss: 95.2571 - accuracy: 0.9123\n","Epoch 00008: val_val_accuracy improved from 0.86943 to 0.87155, saving model to ./models/Bilstm_crf/best_model_bilstm_A5.hdf5\n","11/11 [==============================] - 11s 1s/step - loss: 95.2571 - accuracy: 0.9123 - val_loss_val: 131.7091 - val_val_accuracy: 0.8715\n","Epoch 9/50\n","11/11 [==============================] - ETA: 0s - loss: 82.7479 - accuracy: 0.9246\n","Epoch 00009: val_val_accuracy improved from 0.87155 to 0.87289, saving model to ./models/Bilstm_crf/best_model_bilstm_A5.hdf5\n","11/11 [==============================] - 11s 1s/step - loss: 82.7479 - accuracy: 0.9246 - val_loss_val: 128.0436 - val_val_accuracy: 0.8729\n","Epoch 10/50\n","11/11 [==============================] - ETA: 0s - loss: 71.3704 - accuracy: 0.9332\n","Epoch 00010: val_val_accuracy did not improve from 0.87289\n","11/11 [==============================] - 11s 985ms/step - loss: 71.3704 - accuracy: 0.9332 - val_loss_val: 125.8844 - val_val_accuracy: 0.8727\n","Epoch 11/50\n","11/11 [==============================] - ETA: 0s - loss: 60.4052 - accuracy: 0.9453\n","Epoch 00011: val_val_accuracy improved from 0.87289 to 0.87428, saving model to ./models/Bilstm_crf/best_model_bilstm_A5.hdf5\n","11/11 [==============================] - 11s 1s/step - loss: 60.4052 - accuracy: 0.9453 - val_loss_val: 127.2878 - val_val_accuracy: 0.8743\n","Epoch 12/50\n","11/11 [==============================] - ETA: 0s - loss: 51.1571 - accuracy: 0.9550\n","Epoch 00012: val_val_accuracy did not improve from 0.87428\n","11/11 [==============================] - 11s 985ms/step - loss: 51.1571 - accuracy: 0.9550 - val_loss_val: 132.0555 - val_val_accuracy: 0.8706\n","Epoch 13/50\n","11/11 [==============================] - ETA: 0s - loss: 43.4949 - accuracy: 0.9632\n","Epoch 00013: val_val_accuracy did not improve from 0.87428\n","11/11 [==============================] - 11s 980ms/step - loss: 43.4949 - accuracy: 0.9632 - val_loss_val: 137.9423 - val_val_accuracy: 0.8668\n"]},{"output_type":"stream","name":"stderr","text":["\r6it [22:47, 209.34s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","11/11 [==============================] - ETA: 0s - loss: 564.8522 - accuracy: 0.4661\n","Epoch 00001: val_val_accuracy improved from -inf to 0.83459, saving model to ./models/Bilstm_crf/best_model_bilstm_A6.hdf5\n","11/11 [==============================] - 22s 1s/step - loss: 564.8522 - accuracy: 0.4661 - val_loss_val: 406.6782 - val_val_accuracy: 0.8346\n","Epoch 2/50\n","11/11 [==============================] - ETA: 0s - loss: 277.0113 - accuracy: 0.8587\n","Epoch 00002: val_val_accuracy improved from 0.83459 to 0.84608, saving model to ./models/Bilstm_crf/best_model_bilstm_A6.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 277.0113 - accuracy: 0.8587 - val_loss_val: 213.7614 - val_val_accuracy: 0.8461\n","Epoch 3/50\n","11/11 [==============================] - ETA: 0s - loss: 183.2888 - accuracy: 0.8564\n","Epoch 00003: val_val_accuracy improved from 0.84608 to 0.85062, saving model to ./models/Bilstm_crf/best_model_bilstm_A6.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 183.2888 - accuracy: 0.8564 - val_loss_val: 194.1914 - val_val_accuracy: 0.8506\n","Epoch 4/50\n","11/11 [==============================] - ETA: 0s - loss: 169.2045 - accuracy: 0.8655\n","Epoch 00004: val_val_accuracy improved from 0.85062 to 0.85335, saving model to ./models/Bilstm_crf/best_model_bilstm_A6.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 169.2045 - accuracy: 0.8655 - val_loss_val: 183.0802 - val_val_accuracy: 0.8534\n","Epoch 5/50\n","11/11 [==============================] - ETA: 0s - loss: 158.9474 - accuracy: 0.8672\n","Epoch 00005: val_val_accuracy improved from 0.85335 to 0.85500, saving model to ./models/Bilstm_crf/best_model_bilstm_A6.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 158.9474 - accuracy: 0.8672 - val_loss_val: 174.5592 - val_val_accuracy: 0.8550\n","Epoch 6/50\n","11/11 [==============================] - ETA: 0s - loss: 150.0113 - accuracy: 0.8724\n","Epoch 00006: val_val_accuracy improved from 0.85500 to 0.85593, saving model to ./models/Bilstm_crf/best_model_bilstm_A6.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 150.0113 - accuracy: 0.8724 - val_loss_val: 168.2451 - val_val_accuracy: 0.8559\n","Epoch 7/50\n","11/11 [==============================] - ETA: 0s - loss: 141.9887 - accuracy: 0.8765\n","Epoch 00007: val_val_accuracy improved from 0.85593 to 0.86211, saving model to ./models/Bilstm_crf/best_model_bilstm_A6.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 141.9887 - accuracy: 0.8765 - val_loss_val: 161.2108 - val_val_accuracy: 0.8621\n","Epoch 8/50\n","11/11 [==============================] - ETA: 0s - loss: 133.9024 - accuracy: 0.8838\n","Epoch 00008: val_val_accuracy did not improve from 0.86211\n","11/11 [==============================] - 13s 1s/step - loss: 133.9024 - accuracy: 0.8838 - val_loss_val: 156.1208 - val_val_accuracy: 0.8615\n","Epoch 9/50\n","11/11 [==============================] - ETA: 0s - loss: 125.4644 - accuracy: 0.8898\n","Epoch 00009: val_val_accuracy improved from 0.86211 to 0.86500, saving model to ./models/Bilstm_crf/best_model_bilstm_A6.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 125.4644 - accuracy: 0.8898 - val_loss_val: 151.1609 - val_val_accuracy: 0.8650\n","Epoch 10/50\n","11/11 [==============================] - ETA: 0s - loss: 118.0387 - accuracy: 0.8961\n","Epoch 00010: val_val_accuracy did not improve from 0.86500\n","11/11 [==============================] - 13s 1s/step - loss: 118.0387 - accuracy: 0.8961 - val_loss_val: 147.6973 - val_val_accuracy: 0.8650\n","Epoch 11/50\n","11/11 [==============================] - ETA: 0s - loss: 110.7316 - accuracy: 0.9011\n","Epoch 00011: val_val_accuracy improved from 0.86500 to 0.86804, saving model to ./models/Bilstm_crf/best_model_bilstm_A6.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 110.7316 - accuracy: 0.9011 - val_loss_val: 143.3531 - val_val_accuracy: 0.8680\n","Epoch 12/50\n","11/11 [==============================] - ETA: 0s - loss: 103.5902 - accuracy: 0.9063\n","Epoch 00012: val_val_accuracy improved from 0.86804 to 0.86964, saving model to ./models/Bilstm_crf/best_model_bilstm_A6.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 103.5902 - accuracy: 0.9063 - val_loss_val: 141.6227 - val_val_accuracy: 0.8696\n","Epoch 13/50\n","11/11 [==============================] - ETA: 0s - loss: 96.4050 - accuracy: 0.9127\n","Epoch 00013: val_val_accuracy did not improve from 0.86964\n","11/11 [==============================] - 13s 1s/step - loss: 96.4050 - accuracy: 0.9127 - val_loss_val: 139.5004 - val_val_accuracy: 0.8654\n","Epoch 14/50\n","11/11 [==============================] - ETA: 0s - loss: 89.1097 - accuracy: 0.9198\n","Epoch 00014: val_val_accuracy improved from 0.86964 to 0.87191, saving model to ./models/Bilstm_crf/best_model_bilstm_A6.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 89.1097 - accuracy: 0.9198 - val_loss_val: 137.8902 - val_val_accuracy: 0.8719\n","Epoch 15/50\n","11/11 [==============================] - ETA: 0s - loss: 83.6419 - accuracy: 0.9238\n","Epoch 00015: val_val_accuracy did not improve from 0.87191\n","11/11 [==============================] - 13s 1s/step - loss: 83.6419 - accuracy: 0.9238 - val_loss_val: 138.9414 - val_val_accuracy: 0.8690\n","Epoch 16/50\n","11/11 [==============================] - ETA: 0s - loss: 77.5784 - accuracy: 0.9295\n","Epoch 00016: val_val_accuracy did not improve from 0.87191\n","11/11 [==============================] - 13s 1s/step - loss: 77.5784 - accuracy: 0.9295 - val_loss_val: 141.4497 - val_val_accuracy: 0.8645\n","Epoch 17/50\n","11/11 [==============================] - ETA: 0s - loss: 71.6371 - accuracy: 0.9344\n","Epoch 00017: val_val_accuracy improved from 0.87191 to 0.87232, saving model to ./models/Bilstm_crf/best_model_bilstm_A6.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 71.6371 - accuracy: 0.9344 - val_loss_val: 139.2100 - val_val_accuracy: 0.8723\n"]},{"output_type":"stream","name":"stderr","text":["\r7it [27:32, 234.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","11/11 [==============================] - ETA: 0s - loss: 553.8858 - accuracy: 0.4223\n","Epoch 00001: val_val_accuracy improved from -inf to 0.47031, saving model to ./models/Bilstm_crf/best_model_bilstm_A7.hdf5\n","11/11 [==============================] - 23s 1s/step - loss: 553.8858 - accuracy: 0.4223 - val_loss_val: 422.5653 - val_val_accuracy: 0.4703\n","Epoch 2/50\n","11/11 [==============================] - ETA: 0s - loss: 302.7005 - accuracy: 0.6211\n","Epoch 00002: val_val_accuracy improved from 0.47031 to 0.84443, saving model to ./models/Bilstm_crf/best_model_bilstm_A7.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 302.7005 - accuracy: 0.6211 - val_loss_val: 256.1518 - val_val_accuracy: 0.8444\n","Epoch 3/50\n","11/11 [==============================] - ETA: 0s - loss: 201.6654 - accuracy: 0.8563\n","Epoch 00003: val_val_accuracy improved from 0.84443 to 0.85206, saving model to ./models/Bilstm_crf/best_model_bilstm_A7.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 201.6654 - accuracy: 0.8563 - val_loss_val: 204.9319 - val_val_accuracy: 0.8521\n","Epoch 4/50\n","11/11 [==============================] - ETA: 0s - loss: 179.9676 - accuracy: 0.8636\n","Epoch 00004: val_val_accuracy improved from 0.85206 to 0.85247, saving model to ./models/Bilstm_crf/best_model_bilstm_A7.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 179.9676 - accuracy: 0.8636 - val_loss_val: 201.1865 - val_val_accuracy: 0.8525\n","Epoch 5/50\n","11/11 [==============================] - ETA: 0s - loss: 171.0653 - accuracy: 0.8639\n","Epoch 00005: val_val_accuracy improved from 0.85247 to 0.85423, saving model to ./models/Bilstm_crf/best_model_bilstm_A7.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 171.0653 - accuracy: 0.8639 - val_loss_val: 189.3566 - val_val_accuracy: 0.8542\n","Epoch 6/50\n","11/11 [==============================] - ETA: 0s - loss: 162.4392 - accuracy: 0.8648\n","Epoch 00006: val_val_accuracy improved from 0.85423 to 0.85469, saving model to ./models/Bilstm_crf/best_model_bilstm_A7.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 162.4392 - accuracy: 0.8648 - val_loss_val: 182.4481 - val_val_accuracy: 0.8547\n","Epoch 7/50\n","11/11 [==============================] - ETA: 0s - loss: 153.1332 - accuracy: 0.8649\n","Epoch 00007: val_val_accuracy did not improve from 0.85469\n","11/11 [==============================] - 13s 1s/step - loss: 153.1332 - accuracy: 0.8649 - val_loss_val: 172.3825 - val_val_accuracy: 0.8547\n","Epoch 8/50\n","11/11 [==============================] - ETA: 0s - loss: 142.1828 - accuracy: 0.8652\n","Epoch 00008: val_val_accuracy improved from 0.85469 to 0.85515, saving model to ./models/Bilstm_crf/best_model_bilstm_A7.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 142.1828 - accuracy: 0.8652 - val_loss_val: 163.7299 - val_val_accuracy: 0.8552\n","Epoch 9/50\n","11/11 [==============================] - ETA: 0s - loss: 130.7540 - accuracy: 0.8691\n","Epoch 00009: val_val_accuracy improved from 0.85515 to 0.85727, saving model to ./models/Bilstm_crf/best_model_bilstm_A7.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 130.7540 - accuracy: 0.8691 - val_loss_val: 154.8849 - val_val_accuracy: 0.8573\n","Epoch 10/50\n","11/11 [==============================] - ETA: 0s - loss: 120.0812 - accuracy: 0.8812\n","Epoch 00010: val_val_accuracy improved from 0.85727 to 0.86031, saving model to ./models/Bilstm_crf/best_model_bilstm_A7.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 120.0812 - accuracy: 0.8812 - val_loss_val: 148.1951 - val_val_accuracy: 0.8603\n","Epoch 11/50\n","11/11 [==============================] - ETA: 0s - loss: 110.2446 - accuracy: 0.8965\n","Epoch 00011: val_val_accuracy improved from 0.86031 to 0.86222, saving model to ./models/Bilstm_crf/best_model_bilstm_A7.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 110.2446 - accuracy: 0.8965 - val_loss_val: 142.8625 - val_val_accuracy: 0.8622\n","Epoch 12/50\n","11/11 [==============================] - ETA: 0s - loss: 101.1224 - accuracy: 0.9058\n","Epoch 00012: val_val_accuracy improved from 0.86222 to 0.86474, saving model to ./models/Bilstm_crf/best_model_bilstm_A7.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 101.1224 - accuracy: 0.9058 - val_loss_val: 139.7124 - val_val_accuracy: 0.8647\n","Epoch 13/50\n","11/11 [==============================] - ETA: 0s - loss: 92.0615 - accuracy: 0.9134\n","Epoch 00013: val_val_accuracy did not improve from 0.86474\n","11/11 [==============================] - 13s 1s/step - loss: 92.0615 - accuracy: 0.9134 - val_loss_val: 141.2984 - val_val_accuracy: 0.8604\n","Epoch 14/50\n","11/11 [==============================] - ETA: 0s - loss: 83.0295 - accuracy: 0.9229\n","Epoch 00014: val_val_accuracy improved from 0.86474 to 0.86923, saving model to ./models/Bilstm_crf/best_model_bilstm_A7.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 83.0295 - accuracy: 0.9229 - val_loss_val: 134.7515 - val_val_accuracy: 0.8692\n","Epoch 15/50\n","11/11 [==============================] - ETA: 0s - loss: 73.2969 - accuracy: 0.9326\n","Epoch 00015: val_val_accuracy did not improve from 0.86923\n","11/11 [==============================] - 14s 1s/step - loss: 73.2969 - accuracy: 0.9326 - val_loss_val: 135.6058 - val_val_accuracy: 0.8672\n","Epoch 16/50\n","11/11 [==============================] - ETA: 0s - loss: 64.4518 - accuracy: 0.9416\n","Epoch 00016: val_val_accuracy improved from 0.86923 to 0.86979, saving model to ./models/Bilstm_crf/best_model_bilstm_A7.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 64.4518 - accuracy: 0.9416 - val_loss_val: 134.6388 - val_val_accuracy: 0.8698\n","Epoch 17/50\n","11/11 [==============================] - ETA: 0s - loss: 56.1510 - accuracy: 0.9509\n","Epoch 00017: val_val_accuracy improved from 0.86979 to 0.87175, saving model to ./models/Bilstm_crf/best_model_bilstm_A7.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 56.1510 - accuracy: 0.9509 - val_loss_val: 135.4783 - val_val_accuracy: 0.8718\n","Epoch 18/50\n","11/11 [==============================] - ETA: 0s - loss: 48.7872 - accuracy: 0.9601\n","Epoch 00018: val_val_accuracy improved from 0.87175 to 0.87469, saving model to ./models/Bilstm_crf/best_model_bilstm_A7.hdf5\n","11/11 [==============================] - 13s 1s/step - loss: 48.7872 - accuracy: 0.9601 - val_loss_val: 140.4779 - val_val_accuracy: 0.8747\n","Epoch 19/50\n","11/11 [==============================] - ETA: 0s - loss: 43.0050 - accuracy: 0.9658\n","Epoch 00019: val_val_accuracy did not improve from 0.87469\n","11/11 [==============================] - 13s 1s/step - loss: 43.0050 - accuracy: 0.9658 - val_loss_val: 140.3924 - val_val_accuracy: 0.8719\n"]},{"output_type":"stream","name":"stderr","text":["8it [31:47, 238.47s/it]\n"]}]},{"cell_type":"markdown","metadata":{"id":"Wpjxt9A0frS2"},"source":["##### résulats du grid search"],"id":"Wpjxt9A0frS2"},{"cell_type":"code","metadata":{"id":"hMRi7PFofrS2"},"source":["#on importe les meilleurs modèles \n","val_true=np.argmax(val_tags_y, axis=-1)\n","\n","grid_search_results_A = pd.DataFrame(columns=['model', 'f1 score micro','f1 score macro'])\n","grid_search_results_A['model'] = models_bilstm_crf_param_A\n","\n","f1_scores_micro=[]\n","f1_scores_macro=[]\n","\n","f1_score_selected=0\n","for i in range(len(models_bilstm_crf_A)):\n","  # Restore the weights\n","  model_i = models_bilstm_crf_A[i]\n","  model_i.load_weights(\"./models/Bilstm_crf/best_model_bilstm_A\"+str(i)+\".hdf5\")\n","  val_pred_ner_i = model_i.predict(val_sentences_X)\n","  \n","  \n","  f1_score_micro = f1_score(val_true.flatten(), val_pred_ner_i.flatten(), average='micro')\n","  f1_score_macro = f1_score(val_true.flatten(), val_pred_ner_i.flatten(), average='macro')\n","  f1_scores_micro.append(f1_score_micro)\n","  f1_scores_macro.append(f1_score_macro)\n","  if f1_score_macro >= f1_score_selected:\n","    model_selected = model_i\n","    f1_score_selected = f1_score_macro\n"],"id":"hMRi7PFofrS2","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"xoOc8Rs1zLR-","executionInfo":{"status":"ok","timestamp":1638755899647,"user_tz":300,"elapsed":43,"user":{"displayName":"lamia2 salhi2","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14440555040870778846"}},"outputId":"35eb60f0-678b-4aba-ab36-de6522e69919"},"source":["grid_search_results_A['f1 score micro'] = f1_scores_micro\n","grid_search_results_A['f1 score macro'] = f1_scores_macro\n","grid_search_results_A"],"id":"xoOc8Rs1zLR-","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>f1 score micro</th>\n","      <th>f1 score macro</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[adam, Glove, GRU, 0.3, 10]</td>\n","      <td>0.875309</td>\n","      <td>0.570776</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[adam, None, GRU, 0.3, 10]</td>\n","      <td>0.870000</td>\n","      <td>0.535713</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[adam, Glove, LSTM, 0.3, 10]</td>\n","      <td>0.866237</td>\n","      <td>0.428373</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[adam, None, LSTM, 0.3, 10]</td>\n","      <td>0.863093</td>\n","      <td>0.483890</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[adam, Glove, GRU, 0.3, 64]</td>\n","      <td>0.884124</td>\n","      <td>0.616138</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>[adam, None, GRU, 0.3, 64]</td>\n","      <td>0.873196</td>\n","      <td>0.572482</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>[adam, Glove, LSTM, 0.3, 64]</td>\n","      <td>0.874845</td>\n","      <td>0.575239</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>[adam, None, LSTM, 0.3, 64]</td>\n","      <td>0.875464</td>\n","      <td>0.547626</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                          model  f1 score micro  f1 score macro\n","0   [adam, Glove, GRU, 0.3, 10]        0.875309        0.570776\n","1    [adam, None, GRU, 0.3, 10]        0.870000        0.535713\n","2  [adam, Glove, LSTM, 0.3, 10]        0.866237        0.428373\n","3   [adam, None, LSTM, 0.3, 10]        0.863093        0.483890\n","4   [adam, Glove, GRU, 0.3, 64]        0.884124        0.616138\n","5    [adam, None, GRU, 0.3, 64]        0.873196        0.572482\n","6  [adam, Glove, LSTM, 0.3, 64]        0.874845        0.575239\n","7   [adam, None, LSTM, 0.3, 64]        0.875464        0.547626"]},"metadata":{},"execution_count":162}]},{"cell_type":"markdown","metadata":{"id":"C_MZG1eK1Atr"},"source":["***REMARQUE***\n","on remarque que l'embedding Glove donne en général de meilleurs résultats. De plus le GRU offre également les meilleurs résultats avec un meilleur score f1 score macro de 0.61 et micro de 0.88. Ce résulat est associé à un plus grand nombreux d'unités (64) et un embedding glove."],"id":"C_MZG1eK1Atr"},{"cell_type":"markdown","metadata":{"id":"A4N5p6F1Diri"},"source":["##### Modèle sélectionné"],"id":"A4N5p6F1Diri"},{"cell_type":"code","metadata":{"id":"YTl7K9QcDlin"},"source":["model_sected=models_bilstm_crf_A[0]\n","callback2_A = EarlyStopping(monitor='val_loss_val', patience=3)\n","checkpoint2_A = ModelCheckpoint(\"./models/Bilstm_crf/best_model_bilstm_A_select.hdf5\", monitor='val_val_accuracy', verbose=1,save_best_only=True, mode='auto', save_weights_only=True)\n","\n","model_sected.fit(train_sentences_X, train_tags_y, batch_size=32, epochs=50, validation_data=(val_sentences_X,val_tags_y),callbacks=[checkpoint2_A,callback2_A],verbose=1)"],"id":"YTl7K9QcDlin","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WxwvKiiAfrS3"},"source":["##### Evaluation du modèle (confusion matrix et autre metrics)"],"id":"WxwvKiiAfrS3"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f2wYycCizSi9","executionInfo":{"status":"ok","timestamp":1638756285767,"user_tz":300,"elapsed":657,"user":{"displayName":"lamia2 salhi2","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14440555040870778846"}},"outputId":"feea3ac9-da9e-47e0-a52d-1e1e7f166328"},"source":["val_pred_ner = model_selected.predict(val_sentences_X)\n","labels_for_confusion_m= range(len(tags_A))\n","show_confusion_bis(val_true.flatten(), val_pred_ner.flatten(),labels=labels_for_confusion_m)"],"id":"f2wYycCizSi9","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["f1 micro:0.875\n","f1 macro:0.548\n","recall macro:0.523\n","Confusion Matrix:\n","[[8239    0    0    0    0    0]\n"," [   2 7852  203  146  124   13]\n"," [   0  346  284   53    6    5]\n"," [   0  617  123  255   72    5]\n"," [   2  322   14   25  324    7]\n"," [   2  225   67   10   27   30]]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HDAVrKz5frS3","executionInfo":{"status":"ok","timestamp":1638756289765,"user_tz":300,"elapsed":144,"user":{"displayName":"lamia2 salhi2","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14440555040870778846"}},"outputId":"2a9a9193-57fb-4907-b1eb-313a2502a032"},"source":["print(classification_report(val_true.flatten(), val_pred_ner.flatten(), target_names=tags_A))"],"id":"HDAVrKz5frS3","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           O       1.00      1.00      1.00      8239\n","           B       0.84      0.94      0.89      8340\n","           I       0.41      0.41      0.41       694\n","           L       0.52      0.24      0.33      1072\n","           U       0.59      0.47      0.52       694\n","       -PAD-       0.50      0.08      0.14       361\n","\n","    accuracy                           0.88     19400\n","   macro avg       0.64      0.52      0.55     19400\n","weighted avg       0.86      0.88      0.86     19400\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"Qlh8R8o3D87h"},"source":["***Remarque***\n","\n","On remarque que notre modèle apprend bien les valeurs évidentes comme les valeurs de padding qui sont à zéros. En vu de notre jeu de données qui est très déséquilibrés on s'attend donc à ce que notre modèle prédit beacoup de 'O', ce qui nous donne certes une  bonne precision, recall et f1 score pour le O mais des résultats moyens poour le 'I' par exemple et autre. Le 'B' est également plûtot bien prédit, nous pensons que la couche CRF permet en effet d'ajouter des éléments de logiques permettant d'améliorer nos résultats. En effet, on a testé un modèle Bilstm simple sans CRF et nous avionns de moins bon résultats.\n","\n","\n","NB: pour les valeurs du padding qui sont à 0 nous aurions pu utiliser l'argument masking=0 pour l'embedding mais cela nous donnait de moins bon résultats étonnement."],"id":"Qlh8R8o3D87h"},{"cell_type":"markdown","metadata":{"id":"dwTYXuMBfrS3"},"source":["##### création des fichiers de soumissions pour la validation et le test"],"id":"dwTYXuMBfrS3"},{"cell_type":"code","metadata":{"id":"WMwhyQ7ffrS4"},"source":["def suppress_pad(test_tags_y_predict,test_sentences_X_no_pad):\n","  # we supress the 0s correponding to the additional padding\n","  #test_tags_y_predict_no_pad = np.delete(test_tags_y_predict[0], np.where(test_tags_y_predict[0] == 0))\n","  test_tags_y_predict_no_pad = test_tags_y_predict[0][:len(test_sentences_X_no_pad[0])]\n","  for i in range (1,len(test_tags_y_predict)):\n","    A_i=test_tags_y_predict[i][:len(test_sentences_X_no_pad[i])]\n","    test_tags_y_predict_no_pad = np.concatenate((test_tags_y_predict_no_pad ,A_i))\n","    \n","    if len(test_sentences_X_no_pad[i])!=len(A_i):\n","      print(len(A_i))\n","      print(len(test_sentences_X_no_pad[i]))\n","      print(\"FALSE:\",i)\n","  \n","  return test_tags_y_predict_no_pad"],"id":"WMwhyQ7ffrS4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":441},"id":"APCmfWG9frS4","executionInfo":{"status":"ok","timestamp":1638745233175,"user_tz":300,"elapsed":14894,"user":{"displayName":"lamia2 salhi2","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14440555040870778846"}},"outputId":"e7d19631-b12b-40ec-ed31-8c4e040f6e9d"},"source":["val_pred_ner = model_selected.predict(val_sentences_X)\n","val_pred_ner_no_pad = suppress_pad(val_pred_ner,val_sentences_X_no_pad)\n","submission_val_bilstm_crf_A = pd.DataFrame(columns = ['TokenID', 'Tag'])\n","submission_val_bilstm_crf_A['TokenID'] = val_df['TokenID']\n","#submission_val_bilstm_crf_A['Tag'] = val_pred_ner_no_pad\n","\n","val_ids_concat = np.concatenate(val_ids)\n","for i in tqdm(range(len(val_df))):\n","  k = val_df[val_df[\"TokenID\"] == val_ids_concat[i]].index[0]\n","  submission_val_bilstm_crf_A.at[k, \"Tag\"] = val_pred_ner_no_pad[i]\n","\n","\n","submission_val_bilstm_crf_A['Tag'] = submission_val_bilstm_crf_A['Tag'].map(index2tag)\n","submission_val_bilstm_crf_A"],"id":"APCmfWG9frS4","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 11161/11161 [00:14<00:00, 775.02it/s]\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TokenID</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>S0301010415300355-0</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>S0301010415300355-1</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>S0301010415300355-2</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>S0301010415300355-3</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>S0301010415300355-4</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11156</th>\n","      <td>S1359028614000989-185</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>11157</th>\n","      <td>S1359028614000989-186</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>11158</th>\n","      <td>S1359028614000989-187</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>11159</th>\n","      <td>S1359028614000989-188</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>11160</th>\n","      <td>S1359028614000989-189</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11161 rows × 2 columns</p>\n","</div>"],"text/plain":["                     TokenID Tag\n","0        S0301010415300355-0   O\n","1        S0301010415300355-1   O\n","2        S0301010415300355-2   O\n","3        S0301010415300355-3   O\n","4        S0301010415300355-4   O\n","...                      ...  ..\n","11156  S1359028614000989-185   O\n","11157  S1359028614000989-186   O\n","11158  S1359028614000989-187   O\n","11159  S1359028614000989-188   O\n","11160  S1359028614000989-189   O\n","\n","[11161 rows x 2 columns]"]},"metadata":{},"execution_count":86}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":441},"id":"XdI30f8intqQ","executionInfo":{"status":"ok","timestamp":1638745387996,"user_tz":300,"elapsed":53831,"user":{"displayName":"lamia2 salhi2","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14440555040870778846"}},"outputId":"12a7fe4e-c09e-4203-a423-569c7cc9448b"},"source":["test_pred_ner = model_selected.predict(test_sentences_X)\n","test_pred_ner_no_pad = suppress_pad(test_pred_ner,test_sentences_X_no_pad)\n","submission_test_bilstm_crf_A = pd.DataFrame(columns = ['TokenID', 'Tag'])\n","submission_test_bilstm_crf_A['TokenID'] = test_df['TokenID']\n","#submission_test_bilstm_crf_A['Tag'] = test_pred_ner_no_pad\n","\n","test_ids_concat = np.concatenate(test_ids)\n","for i in tqdm(range(len(test_df))):\n","  k = test_df[test_df[\"TokenID\"] == test_ids_concat[i]].index[0]\n","  submission_test_bilstm_crf_A.at[k, \"Tag\"] = test_pred_ner_no_pad[i]\n","\n","submission_test_bilstm_crf_A['Tag'] = submission_test_bilstm_crf_A['Tag'].map(index2tag)\n","submission_test_bilstm_crf_A"],"id":"XdI30f8intqQ","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 21711/21711 [00:42<00:00, 516.10it/s]\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TokenID</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>S0885230816301759-0</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>S0885230816301759-1</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>S0885230816301759-2</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>S0885230816301759-3</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>S0885230816301759-4</td>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>21706</th>\n","      <td>S1877750313001269-211</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>21707</th>\n","      <td>S1877750313001269-212</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>21708</th>\n","      <td>S1877750313001269-213</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>21709</th>\n","      <td>S1877750313001269-214</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>21710</th>\n","      <td>S1877750313001269-215</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>21711 rows × 2 columns</p>\n","</div>"],"text/plain":["                     TokenID Tag\n","0        S0885230816301759-0   O\n","1        S0885230816301759-1   O\n","2        S0885230816301759-2   O\n","3        S0885230816301759-3   O\n","4        S0885230816301759-4   B\n","...                      ...  ..\n","21706  S1877750313001269-211   O\n","21707  S1877750313001269-212   O\n","21708  S1877750313001269-213   O\n","21709  S1877750313001269-214   O\n","21710  S1877750313001269-215   O\n","\n","[21711 rows x 2 columns]"]},"metadata":{},"execution_count":89}]},{"cell_type":"code","metadata":{"id":"8lUIdJFenzQQ"},"source":["submission_test_bilstm_crf_A.to_csv('Test_submission_Glove_Bilstm_CRF_tache_A.csv',index=False)\n","submission_val_bilstm_crf_A.to_csv('Val_submission_Glove_Bilstm_CRF_tache_A.csv',index=False)"],"id":"8lUIdJFenzQQ","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tc64ySlqtUnb"},"source":["### BERT"],"id":"tc64ySlqtUnb"},{"cell_type":"markdown","metadata":{"id":"7mSobTNUsnq2"},"source":["#### Imports"],"id":"7mSobTNUsnq2"},{"cell_type":"code","metadata":{"id":"B1IvTNQFtZMs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638741813850,"user_tz":300,"elapsed":3776,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"ae429abc-775d-40a1-e581-5c006905241d"},"source":["!pip install transformers"],"id":"B1IvTNQFtZMs","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"]}]},{"cell_type":"code","metadata":{"id":"CdwLpR5ntyHb"},"source":["import transformers\n","from transformers import BertTokenizer\n","from transformers import BertForTokenClassification, AdamW, BertConfig, BertModel\n","from transformers import get_linear_schedule_with_warmup\n","\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from torch.utils.data import TensorDataset, random_split\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","from nltk.tokenize import sent_tokenize\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import time\n","import random\n","import numpy as np\n","import datetime"],"id":"CdwLpR5ntyHb","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1BsyYo_xuEvB"},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"id":"1BsyYo_xuEvB","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qVhYSfuSxGhO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638741814941,"user_tz":300,"elapsed":6,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"866a7838-4933-48f8-8449-1d09cdb90b96"},"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"id":"qVhYSfuSxGhO","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla K80\n"]}]},{"cell_type":"markdown","metadata":{"id":"JFWQNhELtHQu"},"source":["#### Preprocessing for BERT"],"id":"JFWQNhELtHQu"},{"cell_type":"markdown","metadata":{"id":"7leTD9ZGjXNR"},"source":["Since BERT is a massive architecture we can no longer use entire documents ans we need to use sentences instead. Indee the maximal length of the documents is around 350 whereas the maximal length of the sentences is around 200.  \n","Since we don't have clear sentence delimitation in the .csv documents, we use nltk sentence tokenizer on the .txt documents and then the nltk word tokenizer on the sentences. Finally we sequentially attribute the labels we got in the previous parts to the tokens we just optained. Unfortunatly, it seems that the .cvs files don't exactly used this technic to optain their tokens because some sentences are missing points or some token we optaned correspond to several tokens in the csv file.  \n","When the problem is just a missing point we add it. When tokens are not the same with the two tokenization strategy, since these examples are very rare, we just delete them from the dataset."],"id":"7leTD9ZGjXNR"},{"cell_type":"code","metadata":{"id":"qJmO4yZP46bn"},"source":["# We retrieve the texts directly from the .txt files to apply the sentence tokeniezer\n","def get_text_docs(folder):\n","  text_docs = []\n","  print(\"Loading docs...\")\n","  for i, doc_id in enumerate(tqdm(doc_ids[folder])):\n","    with open(f\"data/{folder}/{doc_id}.txt\", \"r\") as file:\n","      text = file.read()[:-1]\n","    text_docs.append(text)\n","  print(\"   DONE.\")\n","  print()\n","  return text_docs\n"],"id":"qJmO4yZP46bn","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Swtmc3J96Myd"},"source":["# We then construct the tokenized sentences and assign the labels\n","\n","def get_doc_sentences(text_docs, folder, original_sentences, doc_tags, ids=None):\n","  # We construct the list sentences (list of tokens) based on nltk sent_tokenize and nltk world_tokenizer\n","  doc_sentences0 = []\n","  print(\"Constructing sentences...\")\n","  for text in tqdm(text_docs):\n","    doc_sentences0.append([np.array(word_tokenize(sent), dtype=str) for sent in sent_tokenize(text)])\n","  diff = 0\n","  invalid_indices = []\n","  print()\n","  for i, (sentences, tokens) in enumerate(zip(doc_sentences0, original_sentences)):\n","    sentence_tokens = np.concatenate(sentences)\n","    if sentence_tokens.size != tokens.size:\n","      doc_sentences0[i][-1] = np.concatenate([doc_sentences0[i][-1], np.array([\".\"])])\n","      if np.concatenate(doc_sentences0[i]).size != tokens.size:\n","        # When sent_tokenizer + word_tokenizer doesn't match the tokens form the .csv file\n","        print(f\" Doc {doc_ids[folder][i]} invalid\")\n","        invalid_indices.append(i)\n","        print(list(np.concatenate(doc_sentences0[i])))\n","        print(list(tokens))\n","        print()\n","  \n","  print(\"   DONE.\")\n","  print()\n","    \n","  # Since we noticed there is only one sentence in the dataset that is wrongly tokenized, we simply delete it from dataset\n","  doc_sentences = [doc_sentences0[i] for i in range(len(doc_sentences0)) if i not in invalid_indices]\n","  if doc_tags is not None: tags_valid = [doc_tags[i] for i in range(len(doc_tags)) if i not in invalid_indices]\n","\n","  # Setting tags to the new tokens\n","  if doc_tags is not None: \n","    doc_tags_ = []\n","    for tags, sentences in zip(tags_valid, doc_sentences):\n","      doc_tags_.append([])\n","      n = 0\n","      for sentence in sentences:\n","        doc_tags_[-1].append(np.array(tags[n:n+len(sentence)]))\n","        n += len(sentence)\n","  \n","  # We flatten the doc_sentences structure to have only one list of sentence independently of the documents\n","  all_sentences = [sentence for sentences in doc_sentences for sentence in sentences]\n","  if doc_tags is not None: \n","    all_tags = [tags for sentence_tags in doc_tags_ for tags in sentence_tags]\n","    print(\"Same number of sentences and tag sequences:\", len(all_sentences) == len(all_tags))\n","    if len(all_sentences) == len(all_tags):\n","      print(\"Size:\",len(all_sentences))\n","\n","    print(\"Looking at some sentences randomly:\")\n","    for i in np.random.choice(range(len(all_sentences)), size=5):\n","      print(list(zip(all_sentences[i], all_tags[i])))\n","\n","  else: all_tags = None\n","  \n","  return all_sentences, all_tags"],"id":"Swtmc3J96Myd","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EgX6bN1ioLNy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638741885376,"user_tz":300,"elapsed":67849,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"80288905-4134-47b2-eb18-d377b92a46eb"},"source":["print(\"==== Train dataset ====\")\n","all_sentences_train, all_tags_train = get_doc_sentences(get_text_docs('train'), 'train', train_sentences, train_tags)\n","print()\n","print()\n","print(\"==== Validation dataset ====\")\n","all_sentences_val, all_tags_val = get_doc_sentences(get_text_docs('val'), 'val', val_sentences, val_tags)"],"id":"EgX6bN1ioLNy","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==== Train dataset ====\n","Loading docs...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 350/350 [00:58<00:00,  5.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["   DONE.\n","\n","Constructing sentences...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 350/350 [00:00<00:00, 441.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Doc S0370269304009359 invalid\n","['Table', '1', 'lists', '8', 'pairs', 'of', 'B', 'decays', '.', 'In', 'fact', ',', 'there', 'are', 'more', 'decay', 'pairs', ',', 'since', 'many', 'of', 'the', 'particles', 'in', 'the', 'final', 'states', 'can', 'be', 'observed', 'as', 'either', 'pseudoscalar', '(', 'P', ')', 'or', 'vector', '(', 'V', ')', 'mesons', '.', 'Note', 'that', 'certain', 'decays', 'are', 'written', 'in', 'terms', 'of', 'VV', 'final', 'states', ',', 'while', 'others', 'are', 'have', 'PP', 'states', '.', 'There', 'are', 'three', 'reasons', 'for', 'this', '.', 'First', ',', 'some', 'decays', 'involve', 'a', 'final-state', 'π0', '.', 'However', ',', 'experimentally', 'it', 'will', 'be', 'necessary', 'to', 'find', 'the', 'decay', 'vertices', 'of', 'the', 'final', 'particles', '.', 'This', 'is', 'virtually', 'impossible', 'for', 'a', 'π0', ',', 'and', 'so', 'we', 'always', 'use', 'a', 'ρ0', '.', 'Second', ',', 'some', 'pairs', 'of', 'decays', 'are', 'related', 'by', 'SU', '(', '3', ')', 'in', 'the', 'SM', 'only', 'if', 'an', '(', 'ss¯', ')', 'quark', 'pair', 'is', 'used', '.', 'However', ',', 'there', 'are', 'no', 'P', \"'s\", 'which', 'are', 'pure', '(', 'ss¯', ')', '.', 'The', 'mesons', 'η', 'and', 'η′', 'have', 'an', '(', 'ss¯', ')', 'component', ',', 'but', 'they', 'also', 'have', 'significant', '(', 'uu¯', ')', 'and', '(', 'dd¯', ')', 'pieces', '.', 'As', 'a', 'result', 'the', 'b¯→s¯', 'and', 'b¯→d¯', 'decays', 'are', 'not', 'really', 'related', 'by', 'SU', '(', '3', ')', 'in', 'the', 'SM', 'if', 'the', 'final', 'state', 'involves', 'an', 'η', 'or', 'η′', '.', 'We', 'therefore', 'consider', 'instead', 'the', 'vector', 'meson', 'ϕ', 'which', 'is', 'essentially', 'a', 'pure', '(', 'ss¯', ')', 'quark', 'state', '.', 'Finally', ',', 'we', 'require', 'that', 'both', 'B0', 'and', 'B¯0', 'be', 'able', 'to', 'decay', 'to', 'the', 'final', 'state', '.', 'This', 'can', 'not', 'happen', 'if', 'the', 'final', 'state', 'contains', 'a', 'single', 'K0', '(', 'or', 'K¯0', ')', 'meson', '.', 'However', ',', 'it', 'can', 'occur', 'if', 'this', 'final-state', 'particle', 'is', 'an', 'excited', 'neutral', 'kaon', '.', 'In', 'this', 'case', 'one', 'decay', 'involves', 'K*0', ',', 'while', 'the', 'other', 'has', 'K¯*0', '.', 'Assuming', 'that', 'the', 'vector', 'meson', 'is', 'detected', 'via', 'its', 'decay', 'to', 'ψKsπ0', '(', 'as', 'in', 'the', 'measurement', 'of', 'sin2β', 'via', 'Bd0', '(', 't', ')', '→J/ψK*', ')', ',', 'then', 'both', 'B0', 'and', 'B¯0', 'can', 'decay', 'to', 'the', 'same', 'final', 'state', '.', '.']\n","['Table', '1', 'lists', '8', 'pairs', 'of', 'B', 'decays', '.', 'In', 'fact', ',', 'there', 'are', 'more', 'decay', 'pairs', ',', 'since', 'many', 'of', 'the', 'particles', 'in', 'the', 'final', 'states', 'can', 'be', 'observed', 'as', 'either', 'pseudoscalar', '(', 'P', ')', 'or', 'vector', '(', 'V', ')', 'mesons', '.', 'Note', 'that', 'certain', 'decays', 'are', 'written', 'in', 'terms', 'of', 'VV', 'final', 'states', ',', 'while', 'others', 'are', 'have', 'PP', 'states', '.', 'There', 'are', 'three', 'reasons', 'for', 'this', '.', 'First', ',', 'some', 'decays', 'involve', 'a', 'final-state', 'π0', '.', 'However', ',', 'experimentally', 'it', 'will', 'be', 'necessary', 'to', 'find', 'the', 'decay', 'vertices', 'of', 'the', 'final', 'particles', '.', 'This', 'is', 'virtually', 'impossible', 'for', 'a', 'π0', ',', 'and', 'so', 'we', 'always', 'use', 'a', 'ρ0', '.', 'Second', ',', 'some', 'pairs', 'of', 'decays', 'are', 'related', 'by', 'SU', '(', '3', ')', 'in', 'the', 'SM', 'only', 'if', 'an', '(', 'ss¯', ')', 'quark', 'pair', 'is', 'used', '.', 'However', ',', 'there', 'are', 'no', 'P', \"'s\", 'which', 'are', 'pure', '(', 'ss¯', ')', '.', 'The', 'mesons', 'η', 'and', 'η′', 'have', 'an', '(', 'ss¯', ')', 'component', ',', 'but', 'they', 'also', 'have', 'significant', '(', 'uu¯', ')', 'and', '(', 'dd¯', ')', 'pieces', '.', 'As', 'a', 'result', 'the', 'b¯→s¯', 'and', 'b¯→d¯', 'decays', 'are', 'not', 'really', 'related', 'by', 'SU', '(', '3', ')', 'in', 'the', 'SM', 'if', 'the', 'final', 'state', 'involves', 'an', 'η', 'or', 'η′', '.', 'We', 'therefore', 'consider', 'instead', 'the', 'vector', 'meson', 'ϕ', 'which', 'is', 'essentially', 'a', 'pure', '(', 'ss¯', ')', 'quark', 'state', '.', 'Finally', ',', 'we', 'require', 'that', 'both', 'B0', 'and', 'B¯0', 'be', 'able', 'to', 'decay', 'to', 'the', 'final', 'state', '.', 'This', 'can', 'not', 'happen', 'if', 'the', 'final', 'state', 'contains', 'a', 'single', 'K0', '(', 'or', 'K¯0', ')', 'meson', '.', 'However', ',', 'it', 'can', 'occur', 'if', 'this', 'final-state', 'particle', 'is', 'an', 'excited', 'neutral', 'kaon', '.', 'In', 'this', 'case', 'one', 'decay', 'involves', 'K', '*', '0', ',', 'while', 'the', 'other', 'has', 'K¯', '*', '0', '.', 'Assuming', 'that', 'the', 'vector', 'meson', 'is', 'detected', 'via', 'its', 'decay', 'to', 'ψKsπ0', '(', 'as', 'in', 'the', 'measurement', 'of', 'sin2β', 'via', 'Bd0', '(', 't', ')', '→J/ψK', '*', ')', ',', 'then', 'both', 'B0', 'and', 'B¯0', 'can', 'decay', 'to', 'the', 'same', 'final', 'state', '.']\n","\n","   DONE.\n","\n","Same number of sentences and tag sequences: True\n","Size: 2402\n","Looking at some sentences randomly:\n","[('In', 'O'), ('this', 'O'), ('case', 'O'), (',', 'O'), ('the', 'O'), ('experimental', 'B'), ('data', 'L'), ('become', 'O'), ('entirely', 'O'), ('integrated', 'O'), ('in', 'O'), ('the', 'O'), ('calibration', 'B'), ('process', 'L'), ('and', 'O'), ('an', 'O'), ('optimization', 'B'), ('routine', 'L'), ('is', 'O'), ('used', 'O'), ('to', 'O'), ('quantify', 'B'), ('the', 'I'), ('best', 'I'), ('set', 'I'), ('of', 'I'), ('parameters', 'L'), ('which', 'O'), ('explain', 'O'), ('the', 'O'), ('observed', 'O'), ('pyrolysis', 'B'), ('behaviour', 'L'), ('(', 'O'), ('i.e', 'O'), ('.', 'O')]\n","[('(', 'O'), ('4', 'O'), (')', 'O'), ('and', 'O'), ('also', 'O'), ('h=5.95', 'O'), ('obtained', 'O'), ('from', 'O'), ('the', 'O'), ('decay', 'O'), ('width', 'O'), ('.', 'O')]\n","[('Some', 'O'), ('nonlinear', 'B'), ('wave', 'I'), ('equations', 'L'), ('are', 'O'), ('more', 'O'), ('difficult', 'O'), ('to', 'O'), ('investigate', 'B'), ('mathematically', 'L'), (',', 'O'), ('as', 'O'), ('no', 'O'), ('general', 'O'), ('analytical', 'B'), ('method', 'L'), ('for', 'O'), ('their', 'O'), ('solutions', 'O'), ('exists', 'O'), ('.', 'O')]\n","[('In', 'O'), ('addition', 'O'), (',', 'O'), ('we', 'O'), ('propose', 'O'), ('the', 'O'), ('major', 'B'), ('types', 'I'), ('of', 'I'), ('learning', 'I'), ('scenarios', 'L'), ('for', 'O'), ('the', 'O'), ('use', 'O'), ('of', 'O'), ('the', 'O'), ('designed', 'O'), ('systems', 'O'), ('.', 'O')]\n","[('Thereupon', 'O'), (',', 'O'), ('the', 'O'), ('new', 'O'), ('emphases', 'O'), ('on', 'O'), ('sustainability', 'B'), ('of', 'I'), ('transportation', 'I'), ('system', 'L'), ('in', 'O'), ('megalopolis', 'O'), ('are', 'O'), ('creating', 'O'), ('new', 'O'), ('demands', 'O'), ('for', 'O'), ('adequate', 'O'), ('approach', 'B'), ('to', 'I'), ('measure', 'I'), ('its', 'I'), ('performance', 'I'), ('and', 'I'), ('diagnosis', 'I'), ('potential', 'I'), ('drawbacks', 'L'), ('.', 'O')]\n","\n","\n","==== Validation dataset ====\n","Loading docs...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 50/50 [00:07<00:00,  6.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["   DONE.\n","\n","Constructing sentences...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 50/50 [00:00<00:00, 359.26it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","   DONE.\n","\n","Same number of sentences and tag sequences: True\n","Size: 413\n","Looking at some sentences randomly:\n","[('Under', 'O'), ('the', 'O'), ('assumption', 'O'), ('that', 'O'), ('the', 'O'), ('noise', 'O'), ('present', 'O'), ('above', 'O'), ('1023Hz', 'O'), ('is', 'O'), ('negligible', 'O'), ('compared', 'O'), ('with', 'O'), ('the', 'O'), ('noise', 'O'), ('present', 'O'), ('below', 'O'), ('0.5Hz', 'O'), (',', 'O'), ('this', 'O'), ('procedure', 'O'), ('enables', 'O'), ('an', 'O'), ('accurate', 'B'), ('recording', 'I'), ('of', 'I'), ('the', 'I'), ('potential', 'I'), ('noise', 'I'), ('in', 'I'), ('the', 'I'), ('frequencies', 'I'), ('of', 'I'), ('interest', 'L'), (',', 'O'), ('avoiding', 'O'), ('aliasing', 'O'), ('of', 'O'), ('frequencies', 'O'), ('between', 'O'), ('0.5', 'O'), ('and', 'O'), ('1023Hz', 'O'), ('and', 'O'), ('minimizing', 'O'), ('the', 'O'), ('50Hz', 'O'), ('interference', 'O'), ('from', 'O'), ('the', 'O'), ('mains', 'O'), ('supply', 'O'), ('.', 'O')]\n","[('This', 'O'), ('particular', 'O'), ('version', 'O'), ('of', 'O'), ('SAFT', 'U'), ('provides', 'O'), ('a', 'O'), ('closed', 'B'), ('form', 'I'), ('EoS', 'L'), ('that', 'O'), ('describes', 'O'), ('the', 'O'), ('macroscopical', 'O'), ('properties', 'O'), ('of', 'O'), ('the', 'O'), ('Mie', 'B'), ('potential', 'L'), ('[', 'O'), ('56', 'O'), (']', 'O'), (',', 'O'), ('also', 'O'), ('known', 'O'), ('as', 'O'), ('the', 'O'), ('(', 'B'), ('m', 'I'), (',', 'I'), ('n', 'I'), (')', 'I'), ('potential', 'L'), (';', 'O'), ('a', 'O'), ('generalized', 'O'), ('form', 'O'), ('of', 'O'), ('the', 'O'), ('LJ', 'B'), ('potential', 'L'), ('(', 'O'), ('albeit', 'O'), ('predating', 'O'), ('it', 'O'), ('by', 'O'), ('decades', 'O'), (')', 'O'), ('.', 'O')]\n","[('An', 'O'), ('essential', 'O'), ('part', 'O'), ('of', 'O'), ('nuclear', 'B'), ('reactor', 'I'), ('analysis', 'L'), ('is', 'O'), ('the', 'O'), ('prediction', 'B'), ('of', 'I'), ('the', 'I'), ('three-dimensional', 'I'), ('space-time', 'I'), ('kinetics', 'I'), ('of', 'I'), ('neutrons', 'L'), ('in', 'O'), ('a', 'O'), ('relatively', 'O'), ('large', 'O'), (',', 'O'), ('finite', 'O'), (',', 'O'), ('heterogeneous', 'O'), (',', 'O'), ('three-dimensional', 'O'), ('reactor', 'B'), ('core', 'L'), ('.', 'O')]\n","[('As', 'O'), ('described', 'O'), ('in', 'O'), ('detail', 'O'), ('in', 'O'), ('Section', 'O'), ('3.1', 'O'), (',', 'O'), ('a', 'O'), ('Rosenblatt', 'B'), ('transformation', 'L'), ('allows', 'O'), ('for', 'O'), ('the', 'O'), ('mapping', 'O'), ('between', 'O'), ('any', 'O'), ('domain', 'O'), ('and', 'O'), ('the', 'O'), ('unit', 'O'), ('hypercube', 'O'), ('[', 'O'), ('0', 'O'), (',', 'O'), ('1', 'O'), (']', 'O'), ('D.', 'O'), ('With', 'O'), ('a', 'O'), ('double', 'O'), ('transformation', 'O'), ('we', 'O'), ('can', 'O'), ('reformulate', 'O'), ('the', 'O'), ('response', 'O'), ('function', 'O'), ('f', 'O'), ('asf', 'O'), ('(', 'O'), ('x', 'O'), (',', 'O'), ('t', 'O'), (',', 'O'), ('Q', 'O'), (')', 'O'), ('=f', 'O'), ('(', 'O'), ('x', 'O'), (',', 'O'), ('t', 'O'), (',', 'O'), ('TQ−1', 'O'), ('(', 'O'), ('TR', 'O'), ('(', 'O'), ('R', 'O'), (')', 'O'), (')', 'O'), (')', 'O'), ('≈fˆ', 'O'), ('(', 'O'), ('x', 'O'), (',', 'O'), ('t', 'O'), (',', 'O'), ('R', 'O'), (')', 'O'), ('=∑n∈INcn', 'O'), ('(', 'O'), ('x', 'O'), (',', 'O'), ('t', 'O'), (')', 'O'), ('Φn', 'O'), ('(', 'O'), ('R', 'O'), (')', 'O'), (',', 'O'), ('where', 'O'), ('R', 'O'), ('is', 'O'), ('any', 'O'), ('random', 'O'), ('variable', 'O'), ('drawn', 'O'), ('from', 'O'), ('pR', 'O'), (',', 'O'), ('which', 'O'), ('for', 'O'), ('simplicity', 'O'), ('is', 'O'), ('chosen', 'O'), ('to', 'O'), ('consists', 'O'), ('of', 'O'), ('independent', 'O'), ('components', 'O'), ('.', 'O')]\n","[('The', 'O'), ('aim', 'O'), ('of', 'O'), ('this', 'O'), ('paper', 'O'), ('is', 'O'), ('to', 'O'), ('provide', 'B'), ('a', 'I'), ('complete', 'I'), ('assignment', 'I'), ('of', 'I'), ('the', 'I'), ('vibrational', 'I'), ('spectra', 'I'), ('of', 'I'), ('l-cysteine', 'L'), ('in', 'O'), ('both', 'O'), ('the', 'O'), ('orthorhombic', 'O'), ('and', 'O'), ('monoclinic', 'O'), ('forms', 'O'), ('by', 'O'), ('the', 'O'), ('use', 'O'), ('of', 'O'), ('a', 'O'), ('combination', 'O'), ('of', 'O'), ('computational', 'B'), ('and', 'I'), ('experimental', 'I'), ('methods', 'L'), ('.', 'O')]\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","metadata":{"id":"-09SY2W5ScVY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638741888763,"user_tz":300,"elapsed":3307,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"a2186b7d-37ac-4202-ad54-7308e66ad6c0"},"source":["lengths = []\n","print('Measuring lengths')\n","for sen in tqdm(all_sentences_train + all_sentences_val):\n","    sen = ' '.join(sen)\n","    encoded_sent = tokenizer.encode(sen, add_special_tokens = True)\n","    \n","    lengths.append(len(encoded_sent))\n","print()\n","print('   Min length: {:,} tokens'.format(min(lengths)))\n","print('   Max length: {:,} tokens'.format(max(lengths)))\n","print('   Median length: {:,} tokens'.format(int(np.median(lengths))))"],"id":"-09SY2W5ScVY","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Measuring lengths\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2815/2815 [00:03<00:00, 853.74it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","   Min length: 4 tokens\n","   Max length: 187 tokens\n","   Median length: 31 tokens\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","metadata":{"id":"JBl4xhlCSdkV","colab":{"base_uri":"https://localhost:8080/","height":394},"executionInfo":{"status":"ok","timestamp":1638741891969,"user_tz":300,"elapsed":3213,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"43ab301a-4a0b-462a-a677-81b1a8b8d1b8"},"source":["sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (20,5)\n","\n","# Plot the distribution of comment lengths.\n","sns.displot(lengths, kde=False, rug=False)\n","\n","plt.title('Sentence Lengths')\n","plt.xlabel('Sentence Length')\n","plt.ylabel('# of Sentences')\n","plt.show()"],"id":"JBl4xhlCSdkV","execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 360x360 with 1 Axes>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"b6FP_MBJSyoy"},"source":["MAX_LEN = 200"],"id":"b6FP_MBJSyoy","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6aNnwmp-Kxxs"},"source":["# Since we built our sentence with nltk tokenizers, we need to adapt the tagging system \n","# We don't use the tutorial version because it doesn't consider, for example, that the character '-' is considered to devide a word for bert but not for nltk\n","null_tag = '-N-'\n","def tokenize_with_tags(tokens, tags):\n","  complete_tags = [null_tag]\n","\n","  for token, tag in zip(tokens, tags):\n","    complete_tags += [tag] + [null_tag] * (len(tokenizer.tokenize(token)) - 1)\n","  complete_tags.append(null_tag)\n","  return complete_tags"],"id":"6aNnwmp-Kxxs","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sGmUJWpoPss-"},"source":["# This function is used for printing comprehensive representation of the data tokenized by BERT. \n","\n","def get_bert_token(token_id):\n","  token_id = token_id.numpy().item()\n","  if token_id == tokenizer.pad_token_id:\n","    return \"PAD\"\n","  elif token_id == tokenizer.cls_token_id:\n","    return \"CLS\"\n","  elif token_id == tokenizer.sep_token_id:\n","    return \"SEP\"\n","  else:\n","    return tokenizer.ids_to_tokens[token_id]"],"id":"sGmUJWpoPss-","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CMbyVDpQNlZw"},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences"],"id":"CMbyVDpQNlZw","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7FapmJeFMSAb"},"source":["# def pad_sequences(sequences, max_len, value,dtype=object):\n","#   ret = np.zeros((len(sequences),max_len), dtype=dtype)\n","#   for i, sequence in enumerate(sequences):\n","#     for j, elt in enumerate(sequence):\n","#       ret[i, j] = elt\n","#     ret[i,len(sequence):] = value\n","#     print(ret)\n","#     print()\n","#   return ret"],"id":"7FapmJeFMSAb","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RjmPoRT6K_CH"},"source":["# a = [[\"aze\", \"fe\", \"e\"]]\n","# pad_sequences(a, 10, value='N')"],"id":"RjmPoRT6K_CH","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T3NTStWqtxuB"},"source":["# We construct final vectors, ready to be transformed into torch tensors.\n","\n","def get_final_data(sentences, all_tags):\n","  if all_tags is not None: \n","    print(\"Adapting tags to BERT tokenizer...\")\n","    ta = [tokenize_with_tags(tokens, tags) for tokens, tags in tqdm(list(zip(sentences, all_tags)))]\n","    print(\"   DONE.\")\n","    tags_tokenized = pad_sequences(ta, maxlen=MAX_LEN, dtype=object, value=null_tag, padding='post')\n","  else:\n","    tags_tokenized = None\n","  sentences_X = []\n","  attention_masks = []\n","  print(\"Applying BERT tokenizer to the sentences...\")\n","  for tokens in tqdm(sentences):\n","    sentence = \" \".join(tokens)\n","    encoded_dict = tokenizer(sentence, \n","                      add_special_tokens=True, \n","                      max_length=MAX_LEN,\n","                      padding=\"max_length\", \n","                      return_attention_mask=True, \n","                      return_tensors ='pt')\n","    sentences_X.append(encoded_dict['input_ids'][0])\n","    attention_masks.append(encoded_dict['attention_mask'][0])\n","  print(\"   DONE.\")\n","  print()\n","  if all_tags is not None:\n","    print(\"Looking at some sentences randomly:\")\n","    for i in np.random.choice(range(len(sentences_X)), size=5):\n","      print(list(zip([get_bert_token(elt) for elt in sentences_X[i]], tags_tokenized[i], [elt.item() for elt in attention_masks[i]])))\n","\n","  return sentences_X, attention_masks, tags_tokenized\n"],"id":"T3NTStWqtxuB","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fd5IcigFwhNc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638741901507,"user_tz":300,"elapsed":9561,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"13fdf5f5-d18f-461e-a043-9204aa036f59"},"source":["print(\"==== Train dataset ====\")\n","train_sentences_X, train_attention_masks, train_tags_tokenized = get_final_data(all_sentences_train, all_tags_train)\n","print()\n","print()\n","print(\"==== Validation dataset ====\")\n","val_sentences_X, val_attention_masks, val_tags_tokenized = get_final_data(all_sentences_val, all_tags_val)"],"id":"Fd5IcigFwhNc","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==== Train dataset ====\n","Adapting tags to BERT tokenizer...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2402/2402 [00:06<00:00, 372.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["   DONE.\n","Applying BERT tokenizer to the sentences...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2402/2402 [00:03<00:00, 705.34it/s]\n"]},{"output_type":"stream","name":"stdout","text":["   DONE.\n","\n","Looking at some sentences randomly:\n","[('CLS', '-N-', 1), ('micro', 'U', 1), ('##hard', '-N-', 1), ('##ness', '-N-', 1), ('can', 'O', 1), ('be', 'O', 1), ('related', 'O', 1), ('to', 'O', 1), ('other', 'O', 1), ('macro', 'O', 1), ('##scopic', '-N-', 1), ('mechanical', 'O', 1), ('properties', 'O', 1), ('such', 'O', 1), ('as', 'O', 1), ('yield', 'O', 1), ('stress', 'O', 1), (',', 'O', 1), ('σ', 'O', 1), (',', 'O', 1), ('and', 'O', 1), ('elastic', 'O', 1), ('mod', 'O', 1), ('##ulus', '-N-', 1), (',', 'O', 1), ('e', 'O', 1), (',', 'O', 1), ('both', 'O', 1), ('derived', 'O', 1), ('from', 'O', 1), ('compression', 'B', 1), ('testing', 'L', 1), ('.', 'O', 1), ('SEP', '-N-', 1), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0)]\n","[('CLS', '-N-', 1), ('longitudinal', 'B', 1), ('beam', 'I', 1), ('and', 'I', 1), ('target', 'I', 1), ('single', 'I', 1), ('-', '-N-', 1), ('spin', '-N-', 1), ('as', 'L', 1), ('##ym', '-N-', 1), ('##met', '-N-', 1), ('##ries', '-N-', 1), ('have', 'O', 1), ('been', 'O', 1), ('at', 'O', 1), ('the', 'O', 1), ('center', 'O', 1), ('of', 'O', 1), ('the', 'O', 1), ('attention', 'O', 1), ('lately', 'O', 1), (',', 'O', 1), ('since', 'O', 1), ('they', 'O', 1), ('have', 'O', 1), ('been', 'O', 1), ('measured', 'O', 1), ('by', 'O', 1), ('the', 'O', 1), ('hermes', 'B', 1), ('and', 'I', 1), ('cl', 'I', 1), ('##as', '-N-', 1), ('experimental', 'I', 1), ('collaborations', 'L', 1), ('[', 'O', 1), ('1', 'O', 1), ('–', '-N-', 1), ('4', '-N-', 1), (']', 'O', 1), ('and', 'O', 1), ('more', 'O', 1), ('measurements', 'O', 1), ('are', 'O', 1), ('planned', 'O', 1), ('.', 'O', 1), ('SEP', '-N-', 1), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0)]\n","[('CLS', '-N-', 1), ('a', 'O', 1), ('process', 'O', 1), ('describes', 'O', 1), ('the', 'O', 1), ('activities', 'O', 1), ('and', 'O', 1), ('relationships', 'O', 1), ('among', 'O', 1), ('them', 'O', 1), ('.', 'O', 1), ('SEP', '-N-', 1), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0)]\n","[('CLS', '-N-', 1), ('since', 'O', 1), ('both', 'O', 1), ('regression', 'U', 1), ('##s', '-N-', 1), ('predict', 'O', 1), ('air', 'B', 1), ('void', 'I', 1), ('content', 'L', 1), ('at', 'O', 1), ('a', 'O', 1), ('maximum', 'O', 1), ('difference', 'O', 1), ('of', 'O', 1), ('0', 'O', 1), ('.', '-N-', 1), ('56', '-N-', 1), ('%', 'O', 1), (',', 'O', 1), ('which', 'O', 1), ('is', 'O', 1), ('within', 'O', 1), ('the', 'O', 1), ('uncertainty', 'O', 1), ('of', 'O', 1), ('the', 'O', 1), ('core', 'B', 1), ('measurement', 'I', 1), ('precision', 'L', 1), ('of', 'O', 1), ('0', 'O', 1), ('.', '-N-', 1), ('7', '-N-', 1), ('%', 'O', 1), (',', 'O', 1), ('use', 'O', 1), ('of', 'O', 1), ('either', 'O', 1), ('the', 'O', 1), ('initial', 'B', 1), ('or', 'I', 1), ('repeat', 'I', 1), ('run', 'I', 1), ('regression', 'I', 1), ('predictions', 'L', 1), ('are', 'O', 1), ('appropriate', 'O', 1), ('.', 'O', 1), ('SEP', '-N-', 1), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0)]\n","[('CLS', '-N-', 1), ('however', 'O', 1), (',', 'O', 1), ('it', 'O', 1), ('is', 'O', 1), ('not', 'O', 1), ('without', 'O', 1), ('its', 'O', 1), ('faults', 'O', 1), ('.', 'O', 1), ('SEP', '-N-', 1), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0)]\n","\n","\n","==== Validation dataset ====\n","Adapting tags to BERT tokenizer...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 413/413 [00:01<00:00, 361.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["   DONE.\n","Applying BERT tokenizer to the sentences...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 413/413 [00:00<00:00, 734.14it/s]"]},{"output_type":"stream","name":"stdout","text":["   DONE.\n","\n","Looking at some sentences randomly:\n","[('CLS', '-N-', 1), ('3', 'O', 1), (',', 'O', 1), ('each', 'O', 1), ('failed', 'B', 1), ('face', 'L', 1), ('is', 'O', 1), ('a', 'O', 1), ('graph', 'O', 1), ('node', 'O', 1), ('and', 'O', 1), ('each', 'O', 1), ('pair', 'O', 1), ('of', 'O', 1), ('neighbouring', 'O', 1), ('failed', 'B', 1), ('faces', 'L', 1), ('is', 'O', 1), ('a', 'O', 1), ('graph', 'O', 1), ('edge', 'O', 1), ('.', 'O', 1), ('SEP', '-N-', 1), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0)]\n","[('CLS', '-N-', 1), ('where', 'O', 1), ('two', 'O', 1), ('micro', 'U', 1), ('-', '-N-', 1), ('cracks', '-N-', 1), ('formed', 'O', 1), ('a', 'O', 1), ('continuous', 'O', 1), ('larger', 'O', 1), ('crack', 'U', 1), ('.', 'O', 1), ('SEP', '-N-', 1), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0)]\n","[('CLS', '-N-', 1), ('long', 'B', 1), ('-', '-N-', 1), ('chain', '-N-', 1), ('qu', 'I', 1), ('##ater', '-N-', 1), ('##nary', '-N-', 1), ('am', 'I', 1), ('##monium', '-N-', 1), ('bro', 'L', 1), ('##mide', '-N-', 1), ('##s', '-N-', 1), ('were', 'O', 1), ('also', 'O', 1), ('reported', 'O', 1), ('to', 'O', 1), ('work', 'O', 1), ('as', 'O', 1), ('efficient', 'O', 1), ('cis', 'U', 1), ('for', 'O', 1), ('steel', 'B', 1), ('materials', 'L', 1), ('[', 'O', 1), ('106', 'O', 1), (']', 'O', 1), ('.', 'O', 1), ('SEP', '-N-', 1), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0)]\n","[('CLS', '-N-', 1), ('moreover', 'O', 1), (',', 'O', 1), ('even', 'O', 1), ('for', 'O', 1), ('it', 'U', 1), ('##er', '-N-', 1), ('this', 'O', 1), ('study', 'O', 1), ('could', 'O', 1), ('be', 'O', 1), ('useful', 'O', 1), ('if', 'O', 1), ('carbon', 'B', 1), ('materials', 'L', 1), ('have', 'O', 1), ('to', 'O', 1), ('be', 'O', 1), ('eventually', 'O', 1), ('installed', 'O', 1), ('in', 'O', 1), ('the', 'O', 1), ('case', 'O', 1), ('that', 'O', 1), ('operation', 'O', 1), ('with', 'O', 1), ('tung', 'B', 1), ('##sten', '-N-', 1), ('tiles', 'L', 1), ('at', 'O', 1), ('the', 'O', 1), ('strike', 'O', 1), ('points', 'O', 1), ('is', 'O', 1), ('pre', 'O', 1), ('##cl', '-N-', 1), ('##uded', '-N-', 1), ('by', 'O', 1), ('unexpected', 'O', 1), ('reasons', 'O', 1), ('.', 'O', 1), ('SEP', '-N-', 1), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0)]\n","[('CLS', '-N-', 1), ('[', 'O', 1), ('4', 'O', 1), (']', 'O', 1), ('took', 'O', 1), ('a', 'O', 1), ('more', 'O', 1), ('systematic', 'O', 1), ('approach', 'O', 1), ('to', 'O', 1), ('such', 'O', 1), ('glass', 'B', 1), ('-', '-N-', 1), ('ceramic', '-N-', 1), ('waste', 'L', 1), ('##forms', '-N-', 1), ('.', 'O', 1), ('SEP', '-N-', 1), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0)]\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","metadata":{"id":"35ewV8j_xA-p"},"source":["# Map each unique label to an integer.\n","# label_map = {'N':-100} # we don't keep that to be able to apply one_hot encoding for custom BERT\n","label_map = {}\n","\n","# For each label...\n","for (i, label) in enumerate(set([tag for L in train_tags_tokenized for tag in L])):\n","    \n","    # Map it to its integer.\n","    label_map[label] = i"],"id":"35ewV8j_xA-p","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"40hQCbXDeE2Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638741901598,"user_tz":300,"elapsed":11,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"3abefb21-d897-4c99-fbd3-5f88fa9a6234"},"source":["print(label_map)"],"id":"40hQCbXDeE2Z","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'O': 0, 'I': 1, '-N-': 2, 'U': 3, 'B': 4, 'L': 5}\n"]}]},{"cell_type":"code","metadata":{"id":"v9iE9FpYTbLo"},"source":["train_new_labels = [[label_map[elt] for elt in L] for L in train_tags_tokenized]\n","val_new_labels = [[label_map[elt] for elt in L] for L in val_tags_tokenized]"],"id":"v9iE9FpYTbLo","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zj6vhuEfy4Ou"},"source":["# We construct the torch data structures that will be used for creating batches to pass to the different Bert models\n","\n","def get_data_loader(sentences_X, attention_masks, new_labels, sampler):\n","  pt_input_ids = torch.stack(sentences_X, dim=0)\n","  pt_attention_masks = torch.stack(attention_masks, dim=0)\n","  if new_labels is not None: \n","    pt_labels = torch.tensor(new_labels, dtype=torch.long)\n","\n","    dataset = TensorDataset(pt_input_ids, pt_attention_masks, pt_labels)\n","\n","  else:\n","    dataset = TensorDataset(pt_input_ids, pt_attention_masks)\n","\n","  batch_size = 16\n","  dataloader = DataLoader(\n","            dataset,  # The training samples.\n","            sampler = sampler(dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","  return dataloader"],"id":"Zj6vhuEfy4Ou","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KVkGyWjWzuuX"},"source":["train_dataloader = get_data_loader(train_sentences_X, train_attention_masks, train_new_labels, RandomSampler)\n","validation_dataloader = get_data_loader(val_sentences_X, val_attention_masks, val_new_labels, SequentialSampler)"],"id":"KVkGyWjWzuuX","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JuI-p8_xtW4o"},"source":["#### Model definition"],"id":"JuI-p8_xtW4o"},{"cell_type":"code","metadata":{"id":"DiWmYEF6xg77"},"source":["def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n"],"id":"DiWmYEF6xg77","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uA1wuYz_e5QW"},"source":["def get_BertForTokenClassification(label_map):\n","  model = BertForTokenClassification.from_pretrained(\n","    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = len(label_map) + 1, # The number of output labels\n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n","  )\n","  return model"],"id":"uA1wuYz_e5QW","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yvJCu8chcnOJ"},"source":["# Here we difine different strategies for aggregating the last layers of Bert into a single vector to pass to the fine-tuned part\n","\n","def Nth_layer(layer_list, N):\n","  \"\"\"return layer at index N statring from the end. For example to get last lyer you should use N=0\"\"\"\n","  return layer_list[-(N+1)]\n","\n","def get_Nth_layer(N):\n","  return lambda layer_list: Nth_layer(layer_list, N)\n","\n","last_layer_alone = get_Nth_layer(0)\n","\n","def mean_N_last_layer(layer_list, N):\n","  return torch.mean(torch.stack(layer_list[-N:]),dim=0)\n","\n","def get_mean_N_last_layer(N):\n","  return lambda layer_list: mean_N_last_layer(layer_list, N)\n","  \n"],"id":"yvJCu8chcnOJ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qHkB69nqgZJy"},"source":["import torch\n","import torch.nn as nn\n","from transformers import BertModel\n","\n","class BertCustom(nn.Module):\n","    def __init__(self, aggregation_stategy=last_layer_alone, H=50, freeze_bert=False):\n","        super(BertCustom, self).__init__()\n","        # Instantiate BERT model\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","\n","        # Bert-base embedding have dim 768\n","        D_in, H, D_out =  768, H, len(label_map) + 1\n","\n","        # Instantiate an one-layer feed-forward network\n","        self.classifier = nn.Sequential(\n","            nn.Linear(D_in, H),\n","            nn.ReLU(),\n","            #nn.Dropout(0.5),\n","            nn.Linear(H, D_out)\n","        )\n","\n","        # Freeze the BERT model\n","        if freeze_bert:\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","\n","        self.aggregation_stategy = aggregation_stategy\n","        \n","    def forward(self, input_ids, attention_mask,labels):\n","        # Feed input to BERT\n","        outputs = self.bert(input_ids=input_ids,\n","                            attention_mask=attention_mask,\n","                            output_hidden_states=True)\n","        \n","        # Extract the last hidden state of the token `[CLS]` for classification task\n","        hidden_state_aggregation = self.aggregation_stategy(outputs.hidden_states)\n","\n","        # Feed input to classifier to compute logits\n","        logits = self.classifier(hidden_state_aggregation)\n","\n","        # Compute loss\n","        if labels is None:\n","          return logits, None\n","\n","        loss_fn = nn.CrossEntropyLoss()\n","\n","        cat_labels = nn.functional.one_hot(labels, len(label_map) + 1).type(torch.float32)\n","        cat_labels.to(device)\n","\n","        loss = loss_fn(logits,cat_labels)\n","        return logits, loss"],"id":"qHkB69nqgZJy","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jmb91ItTlnZi"},"source":["def get_Bert_custom(aggregation_stategy=last_layer_alone,H=50):\n","  return BertCustom(aggregation_stategy=aggregation_stategy, H=H)"],"id":"jmb91ItTlnZi","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yJ1ne16LyRXm"},"source":["#### Model training and saving explicit"],"id":"yJ1ne16LyRXm"},{"cell_type":"code","metadata":{"id":"FQeoRjOywpwm","colab":{"base_uri":"https://localhost:8080/","height":161,"referenced_widgets":["24dee2ed298e466b8b9cdc1afe66b729","dc4ba2d02a344bb29427e44a81dd3485","add506e0d3a94f1b9244f78fee3098b3","0f36735185f74b92bd5f093fdfc2fa84","c357de13e50a473e942884cab6393f37","800640c0e2e945be9eb7bf64c751079c","b908e8268d194f7cba5e3ad848301fa0","dab07a1b62e84ebaae993735b5ab4516","6a6da3cd54314b0a8dde1200fb2ba6a4","bf16cec6e7de49dda8d258405b7eae02","63c546fa1d2b48dbb3f73ef19348d80e"]},"executionInfo":{"status":"ok","timestamp":1638741993782,"user_tz":300,"elapsed":29928,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"c2eeb6de-98f5-42aa-9be2-5e03bea2eb53"},"source":["# Load model \n","# model = get_Bert_custom(aggregation_stategy=get_mean_N_last_layer(6))\n","model = get_BertForTokenClassification(label_map)\n","\n","# # Tell pytorch to run this model on the GPU.\n","model = model.to(device)"],"id":"FQeoRjOywpwm","execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"24dee2ed298e466b8b9cdc1afe66b729","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","metadata":{"id":"R0oraGZ1xm-i"},"source":["# Load the AdamW optimizer\n","optimizer = AdamW(model.parameters(),\n","                  lr = 5e-5, # args.learning_rate \n","                  eps = 1e-8 # args.adam_epsilon \n","                )\n"],"id":"R0oraGZ1xm-i","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zQjj-0Dyx-2O"},"source":["# Number of training epochs \n","epochs = 5\n","\n","# Total number of training steps is number of batches * number of epochs.\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0,\n","                                            num_training_steps = total_steps)"],"id":"zQjj-0Dyx-2O","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zKF_39bzyZkj","colab":{"base_uri":"https://localhost:8080/","height":492},"executionInfo":{"status":"error","timestamp":1638742011287,"user_tz":300,"elapsed":17513,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"eab1affb-3113-4999-ed09-18bed10038df"},"source":["# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Store the average loss after each epoch so we can plot them.\n","loss_values = []\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","  # print(i)\n","  # print(isinstance(model,BertClassifier))\n","    \n","  # ========================================\n","  #               Training\n","  # ========================================\n","  \n","  # Perform one full pass over the training set.\n","\n","  print(\"\")\n","  print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","  print('Training...')\n","\n","  # Measure how long the training epoch takes.\n","  t0 = time.time()\n","\n","  # Reset the total loss for this epoch.\n","  total_loss = 0\n","\n","  # Put the model into training mode. Don't be mislead--the call to \n","  # `train` just changes the *mode*, it doesn't *perform* the training.\n","  # `dropout` and `batchnorm` layers behave differently during training\n","  # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","  model.train()\n","\n","  # For each batch of training data...\n","  for step, batch in enumerate(tqdm(train_dataloader)):\n","\n","      # Unpack this training batch from our dataloader. \n","      #\n","      # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","      # `to` method.\n","      #\n","      # `batch` contains three pytorch tensors:\n","      #   [0]: input ids \n","      #   [1]: attention masks\n","      #   [2]: labels \n","      b_input_ids = batch[0].to(device)\n","      b_input_mask = batch[1].to(device)\n","      b_labels = batch[2].to(device)\n","\n","      # Always clear any previously calculated gradients before performing a\n","      # backward pass. PyTorch doesn't do this automatically because \n","      # accumulating the gradients is \"convenient while training RNNs\". \n","      # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","      model.zero_grad()        \n","\n","      # In PyTorch, calling `model` will in turn call the model's `forward` \n","      # function and pass down the arguments. The `forward` function is \n","      # documented here: \n","      # https://huggingface.co/transformers/model_doc/bert.html#bertfortokenclassification\n","      # The results are returned in a results object, documented here:\n","      # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.TokenClassifierOutput\n","      \n","      if isinstance(model,BertCustom):\n","        logits,loss = model(b_input_ids,\n","                  attention_mask=b_input_mask,\n","                  labels=b_labels)\n","      else:\n","        result = model(b_input_ids, \n","                  token_type_ids=None, \n","                  attention_mask=b_input_mask\n","                  ,labels=b_labels)\n","        loss = result.loss\n","\n","      # Accumulate the training loss over all of the batches so that we can\n","      # calculate the average loss at the end. `loss` is a Tensor containing a\n","      # single value; the `.item()` function just returns the Python value \n","      # from the tensor.\n","      total_loss += loss.item()\n","\n","      # Perform a backward pass to calculate the gradients.\n","      loss.backward()\n","\n","      # Clip the norm of the gradients to 1.0.\n","      # This is to help prevent the \"exploding gradients\" problem.\n","      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","      # Update parameters and take a step using the computed gradient.\n","      # The optimizer dictates the \"update rule\"--how the parameters are\n","      # modified based on their gradients, the learning rate, etc.\n","      optimizer.step()\n","\n","      # Update the learning rate.\n","      scheduler.step()\n","      \n","\n","  # Calculate the average loss over the training data.\n","  avg_train_loss = total_loss / len(train_dataloader)            \n","  \n","  # Store the loss value for plotting the learning curve.\n","  loss_values.append(avg_train_loss)\n","\n","  print(\"\")\n","  print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","  print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n","        \n","\n","print(\"\")\n","print(\"Training complete!\")"],"id":"zKF_39bzyZkj","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Epoch 1 / 5 ========\n","Training...\n"]},{"output_type":"stream","name":"stderr","text":[" 11%|█         | 16/151 [00:16<02:21,  1.05s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-49-5566c9e08523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m                   \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                   \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_input_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                   ,labels=b_labels)\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 \u001b[0mactive_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m                 active_labels = torch.where(\n\u001b[0;32m-> 1751\u001b[0;31m                     \u001b[0mactive_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m                 )\n\u001b[1;32m   1753\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactive_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"n5R63y27XKMx"},"source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","\n","import seaborn as sns\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(loss_values, 'b-o')\n","\n","# Label the plot.\n","plt.title(\"Training loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","\n","plt.show()"],"id":"n5R63y27XKMx","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zVexzYMv3EQ_"},"source":["# import os\n","\n","# # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","\n","# output_dir = 'models/BERT_models_V5 (?)'\n","\n","# # Create output directory if needed\n","# if not os.path.exists(output_dir):\n","#     os.makedirs(output_dir)\n","\n","# print(\"Saving model to %s\" % output_dir)\n","\n","# # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# # They can then be reloaded using `from_pretrained()`\n","# model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","# if 'save_pretrained' in dir(model_to_save):\n","#   model_to_save.save_pretrained(output_dir)\n","# else:\n","#   torch.save(model_to_save.state_dict(), os.path.join(output_dir, 'model.pt'))\n","# tokenizer.save_pretrained(output_dir)"],"id":"zVexzYMv3EQ_","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CPdku7T33Vi_"},"source":["# Load a trained model and vocabulary that you have fine-tuned\n","# model = model_class.from_pretrained(output_dir)\n","# tokenizer = tokenizer_class.from_pretrained(output_dir)\n","\n","# # Copy the model to the GPU.\n","# model.to(device)"],"id":"CPdku7T33Vi_","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h7LwyydixzWT"},"source":["#### Model optimization"],"id":"h7LwyydixzWT"},{"cell_type":"code","metadata":{"id":"MRX14zBOx59B"},"source":["def train_model(model, train_dataloader, epochs=5):\n","  optimizer = AdamW(model.parameters(), lr = 5e-5, eps = 1e-8)\n","\n","  total_steps = len(train_dataloader) * epochs\n","\n","  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n","  seed_val = 42\n","\n","  random.seed(seed_val)\n","  np.random.seed(seed_val)\n","  torch.manual_seed(seed_val)\n","  torch.cuda.manual_seed_all(seed_val)\n","\n","  loss_values = []\n","\n","  for epoch_i in tqdm(list(range(0, epochs))):\n","    total_loss = 0\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        model.zero_grad()        \n","\n","        if isinstance(model,BertCustom):\n","          logits,loss = model(b_input_ids,\n","                    attention_mask=b_input_mask,\n","                    labels=b_labels)\n","        else:\n","          result = model(b_input_ids, \n","                    token_type_ids=None, \n","                    attention_mask=b_input_mask\n","                    ,labels=b_labels)\n","          loss = result.loss\n","\n","        total_loss += loss.item()\n","\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        optimizer.step()\n","\n","        scheduler.step()"],"id":"MRX14zBOx59B","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wGuvjXJN0D_1"},"source":["from sklearn.metrics import f1_score, accuracy_score\n","def evaluate_model(model, validation_dataloader, label_map):\n","\n","  model.eval()\n","\n","  predictions , true_labels = [], []\n","\n","  for batch in tqdm(validation_dataloader):\n","    batch = tuple(t.to(device) for t in batch)\n","    \n","    b_input_ids, b_input_mask, b_labels = batch\n","    \n","    with torch.no_grad():\n","        if isinstance(model,BertCustom):\n","          logits,loss = model(b_input_ids,\n","                    attention_mask=b_input_mask,\n","                    labels=b_labels)\n","        else:\n","          result = model(b_input_ids, \n","                    token_type_ids=None, \n","                    attention_mask=b_input_mask\n","                    ,labels=b_labels)\n","          loss = result.loss\n","          logits = result.logits\n","\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    \n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","\n","    all_predictions = np.concatenate(predictions, axis=0)\n","\n","\n","  all_true_labels = np.concatenate(true_labels, axis=0)\n","\n","  predicted_label_ids = np.argmax(all_predictions, axis=2)\n","\n","  predicted_label_ids = np.concatenate(predicted_label_ids, axis=0)\n","  all_true_labels = np.concatenate(all_true_labels, axis=0)\n","\n","  index_map = {value: key for key,value in label_map.items()}\n","\n","  real_token_predictions = []\n","  real_token_labels = []\n","\n","  for predicted_label, real_label in zip(predicted_label_ids,all_true_labels):\n","      if not index_map[real_label] in (null_tag,\"P\"):\n","          real_token_predictions.append(predicted_label)\n","          real_token_labels.append(real_label)\n","\n","  \n","  print(f\"f1 micro: {f1_score(real_token_labels, real_token_predictions, average='micro'):.3f}\")\n","  print(f\"Accuracy: {accuracy_score(real_token_labels, real_token_predictions):.3f}\")\n","  print(confusion_matrix(real_token_labels, real_token_predictions))\n","  return f1_score(real_token_labels, real_token_predictions, average='micro')"],"id":"wGuvjXJN0D_1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4TmuEAuq0-UU"},"source":["models = [\n","          get_BertForTokenClassification(), # default strategy, I don't know which it is\n","          get_Bert_custom(), # using only last layer, I get equivalent to default strategy\n","          get_Bert_custom(aggregation_stategy=get_mean_N_last_layer(3)), # averaging 3 last layers\n","          get_Bert_custom(aggregation_stategy=get_mean_N_last_layer(5)), # averaging 5 last layers\n","          get_Bert_custom(aggregation_stategy=get_Nth_layer(1)), # using only second last layer\n","          get_Bert_custom(aggregation_stategy=get_Nth_layer(2)) # using only third last layer\n","          ]\n","best_model, best_score = None, float('-inf')\n","for i,model in enumerate(models):\n","  model.cuda()\n","  print(f\"Model n°{i}/{len(models)}\")\n","  print(\"Training...\")\n","  train_model(model, train_dataloader, 3)\n","  print(\"  Validation ->\")\n","  score = evaluate_model(model, validation_dataloader, label_map)\n","  if score > best_score:\n","    best_model = model\n","  print()\n","\n","model = best_model"],"id":"4TmuEAuq0-UU","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K5ktDqWdWdhv"},"source":["#### Model test predictions"],"id":"K5ktDqWdWdhv"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vHNUDXEFWh4i","executionInfo":{"status":"ok","timestamp":1638727363241,"user_tz":300,"elapsed":40223,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"b6877b59-9eed-400e-a436-b4e7a323d6d9"},"source":["all_sentences_test, _ = get_doc_sentences(get_text_docs('test'), 'test', test_sentences, None)"],"id":"vHNUDXEFWh4i","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading docs...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:39<00:00,  2.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["   DONE.\n","\n","Constructing sentences...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:00<00:00, 384.41it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","   DONE.\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xhjK53HgNCQY","executionInfo":{"status":"ok","timestamp":1638727389044,"user_tz":300,"elapsed":1416,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"4cb1ccd3-b25a-424a-9919-7e62442fba94"},"source":["lengths = []\n","print('Measuring lengths')\n","for sen in tqdm(all_sentences_test):\n","    sen = ' '.join(sen)\n","    encoded_sent = tokenizer.encode(sen, add_special_tokens = True)\n","    \n","    lengths.append(len(encoded_sent))\n","print()\n","print('   Min length: {:,} tokens'.format(min(lengths)))\n","print('   Max length: {:,} tokens'.format(max(lengths)))\n","print('   Median length: {:,} tokens'.format(int(np.median(lengths))))"],"id":"xhjK53HgNCQY","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Measuring lengths\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 829/829 [00:00<00:00, 937.34it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","   Min length: 4 tokens\n","   Max length: 137 tokens\n","   Median length: 31 tokens\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aAeucARrQdwU","executionInfo":{"status":"ok","timestamp":1638727394092,"user_tz":300,"elapsed":1614,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"859f9f59-7c35-481c-fdf9-b7bd82bc2269"},"source":["test_sentences_X, test_attention_masks, _ = get_final_data(all_sentences_test, None)"],"id":"aAeucARrQdwU","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Applying BERT tokenizer to the sentences...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 829/829 [00:01<00:00, 789.66it/s]"]},{"output_type":"stream","name":"stdout","text":["   DONE.\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","metadata":{"id":"uW2nZeuMQWOa"},"source":["test_dataloader = get_data_loader(test_sentences_X, test_attention_masks, None, SequentialSampler)"],"id":"uW2nZeuMQWOa","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GOOluohnMgTx","executionInfo":{"status":"ok","timestamp":1638727460330,"user_tz":300,"elapsed":25854,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"6eb2c5a8-477f-4bdf-dd4d-efb79f69f528"},"source":["# Prediction on test set\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions_test , true_labels_test = [], []\n","\n","# Predict \n","for batch in tqdm(test_dataloader):\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","\n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      if isinstance(model,BertCustom):\n","        logits,loss = model(b_input_ids, b_input_mask, None)\n","      else:\n","        result = model(b_input_ids, \n","                      token_type_ids=None, \n","                      attention_mask=b_input_mask,\n","                      return_dict=True)\n","        logits = result.logits\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions_test.append(logits)\n","  true_labels_test.append(label_ids)\n","\n","print('    DONE.')"],"id":"GOOluohnMgTx","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 52/52 [00:25<00:00,  2.05it/s]"]},{"output_type":"stream","name":"stdout","text":["    DONE.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","metadata":{"id":"b6ux71QyymEU"},"source":["all_predictions_test = np.concatenate(predictions_test, axis=0)\n","predicted_label_ids_test = np.argmax(all_predictions_test, axis=2)\n","\n","predicted_label_ids_test = np.concatenate(predicted_label_ids_test, axis=0)"],"id":"b6ux71QyymEU","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"llyp6ihsy4gG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638728252660,"user_tz":300,"elapsed":265,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"1f1a6383-0e3a-4f1f-b520-8c5cfab46268"},"source":["index_map = {value: key for key,value in label_map.items()}\n","index_map"],"id":"llyp6ihsy4gG","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'N', 1: 'B', 2: 'L', 3: 'U', 4: 'O', 5: 'I'}"]},"metadata":{},"execution_count":89}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6feQ_2HHzjv_","executionInfo":{"status":"ok","timestamp":1638733381503,"user_tz":300,"elapsed":66700,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"f2018f95-f57b-41dd-bd72-852f284a3bdc"},"source":["test_df_soumission = test_df.drop(['DocID', 'Token'], 1)\n","test_df_soumission = test_df_soumission.astype({'Tag': 'string'})\n","\n","clean_tokens = []\n","clean_predictions = []\n","for pred, token in zip(predicted_label_ids_test, np.concatenate(test_sentences_X)):\n","  if token not in (tokenizer.pad_token_id, tokenizer.cls_token_id, tokenizer.sep_token_id):\n","    clean_tokens.append(token)\n","    if index_map[pred] == null_tag:\n","      clean_predictions.append(\"O\")\n","    else:\n","      clean_predictions.append(index_map[pred])\n","\n","i = 0\n","for word, word_id in tqdm(list(zip(np.concatenate(all_sentences_test), np.concatenate(ids)))):\n","  k = test_df_soumission[test_df_soumission[\"TokenID\"] == word_id].index[0]\n","  test_df_soumission.at[k, \"Tag\"] = clean_predictions[i]\n","  i += len(tokenizer.tokenize(word))"],"id":"6feQ_2HHzjv_","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[""]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"9pKtE_i_1aJl","executionInfo":{"status":"ok","timestamp":1638733382503,"user_tz":300,"elapsed":34,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"49698a6f-8927-4445-df74-a60875db7b5c"},"source":["test_df_soumission"],"id":"9pKtE_i_1aJl","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TokenID</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>S0885230816301759-0</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>S0885230816301759-1</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>S0885230816301759-2</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>S0885230816301759-3</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>S0885230816301759-4</td>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>21706</th>\n","      <td>S1877750313001269-211</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>21707</th>\n","      <td>S1877750313001269-212</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>21708</th>\n","      <td>S1877750313001269-213</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>21709</th>\n","      <td>S1877750313001269-214</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>21710</th>\n","      <td>S1877750313001269-215</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>21711 rows × 2 columns</p>\n","</div>"],"text/plain":["                     TokenID Tag\n","0        S0885230816301759-0   O\n","1        S0885230816301759-1   O\n","2        S0885230816301759-2   O\n","3        S0885230816301759-3   O\n","4        S0885230816301759-4   B\n","...                      ...  ..\n","21706  S1877750313001269-211   O\n","21707  S1877750313001269-212   O\n","21708  S1877750313001269-213   O\n","21709  S1877750313001269-214   O\n","21710  S1877750313001269-215   O\n","\n","[21711 rows x 2 columns]"]},"metadata":{},"execution_count":162}]},{"cell_type":"code","metadata":{"id":"dwbrl98I1-qK"},"source":["test_df_soumission.to_csv('submission_test_A_bert.csv', index=False, encoding='utf-8')"],"id":"dwbrl98I1-qK","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Lxmt0Izypnl"},"source":["from sklearn.metrics import confusion_matrix, plot_confusion_matrix, recall_score, precision_score, f1_score, accuracy_score"],"id":"8Lxmt0Izypnl","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_tchrQVycoHL"},"source":["#### BERT Evaluation"],"id":"_tchrQVycoHL"},{"cell_type":"code","metadata":{"id":"YfHyJe--tfON","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638743724848,"user_tz":300,"elapsed":10130,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"d19bb324-3935-4bad-9ee9-ee4afb91a915"},"source":["# Prediction on test set\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in tqdm(validation_dataloader):\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      if isinstance(model,BertCustom):\n","        logits,loss = model(b_input_ids,\n","                  attention_mask=b_input_mask,\n","                  labels=b_labels)\n","      else:\n","        result = model(b_input_ids, \n","                  token_type_ids=None, \n","                  attention_mask=b_input_mask\n","                  ,labels=b_labels)\n","        loss = result.loss\n","        logits = result.logits\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","print('    DONE.')"],"id":"YfHyJe--tfON","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 26/26 [00:10<00:00,  2.58it/s]"]},{"output_type":"stream","name":"stdout","text":["    DONE.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vr_T2EEGuKzX","executionInfo":{"status":"ok","timestamp":1638743529549,"user_tz":300,"elapsed":106,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"2321e54f-41a6-441c-c669-da01cedb0c43"},"source":["predicted_label_ids.shape"],"id":"Vr_T2EEGuKzX","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(82600,)"]},"metadata":{},"execution_count":83}]},{"cell_type":"code","metadata":{"id":"jwqmP307wLQe"},"source":["all_predictions = np.concatenate(predictions, axis=0)\n","all_true_labels = np.concatenate(true_labels, axis=0)\n","\n","predicted_label_ids = np.argmax(all_predictions, axis=2)\n","\n","predicted_label_ids = np.concatenate(predicted_label_ids, axis=0)\n","all_true_labels = np.concatenate(all_true_labels, axis=0)"],"id":"jwqmP307wLQe","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S-V9UTPqgJzQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638743727652,"user_tz":300,"elapsed":14,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"2b61ad29-974d-4490-91ec-cdf152bb975f"},"source":["index_map = {value: key for key,value in label_map.items()}\n","index_map"],"id":"S-V9UTPqgJzQ","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'O', 1: 'I', 2: '-N-', 3: 'U', 4: 'B', 5: 'L'}"]},"metadata":{},"execution_count":90}]},{"cell_type":"code","metadata":{"id":"6ZGqjNH4f80i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638743729017,"user_tz":300,"elapsed":107,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"70772646-8707-47c0-ae12-38fad128bb3d"},"source":["# Construct new lists of predictions which don't include any null tokens.\n","real_token_predictions = []\n","real_token_labels = []\n","\n","# For each of the input tokens in the dataset...\n","for predicted_label, real_label in zip(predicted_label_ids,all_true_labels):\n","\n","    # If it's not a token with a null label...\n","    if not index_map_B[real_label] == null_tag:\n","        \n","        # Add the prediction and the ground truth to their lists.\n","        if index_map_B[predicted_label] == null_tag:\n","          real_token_predictions.append(label_map[\"O\"])\n","        else:\n","          real_token_predictions.append(predicted_label)\n","        real_token_labels.append(real_label)\n","\n","print(\"Before filtering out `null` tokens, length = {:,}\".format(len(all_true_labels)))\n","print(\" After filtering out `null` tokens, length = {:,}\".format(len(real_token_labels)))"],"id":"6ZGqjNH4f80i","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Before filtering out `null` tokens, length = 82,600\n"," After filtering out `null` tokens, length = 11,161\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jRo_gjKKVcHW","executionInfo":{"status":"ok","timestamp":1638727280933,"user_tz":300,"elapsed":287,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"e6b9f6c6-8dd2-4d95-d4f6-f5687d1b23b3"},"source":["set(all_true_labels)"],"id":"jRo_gjKKVcHW","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0, 1, 2, 3, 4, 5}"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","metadata":{"id":"5_AJhm4MzwpO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638727282239,"user_tz":300,"elapsed":9,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"6b3ba62d-5bac-4682-b242-57db6311103a"},"source":["label_map"],"id":"5_AJhm4MzwpO","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'B': 1, 'I': 5, 'L': 2, 'N': 0, 'O': 4, 'U': 3}"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","metadata":{"id":"UQhEk2FsCGj6"},"source":["target_names = list(index_map.values())\n","target_names.remove(null_tag)"],"id":"UQhEk2FsCGj6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NGmOZLS54uDO","executionInfo":{"status":"ok","timestamp":1638763491675,"user_tz":300,"elapsed":286,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16673165226177949939"}},"outputId":"44dd277b-cb81-426a-eb30-29b1a5b22a79"},"source":["print(classification_report(real_token_labels, real_token_predictions, target_names=target_names))"],"id":"NGmOZLS54uDO","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           B       0.59      0.72      0.65       694\n","           U       0.73      0.58      0.65       361\n","           I       0.56      0.59      0.57      1072\n","           L       0.68      0.82      0.74       694\n","           O       0.93      0.90      0.91      8340\n","\n","    accuracy                           0.84     11161\n","   macro avg       0.70      0.72      0.70     11161\n","weighted avg       0.85      0.84      0.84     11161\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"3PXTzQNlcvIy"},"source":["#### Bi-LSTM Evaluation"],"id":"3PXTzQNlcvIy"},{"cell_type":"code","metadata":{"id":"rrJZzgNyEuGK"},"source":["all_weights"],"id":"rrJZzgNyEuGK","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lwsF16BXOlVM"},"source":["# model = models[-2]\n","def show_confusion(model):\n","  results = model.predict(val_sentences_X)\n","  pred = np.argmax(results, axis=2).flatten()\n","  true = np.argmax(val_tags_y, axis=2).flatten()\n","  print(f\"f1 micro:{f1_score(true, pred, average='micro'):.3f}\")\n","  print(f\"f1 micro:{f1_score(true, pred, average='macro'):.3f}\")\n","  print(f\"recall macro:{recall_score(true, pred, average='macro'):.3f}\")\n","  print(\"Confusion Matrix:\")\n","  print(confusion_matrix(true, pred))\n","print(label_map)\n","show_confusion(model)\n","# print(tag2weights_list[0])\n","# show_confusion(models[0])\n","# print()\n","# print(tag2weights_list[1])\n","# show_confusion(models[2])\n","# print()\n","# print(tag2weights_list[2])\n","# show_confusion(models[4])"],"id":"lwsF16BXOlVM","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"568dadfa-ce2a-4432-b5eb-d915e6c2aecf"},"source":["# 2. Sous-tâche B : Identification des mots-clés et de leurs types (85%)"],"id":"568dadfa-ce2a-4432-b5eb-d915e6c2aecf"},{"cell_type":"markdown","metadata":{"id":"Ee9_zHgqF8a4"},"source":["## a. Prétraitement des données\n","Premières étapes de prétraitement communes à tous les modèles. Nous récupérons les informations des fichiers .ann pour associé les tags aux jetons"],"id":"Ee9_zHgqF8a4"},{"cell_type":"code","metadata":{"id":"ASNRycSLNU5N"},"source":["show_id = 18\n","def set_tag_B(df, sub_data):\n","  df[\"Tag\"] = df[\"Tag\"].astype(str)\n","  df[\"Token\"] = df[\"Token\"].astype(str)\n","  doc_id = None\n","  for i in tqdm(range(len(df))):\n","    if df[\"DocID\"][i] != doc_id:\n","      i0 = i\n","      doc_id = df[\"DocID\"][i]\n","      ANN = pd.read_csv(f\"data/{sub_data}/{doc_id}.ann\", delimiter=\"\\t\", names=[\"Type\", \"Annotation\", \"Tokens\"])\n","      ANN = ANN.drop(ANN[ANN['Type'].map(lambda x: x[0]) != \"T\"].index)\n","      ANN[\"Tokens begining\"] = ANN[\"Annotation\"].apply(lambda x: int(x.split()[1]))\n","      ANN[\"Type_Letter\"]=ANN[\"Annotation\"].apply(lambda x: x.split()[0][0])\n","\n","      ANN = ANN.to_numpy()\n","      ANN = ANN[np.argsort(ANN[:, 3])]\n","\n","      tokens_id = 0\n","      match_id = 0\n","      entity_ids = []\n","    if tokens_id >= len(ANN): \n","      df.at[i, 'Tag'] = \"O\"\n","      continue\n","    tokens = nltk.word_tokenize(ANN[tokens_id][2])\n","    token = df['Token'][i]\n","    if tokens[match_id] in token:\n","      if match_id == 0 and len(tokens) == 1:\n","        df.at[i, 'Tag'] = \"U\" + \"_\" + ANN[tokens_id,-1]\n","        previous_tokens_id = tokens_id\n","        tokens_id += 1\n","        while tokens_id < len(ANN) and int(re.split(\" |;\", ANN[tokens_id][1])[1]) < int(re.split(\" |;\", ANN[previous_tokens_id][1])[2]):\n","          tokens_id += 1\n","      elif match_id == 0:\n","        entity_ids = [i]\n","        match_id += 1\n","      elif len(tokens) == match_id + 1:\n","        df.at[entity_ids[0], 'Tag'] = \"B\" +  \"_\" + ANN[tokens_id,-1]\n","        for k in entity_ids[1:]:\n","          df.at[k, 'Tag'] = \"I\" +  \"_\" + ANN[tokens_id,-1]\n","        df.at[i, 'Tag'] = \"L\" +  \"_\" + ANN[tokens_id,-1]\n","        match_id = 0\n","        previous_tokens_id = tokens_id\n","        tokens_id += 1\n","        while tokens_id < len(ANN) and int(re.split(\" |;\", ANN[tokens_id][1])[1]) < int(re.split(\" |;\", ANN[previous_tokens_id][1])[2]):\n","          tokens_id += 1\n","        entity_ids = []\n","      else:\n","        entity_ids.append(i)\n","        match_id += 1\n","    else:\n","      df.at[i, 'Tag'] = \"O\"\n","      for j in entity_ids:\n","        df.at[j, 'Tag'] = \"O\"\n","      entity_ids = []\n","      match_id = 0\n","  return df"],"id":"ASNRycSLNU5N","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0IDJD-F6Ggco"},"source":["if not \"train_B_bilou.csv\" in os.listdir():\n","  train_B_df = set_tag_B(train_df, \"train\")\n","  train_B_df.to_csv(\"train_B_bilou.csv\", index = False, header = True)\n","else:\n","  train_B_df = pd.read_csv(\"train_B_bilou.csv\")\n","\n","if not \"val_B_bilou.csv\" in os.listdir():\n","  val_B_df = set_tag_B(val_df, \"val\")\n","  val_B_df.to_csv(\"val_B_bilou.csv\", index = False, header = True)\n","else:\n","  val_B_df = pd.read_csv(\"val_B_bilou.csv\")"],"id":"0IDJD-F6Ggco","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EZGMz3iqpu6S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638764472129,"user_tz":300,"elapsed":33,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"222d0b73-8f12-4e60-c608-0811be7d5dda"},"source":["train_B_df"],"id":"EZGMz3iqpu6S","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>DocID</th>\n","      <th>TokenID</th>\n","      <th>Token</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>S0022311514001640</td>\n","      <td>S0022311514001640-0</td>\n","      <td>The</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>S0022311514001640</td>\n","      <td>S0022311514001640-1</td>\n","      <td>vapour</td>\n","      <td>B_M</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>S0022311514001640</td>\n","      <td>S0022311514001640-2</td>\n","      <td>phase</td>\n","      <td>L_M</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>S0022311514001640</td>\n","      <td>S0022311514001640-3</td>\n","      <td>consists</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>S0022311514001640</td>\n","      <td>S0022311514001640-4</td>\n","      <td>of</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>64177</th>\n","      <td>S0370269304009979</td>\n","      <td>S0370269304009979-187</td>\n","      <td>various</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>64178</th>\n","      <td>S0370269304009979</td>\n","      <td>S0370269304009979-188</td>\n","      <td>brane</td>\n","      <td>B_P</td>\n","    </tr>\n","    <tr>\n","      <th>64179</th>\n","      <td>S0370269304009979</td>\n","      <td>S0370269304009979-189</td>\n","      <td>world</td>\n","      <td>I_P</td>\n","    </tr>\n","    <tr>\n","      <th>64180</th>\n","      <td>S0370269304009979</td>\n","      <td>S0370269304009979-190</td>\n","      <td>models</td>\n","      <td>L_P</td>\n","    </tr>\n","    <tr>\n","      <th>64181</th>\n","      <td>S0370269304009979</td>\n","      <td>S0370269304009979-191</td>\n","      <td>.</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>64182 rows × 4 columns</p>\n","</div>"],"text/plain":["                   DocID                TokenID     Token  Tag\n","0      S0022311514001640    S0022311514001640-0       The    O\n","1      S0022311514001640    S0022311514001640-1    vapour  B_M\n","2      S0022311514001640    S0022311514001640-2     phase  L_M\n","3      S0022311514001640    S0022311514001640-3  consists    O\n","4      S0022311514001640    S0022311514001640-4        of    O\n","...                  ...                    ...       ...  ...\n","64177  S0370269304009979  S0370269304009979-187   various    O\n","64178  S0370269304009979  S0370269304009979-188     brane  B_P\n","64179  S0370269304009979  S0370269304009979-189     world  I_P\n","64180  S0370269304009979  S0370269304009979-190    models  L_P\n","64181  S0370269304009979  S0370269304009979-191         .    O\n","\n","[64182 rows x 4 columns]"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"079706f0-35ea-4d30-bd75-c2e9fa086031","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638764473744,"user_tz":300,"elapsed":1644,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"12618058-a055-4ce5-b6c4-d9c067f6ab26"},"source":["train_sentences_B, train_tags_B, train_ids_B = vectorize_tagged_sentence(train_B_df, 'train')\n","val_sentences_B, val_tags_B, val_ids_B = vectorize_tagged_sentence(val_B_df, 'val')\n","test_sentences_B, test_ids_B = vectorize_test_sentence(test_df)"],"id":"079706f0-35ea-4d30-bd75-c2e9fa086031","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 350/350 [00:01<00:00, 244.21it/s]\n","100%|██████████| 50/50 [00:00<00:00, 752.71it/s]\n","100%|██████████| 100/100 [00:00<00:00, 549.17it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"y-HAJGGJHRJ4"},"source":["## b.&c. Models et Evaluations"],"id":"y-HAJGGJHRJ4"},{"cell_type":"markdown","metadata":{"id":"IVnRYaXoMKUZ"},"source":["### CRF"],"id":"IVnRYaXoMKUZ"},{"cell_type":"markdown","metadata":{"id":"E5eJGLBg7Faw"},"source":["On utilise le même modèle que pour la tache, L-BFGS avec Elastic Net (L1 + L2) regularization comme proposé par le tutoriel en ligne."],"id":"E5eJGLBg7Faw"},{"cell_type":"markdown","metadata":{"id":"UcXgLC_3HgpF"},"source":["#### Pre-traitement des données"],"id":"UcXgLC_3HgpF"},{"cell_type":"code","metadata":{"id":"-KgrATO0HgpF"},"source":["# we define the input of our CRF by addind the POS tag for each sentence, \n","# we need them for the features of our CRF\n","X_crf_train_B= nltk.pos_tag_sents(train_sentences_B)\n","X_crf_val_B = nltk.pos_tag_sents(val_sentences_B)\n","X_crf_test_B = nltk.pos_tag_sents(test_sentences_B)"],"id":"-KgrATO0HgpF","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rODQxRt6HgpF"},"source":["#### Défintion du model"],"id":"rODQxRt6HgpF"},{"cell_type":"code","metadata":{"id":"8i6qSSFuHgpF"},"source":["CRF_train_sentences_B = transform_to_dataset(X_crf_train_B)"],"id":"8i6qSSFuHgpF","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cDuRewFqHgpG"},"source":["#### training"],"id":"cDuRewFqHgpG"},{"cell_type":"code","metadata":{"id":"884Q1pWGHgpG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638764490163,"user_tz":300,"elapsed":12035,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"318793f2-3c74-4ed8-be39-96739c8de9a5"},"source":[" \n","model_CRF_tacheB_ = CRF(\n","    algorithm='lbfgs',\n","    c1=0.1,\n","    c2=0.1,\n","    max_iterations=100,\n","    all_possible_transitions=True\n","    )\n","\n","\n","model_CRF_tacheB_.fit(CRF_train_sentences_B, train_tags_B)"],"id":"884Q1pWGHgpG","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/base.py:213: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n","  FutureWarning)\n"]},{"output_type":"execute_result","data":{"text/plain":["CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n","    keep_tempfiles=None, max_iterations=100)"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","metadata":{"id":"ULzjkR-heNln","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638764490165,"user_tz":300,"elapsed":132,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"2a68a738-9fb5-4424-caa2-955e09314ad1"},"source":["model_CRF_tacheB_.classes_"],"id":"ULzjkR-heNln","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['O',\n"," 'B_P',\n"," 'L_P',\n"," 'U_T',\n"," 'I_P',\n"," 'B_M',\n"," 'I_M',\n"," 'L_M',\n"," 'U_M',\n"," 'U_P',\n"," 'B_T',\n"," 'L_T',\n"," 'I_T']"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"JdfAH3OkM2d3"},"source":["model_CRF_tacheB =model_CRF_tacheB_"],"id":"JdfAH3OkM2d3","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CFI2q4pYMpli"},"source":["#### Gridsearch sur C1 et C2 qui sont les \"regularizers\" L1 et L2"],"id":"CFI2q4pYMpli"},{"cell_type":"code","metadata":{"id":"ihNdkLviHgpG","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1638764490170,"user_tz":300,"elapsed":131,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"2891d6ce-f4bf-4255-ec1d-5b848a788d01"},"source":["'''\n","model_CRF_tacheB = CRF(\n","    algorithm='lbfgs',\n","    max_iterations=100,\n","    all_possible_transitions=True\n","    )\n","\n","params_space = {\n","    'c1': scipy.stats.expon(scale=0.5),\n","    'c2': scipy.stats.expon(scale=0.05),\n","}\n","\n","# metric of evaluation\n","labels = list(model_CRF_tacheB_.classes_)\n","labels.remove('O')\n","f1_scorer = make_scorer(metrics.flat_f1_score,\n","                        average='weighted', labels=labels)\n","\n","# search\n","rs_B = RandomizedSearchCV(model_CRF_tacheB, params_space,\n","                        cv=3,\n","                        verbose=1,\n","                        n_jobs=-1,\n","                        n_iter=50,\n","                        scoring=f1_scorer)\n","rs_B.fit(CRF_train_sentences_B, train_tags_B)\n","'''"],"id":"ihNdkLviHgpG","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nmodel_CRF_tacheB = CRF(\\n    algorithm='lbfgs',\\n    max_iterations=100,\\n    all_possible_transitions=True\\n    )\\n\\nparams_space = {\\n    'c1': scipy.stats.expon(scale=0.5),\\n    'c2': scipy.stats.expon(scale=0.05),\\n}\\n\\n# metric of evaluation\\nlabels = list(model_CRF_tacheB_.classes_)\\nlabels.remove('O')\\nf1_scorer = make_scorer(metrics.flat_f1_score,\\n                        average='weighted', labels=labels)\\n\\n# search\\nrs_B = RandomizedSearchCV(model_CRF_tacheB, params_space,\\n                        cv=3,\\n                        verbose=1,\\n                        n_jobs=-1,\\n                        n_iter=50,\\n                        scoring=f1_scorer)\\nrs_B.fit(CRF_train_sentences_B, train_tags_B)\\n\""]},"metadata":{},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"NxI_IYvBHgpG","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1638764490171,"user_tz":300,"elapsed":129,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"6ad7034e-471a-493e-d974-c44f70d0e0c8"},"source":["'''\n","print('best params:', rs_B.best_params_)\n","print('best CV score:', rs_B.best_score_)\n","print('model size: {:0.2f}M'.format(rs_B.best_estimator_.size_ / 1000000))\n","'''"],"id":"NxI_IYvBHgpG","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nprint('best params:', rs_B.best_params_)\\nprint('best CV score:', rs_B.best_score_)\\nprint('model size: {:0.2f}M'.format(rs_B.best_estimator_.size_ / 1000000))\\n\""]},"metadata":{},"execution_count":59}]},{"cell_type":"code","metadata":{"id":"b3NyydXBJheL"},"source":["#model_CRF_tacheB = rs_B.best_estimator_"],"id":"b3NyydXBJheL","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qbi_D5ixHgpG"},"source":["#### prediction"],"id":"qbi_D5ixHgpG"},{"cell_type":"code","metadata":{"id":"Dg37GzuOHgpG"},"source":["CRF_train_sentences_B, y_train_pred_B = tag_prediction(X_crf_train_B,model_CRF_tacheB)\n","CRF_val_sentences_B, y_val_pred_B = tag_prediction(X_crf_val_B,model_CRF_tacheB)\n","CRF_test_sentences_B, y_test_pred_B = tag_prediction(X_crf_test_B,model_CRF_tacheB)"],"id":"Dg37GzuOHgpG","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WB81b5L5HgpH"},"source":["#### Evaluation sur le validation set"],"id":"WB81b5L5HgpH"},{"cell_type":"markdown","metadata":{"id":"Hw9GGzg1HgpH"},"source":["val set"],"id":"Hw9GGzg1HgpH"},{"cell_type":"code","metadata":{"id":"_e3ppvxrHgpJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638765904905,"user_tz":300,"elapsed":134,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"ad53c3ff-aefe-43ee-8a9f-f84b04500d67"},"source":["print(metrics.flat_accuracy_score(val_tags_B, y_val_pred_B))"],"id":"_e3ppvxrHgpJ","execution_count":95,"outputs":[{"output_type":"stream","name":"stdout","text":["0.7441089508108593\n"]}]},{"cell_type":"code","metadata":{"id":"tjodW7ELHgpJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638764796265,"user_tz":300,"elapsed":115,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"4fe8557f-6b13-468c-e646-a7fdf8befc59"},"source":["CRF_show_confusion(list(np.concatenate(val_tags_B)), list(np.concatenate(y_val_pred_B)))"],"id":"tjodW7ELHgpJ","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["f1 micro:0.744\n","f1 micro:0.332\n","recall macro:0.307\n","Confusion Matrix:\n","[[  76   17    7   12   16   13    0    1    0   88    6    1    0]\n"," [  12  107   18    3   24   14    0    0    0  150    1    2    0]\n"," [   4   22   22    0    3    5    0    0    0   70    0    0    0]\n"," [  12    3    0   48   29   12    3    3    0   62    1    1    0]\n"," [   9   18    0   18  101   30    3   15    2  245    7    1    0]\n"," [   7   17    7    8   54   93    1    4    5  250    3    0    0]\n"," [   0    0    0    6    4    5   91   29   15   84    3    0    0]\n"," [   0    0    0    2   13    8   19  121   22  141    4    1    0]\n"," [   0    0    0    0    4    3   10   34   26   47    2    0    0]\n"," [  45   87   55   54  207  193   36   71   40 7533    4   15    0]\n"," [  10   12    4    2    8   11   12    7    3  132   51   12    0]\n"," [   1    4    1    1    3    4    0    1    1   33    3   36    0]\n"," [   0    0    0    0    0    0    1    0    0    8    0    0    0]]\n"]}]},{"cell_type":"code","metadata":{"id":"Y_9fa2stHgpJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638764809081,"user_tz":300,"elapsed":155,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"6ab3a1e1-4c19-4463-d414-7cbf9b0420cd"},"source":["\n","labels = model_CRF_tacheB.classes_\n","print(metrics.flat_classification_report(\n","    y_val_pred_B, val_tags_B, labels=labels, digits=3\n","))"],"id":"Y_9fa2stHgpJ","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           O      0.903     0.852     0.877      8843\n","         B_P      0.323     0.373     0.346       287\n","         L_P      0.366     0.423     0.392       286\n","         U_T      0.000     0.000     0.000         0\n","         I_P      0.225     0.217     0.221       466\n","         B_M      0.321     0.432     0.368       176\n","         I_M      0.276     0.312     0.293       154\n","         L_M      0.384     0.517     0.441       176\n","         U_M      0.193     0.600     0.292        85\n","         U_P      0.409     0.522     0.459        69\n","         B_T      0.175     0.193     0.183       114\n","         L_T      0.206     0.228     0.217       114\n","         I_T      0.207     0.238     0.221       391\n","\n","    accuracy                          0.744     11161\n","   macro avg      0.307     0.377     0.332     11161\n","weighted avg      0.773     0.744     0.757     11161\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:70: FutureWarning: Pass labels=['O', 'B_P', 'L_P', 'U_T', 'I_P', 'B_M', 'I_M', 'L_M', 'U_M', 'U_P', 'B_T', 'L_T', 'I_T'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n","  FutureWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"markdown","metadata":{"id":"SzOLg1RB_tJ-"},"source":["***REMARQUE***\n","\n","ENcore une fois le label 'O' est le mieux prédit. Par rapport à la tache A c'est encore plus flagrant. En effet, notre modèle CRF ne prend pas en compte le déséquilibre du jeu de données."],"id":"SzOLg1RB_tJ-"},{"cell_type":"markdown","metadata":{"id":"0fmkCf9lHgpK"},"source":["#### Fichiers de soumissions for CRF"],"id":"0fmkCf9lHgpK"},{"cell_type":"code","metadata":{"id":"dqmAObjJHgpK","executionInfo":{"status":"ok","timestamp":1638765943047,"user_tz":300,"elapsed":164,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}}},"source":["\n","y_val_csv=list(np.concatenate(y_val_pred))\n","\n","CRF_val_df_soumission=val_df.drop(['DocID', 'Token'], 1)\n","CRF_val_df_soumission['Tag']=y_val_csv\n","CRF_val_df_soumission\n","\n","CRF_val_df_soumission.to_csv('CRF_submission_val_B.csv', index=False, encoding='utf-8')\n"],"id":"dqmAObjJHgpK","execution_count":100,"outputs":[]},{"cell_type":"code","metadata":{"id":"1gemHso6HgpK","executionInfo":{"status":"ok","timestamp":1638765945347,"user_tz":300,"elapsed":195,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}}},"source":["y_test_csv=list(np.concatenate(y_test_pred))\n","\n","CRF_test_df_soumission=test_df.drop(['DocID', 'Token'], 1)\n","CRF_test_df_soumission['Tag']=y_test_csv\n","CRF_test_df_soumission\n","\n","CRF_test_df_soumission.to_csv('CRF_submission_test_B.csv', index=False, encoding='utf-8')"],"id":"1gemHso6HgpK","execution_count":101,"outputs":[]},{"cell_type":"code","metadata":{"id":"NLFNumNdHgpK","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1638765927793,"user_tz":300,"elapsed":179,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"c6a12dda-7baf-4527-ef66-32f6254c717f"},"source":["CRF_test_df_soumission.iloc[:5]"],"id":"NLFNumNdHgpK","execution_count":99,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TokenID</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>S0885230816301759-0</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>S0885230816301759-1</td>\n","      <td>B</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>S0885230816301759-2</td>\n","      <td>I</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>S0885230816301759-3</td>\n","      <td>I</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>S0885230816301759-4</td>\n","      <td>I</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               TokenID Tag\n","0  S0885230816301759-0   O\n","1  S0885230816301759-1   B\n","2  S0885230816301759-2   I\n","3  S0885230816301759-3   I\n","4  S0885230816301759-4   I"]},"metadata":{},"execution_count":99}]},{"cell_type":"markdown","metadata":{"id":"z6XVi4p5n0TO"},"source":["### Bilstm-CRF avec glove embedding"],"id":"z6XVi4p5n0TO"},{"cell_type":"code","metadata":{"id":"6sJ5SNewNuwR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638745761419,"user_tz":300,"elapsed":247,"user":{"displayName":"lamia2 salhi2","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14440555040870778846"}},"outputId":"d7af1362-ec0b-489b-8612-21d19c0d1f56"},"source":["# few parameters\n","MAX_LEN = MAX_LENGTH_B  # Max length of words\n","WORD_EMBEDDING_OUT_DIM = 300\n","\n","tags_B= ['-PAD-', 'O', 'U_M', 'I_M', 'B_M', 'L_M', 'U_P', 'I_P', 'B_P', 'L_P', 'U_T', 'I_T', 'B_T', 'L_T']\n","n_tags = len(tags_B)\n","words_voc=list(set(train_B_df[\"Token\"].values))\n","n_words = len(words_voc) +2\n","print(len(words_voc[1]))\n","max_word_len = len(max(words_voc[1:], key=len))"],"id":"6sJ5SNewNuwR","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["7\n"]}]},{"cell_type":"markdown","metadata":{"id":"oBU1j2vBbFMw"},"source":["#### Importation de Glove"],"id":"oBU1j2vBbFMw"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BMTjiFA2n_ff","executionInfo":{"status":"ok","timestamp":1638745768197,"user_tz":300,"elapsed":471,"user":{"displayName":"lamia2 salhi2","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14440555040870778846"}},"outputId":"f4fc1239-0c99-45e9-8f20-24d3f956c5fc"},"source":["words_not_found = []\n","\n","embedding_matrix = np.zeros((n_words, WORD_EMBEDDING_OUT_DIM))\n","\n","for word, i in word2indexB.items():\n","    if i >= n_words:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if (embedding_vector is not None) and len(embedding_vector) > 0:\n","        # words not found in embedding index will be all-zeros.\n","        embedding_matrix[i] = embedding_vector\n","    else:\n","        words_not_found.append(word)\n","print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"],"id":"BMTjiFA2n_ff","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of null word embeddings: 1302\n"]}]},{"cell_type":"markdown","metadata":{"id":"Z5sJPq9S3Spk"},"source":["#### Définition du modéle Bilstm+Crf"],"id":"Z5sJPq9S3Spk"},{"cell_type":"markdown","metadata":{"id":"NmejF0212ONq"},"source":["On utilise la librairie tf2crf de pypi, qui est est un module permettant d'implémenter un layer crf sur tensorflow 2. Ici on utilise la fonction ModelWithCRFLossDSCLoss qui permet d'uitiliser la fonction de cout DSC (Dice similarity coefficient) qui est trés pratique dans le cadre des données qui sont déséquilibrés. On utilise également de l'early stopping et un checkpoint conservant le meilleur modèle en terme d'accuracy sur le validation set. Finalement, un embedding glove est utilisé pour augementer nos performances. On compare également le GRU et le LSTM comme layers."],"id":"NmejF0212ONq"},{"cell_type":"code","metadata":{"id":"v824DQ3C2VBN"},"source":["\n","def create_model_Bilstm_crf_B( optimizer,embedding_name,layer,dropout, unit):\n","  inputs = Input(shape=(MAX_LEN, ))\n","  if embedding_name == 'Glove':\n","    x = Embedding(input_dim=n_words, output_dim=WORD_EMBEDDING_OUT_DIM, input_length=MAX_LEN,weights=[embedding_matrix])(inputs)\n","  else:\n","    x = Embedding(input_dim=n_words, output_dim=WORD_EMBEDDING_OUT_DIM, input_length=MAX_LEN)(inputs)\n","\n","  if layer == 'GRU':\n","    x = Bidirectional(GRU(units=unit, return_sequences=True, dropout=dropout))(x)\n","  else:\n","    x = Bidirectional(LSTM(units=unit, return_sequences=True, dropout=dropout))(x)\n","  crf = CRF(units=n_tags)\n","  output = crf(x)\n","  base_model = Model(inputs, output)\n","  model_crf = ModelWithCRFLossDSCLoss(base_model,sparse_target=False)\n","  model_crf.compile(optimizer=optimizer)\n","  return model_crf"],"id":"v824DQ3C2VBN","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PgWWT3QlI8qv"},"source":["#### Benchmark/Grid search"],"id":"PgWWT3QlI8qv"},{"cell_type":"code","metadata":{"id":"qCUMi5yqAww5"},"source":["optimizers = ['adam'] #['adam','rmsprop','SGD']#['rmsprop','adam','SGD','adadelta']#['Adam()', 'SGD()', 'RMSprop()','Adagrad()','Adadelta()']\n","all_dropouts = [0.3]\n","units = [10,64] #[10, 64, 150, 250]\n","embeddin_meth = ['Glove', None]#['Glove', None] \n","models_bilstm_crf_param = [[optimizer,embedding_name,layer, dropout, unit] for unit in units for layer in ['GRU', 'LSTM'] for dropout in all_dropouts for optimizer in optimizers for embedding_name in  embeddin_meth ]\n","models_bilstm_crf= [create_model_Bilstm_crf_B( optimizer,embedding_name,layer, dropout, unit) for unit in units for layer in ['GRU', 'LSTM'] for dropout in all_dropouts for optimizer in optimizers for embedding_name in  embeddin_meth ]"],"id":"qCUMi5yqAww5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JV4pBoHrpjVl","executionInfo":{"status":"ok","timestamp":1638732736077,"user_tz":300,"elapsed":4837720,"user":{"displayName":"lamia2 salhi2","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14440555040870778846"}},"outputId":"a8252ae5-39ee-4b25-d148-a2cd9a6f5464"},"source":["callback2 = EarlyStopping(monitor='val_loss_val', patience=3)\n","histories_B_Bilstm_crf = []\n","for i,model in tqdm(enumerate(models_bilstm_crf)):\n","  checkpoint2 = ModelCheckpoint(\"./models/Bilstm_crf/best_model_bilstm\"+str(i)+\".hdf5\", monitor='val_val_accuracy', verbose=1,save_best_only=True, mode='auto', save_weights_only=True)\n","  histories_B_Bilstm_crf.append(model.fit(train_sentences_X_B, train_tags_y_B, batch_size=32, epochs=50, validation_data=(val_sentences_X_B,val_tags_y_B),callbacks=[checkpoint2,callback2],verbose=1))\n","#model_tuning = create_model_B_glove(all_weights_B[0], optimizer='adam',embedding_name='Glove')"],"id":"JV4pBoHrpjVl","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r0it [00:00, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","11/11 [==============================] - ETA: 0s - loss: 807.0024 - accuracy: 0.5411\n","Epoch 00001: val_val_accuracy improved from -inf to 0.83979, saving model to ./models/Bilstm_crf/best_model_bilstm0.hdf5\n","11/11 [==============================] - 20s 2s/step - loss: 807.0024 - accuracy: 0.5411 - val_loss_val: 680.4657 - val_val_accuracy: 0.8398\n","Epoch 2/50\n","11/11 [==============================] - ETA: 0s - loss: 587.5208 - accuracy: 0.8569\n","Epoch 00002: val_val_accuracy improved from 0.83979 to 0.85170, saving model to ./models/Bilstm_crf/best_model_bilstm0.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 587.5208 - accuracy: 0.8569 - val_loss_val: 469.6287 - val_val_accuracy: 0.8517\n","Epoch 3/50\n","11/11 [==============================] - ETA: 0s - loss: 381.4702 - accuracy: 0.8624\n","Epoch 00003: val_val_accuracy improved from 0.85170 to 0.85443, saving model to ./models/Bilstm_crf/best_model_bilstm0.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 381.4702 - accuracy: 0.8624 - val_loss_val: 326.3964 - val_val_accuracy: 0.8544\n","Epoch 4/50\n","11/11 [==============================] - ETA: 0s - loss: 271.4140 - accuracy: 0.8646\n","Epoch 00004: val_val_accuracy improved from 0.85443 to 0.85459, saving model to ./models/Bilstm_crf/best_model_bilstm0.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 271.4140 - accuracy: 0.8646 - val_loss_val: 269.8978 - val_val_accuracy: 0.8546\n","Epoch 5/50\n","11/11 [==============================] - ETA: 0s - loss: 231.8076 - accuracy: 0.8648\n","Epoch 00005: val_val_accuracy did not improve from 0.85459\n","11/11 [==============================] - 18s 2s/step - loss: 231.8076 - accuracy: 0.8648 - val_loss_val: 247.4249 - val_val_accuracy: 0.8545\n","Epoch 6/50\n","11/11 [==============================] - ETA: 0s - loss: 214.0408 - accuracy: 0.8650\n","Epoch 00006: val_val_accuracy did not improve from 0.85459\n","11/11 [==============================] - 18s 2s/step - loss: 214.0408 - accuracy: 0.8650 - val_loss_val: 234.9150 - val_val_accuracy: 0.8545\n","Epoch 7/50\n","11/11 [==============================] - ETA: 0s - loss: 203.0500 - accuracy: 0.8655\n","Epoch 00007: val_val_accuracy improved from 0.85459 to 0.85474, saving model to ./models/Bilstm_crf/best_model_bilstm0.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 203.0500 - accuracy: 0.8655 - val_loss_val: 226.6723 - val_val_accuracy: 0.8547\n","Epoch 8/50\n","11/11 [==============================] - ETA: 0s - loss: 194.8345 - accuracy: 0.8664\n","Epoch 00008: val_val_accuracy improved from 0.85474 to 0.85515, saving model to ./models/Bilstm_crf/best_model_bilstm0.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 194.8345 - accuracy: 0.8664 - val_loss_val: 220.2858 - val_val_accuracy: 0.8552\n","Epoch 9/50\n","11/11 [==============================] - ETA: 0s - loss: 187.9812 - accuracy: 0.8672\n","Epoch 00009: val_val_accuracy improved from 0.85515 to 0.85536, saving model to ./models/Bilstm_crf/best_model_bilstm0.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 187.9812 - accuracy: 0.8672 - val_loss_val: 214.9545 - val_val_accuracy: 0.8554\n","Epoch 10/50\n","11/11 [==============================] - ETA: 0s - loss: 181.6240 - accuracy: 0.8689\n","Epoch 00010: val_val_accuracy did not improve from 0.85536\n","11/11 [==============================] - 18s 2s/step - loss: 181.6240 - accuracy: 0.8689 - val_loss_val: 209.4672 - val_val_accuracy: 0.8552\n","Epoch 11/50\n","11/11 [==============================] - ETA: 0s - loss: 176.1293 - accuracy: 0.8701\n","Epoch 00011: val_val_accuracy did not improve from 0.85536\n","11/11 [==============================] - 18s 2s/step - loss: 176.1293 - accuracy: 0.8701 - val_loss_val: 206.0206 - val_val_accuracy: 0.8549\n","Epoch 12/50\n","11/11 [==============================] - ETA: 0s - loss: 170.9489 - accuracy: 0.8720\n","Epoch 00012: val_val_accuracy improved from 0.85536 to 0.85634, saving model to ./models/Bilstm_crf/best_model_bilstm0.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 170.9489 - accuracy: 0.8720 - val_loss_val: 201.8775 - val_val_accuracy: 0.8563\n","Epoch 13/50\n","11/11 [==============================] - ETA: 0s - loss: 165.8206 - accuracy: 0.8736\n","Epoch 00013: val_val_accuracy did not improve from 0.85634\n","11/11 [==============================] - 18s 2s/step - loss: 165.8206 - accuracy: 0.8736 - val_loss_val: 198.5037 - val_val_accuracy: 0.8558\n","Epoch 14/50\n","11/11 [==============================] - ETA: 0s - loss: 161.2365 - accuracy: 0.8754\n","Epoch 00014: val_val_accuracy improved from 0.85634 to 0.85722, saving model to ./models/Bilstm_crf/best_model_bilstm0.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 161.2365 - accuracy: 0.8754 - val_loss_val: 195.3550 - val_val_accuracy: 0.8572\n","Epoch 15/50\n","11/11 [==============================] - ETA: 0s - loss: 156.6047 - accuracy: 0.8768\n","Epoch 00015: val_val_accuracy improved from 0.85722 to 0.85753, saving model to ./models/Bilstm_crf/best_model_bilstm0.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 156.6047 - accuracy: 0.8768 - val_loss_val: 193.5338 - val_val_accuracy: 0.8575\n","Epoch 16/50\n","11/11 [==============================] - ETA: 0s - loss: 152.0824 - accuracy: 0.8796\n","Epoch 00016: val_val_accuracy did not improve from 0.85753\n","11/11 [==============================] - 18s 2s/step - loss: 152.0824 - accuracy: 0.8796 - val_loss_val: 190.8756 - val_val_accuracy: 0.8568\n","Epoch 17/50\n","11/11 [==============================] - ETA: 0s - loss: 147.9516 - accuracy: 0.8821\n","Epoch 00017: val_val_accuracy did not improve from 0.85753\n","11/11 [==============================] - 18s 2s/step - loss: 147.9516 - accuracy: 0.8821 - val_loss_val: 188.4414 - val_val_accuracy: 0.8572\n","Epoch 18/50\n","11/11 [==============================] - ETA: 0s - loss: 143.9324 - accuracy: 0.8841\n","Epoch 00018: val_val_accuracy improved from 0.85753 to 0.85938, saving model to ./models/Bilstm_crf/best_model_bilstm0.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 143.9324 - accuracy: 0.8841 - val_loss_val: 185.4940 - val_val_accuracy: 0.8594\n","Epoch 19/50\n","11/11 [==============================] - ETA: 0s - loss: 140.0131 - accuracy: 0.8876\n","Epoch 00019: val_val_accuracy did not improve from 0.85938\n","11/11 [==============================] - 18s 2s/step - loss: 140.0131 - accuracy: 0.8876 - val_loss_val: 183.6222 - val_val_accuracy: 0.8585\n","Epoch 20/50\n","11/11 [==============================] - ETA: 0s - loss: 136.3965 - accuracy: 0.8897\n","Epoch 00020: val_val_accuracy did not improve from 0.85938\n","11/11 [==============================] - 18s 2s/step - loss: 136.3965 - accuracy: 0.8897 - val_loss_val: 182.1151 - val_val_accuracy: 0.8591\n","Epoch 21/50\n","11/11 [==============================] - ETA: 0s - loss: 132.1985 - accuracy: 0.8933\n","Epoch 00021: val_val_accuracy did not improve from 0.85938\n","11/11 [==============================] - 18s 2s/step - loss: 132.1985 - accuracy: 0.8933 - val_loss_val: 180.2946 - val_val_accuracy: 0.8587\n","Epoch 22/50\n","11/11 [==============================] - ETA: 0s - loss: 128.6931 - accuracy: 0.8959\n","Epoch 00022: val_val_accuracy improved from 0.85938 to 0.86000, saving model to ./models/Bilstm_crf/best_model_bilstm0.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 128.6931 - accuracy: 0.8959 - val_loss_val: 179.0367 - val_val_accuracy: 0.8600\n","Epoch 23/50\n","11/11 [==============================] - ETA: 0s - loss: 124.9890 - accuracy: 0.8990\n","Epoch 00023: val_val_accuracy improved from 0.86000 to 0.86082, saving model to ./models/Bilstm_crf/best_model_bilstm0.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 124.9890 - accuracy: 0.8990 - val_loss_val: 178.0174 - val_val_accuracy: 0.8608\n","Epoch 24/50\n","11/11 [==============================] - ETA: 0s - loss: 121.7529 - accuracy: 0.9025\n","Epoch 00024: val_val_accuracy did not improve from 0.86082\n","11/11 [==============================] - 18s 2s/step - loss: 121.7529 - accuracy: 0.9025 - val_loss_val: 177.0018 - val_val_accuracy: 0.8587\n","Epoch 25/50\n","11/11 [==============================] - ETA: 0s - loss: 118.6632 - accuracy: 0.9039\n","Epoch 00025: val_val_accuracy improved from 0.86082 to 0.86098, saving model to ./models/Bilstm_crf/best_model_bilstm0.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 118.6632 - accuracy: 0.9039 - val_loss_val: 175.3280 - val_val_accuracy: 0.8610\n","Epoch 26/50\n","11/11 [==============================] - ETA: 0s - loss: 115.2608 - accuracy: 0.9075\n","Epoch 00026: val_val_accuracy did not improve from 0.86098\n","11/11 [==============================] - 18s 2s/step - loss: 115.2608 - accuracy: 0.9075 - val_loss_val: 173.6423 - val_val_accuracy: 0.8601\n","Epoch 27/50\n","11/11 [==============================] - ETA: 0s - loss: 111.9921 - accuracy: 0.9100\n","Epoch 00027: val_val_accuracy did not improve from 0.86098\n","11/11 [==============================] - 18s 2s/step - loss: 111.9921 - accuracy: 0.9100 - val_loss_val: 172.5751 - val_val_accuracy: 0.8605\n","Epoch 28/50\n","11/11 [==============================] - ETA: 0s - loss: 109.0982 - accuracy: 0.9128\n","Epoch 00028: val_val_accuracy did not improve from 0.86098\n","11/11 [==============================] - 18s 2s/step - loss: 109.0982 - accuracy: 0.9128 - val_loss_val: 171.7777 - val_val_accuracy: 0.8601\n","Epoch 29/50\n","11/11 [==============================] - ETA: 0s - loss: 106.2899 - accuracy: 0.9150\n","Epoch 00029: val_val_accuracy improved from 0.86098 to 0.86103, saving model to ./models/Bilstm_crf/best_model_bilstm0.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 106.2899 - accuracy: 0.9150 - val_loss_val: 169.7029 - val_val_accuracy: 0.8610\n","Epoch 30/50\n","11/11 [==============================] - ETA: 0s - loss: 103.2630 - accuracy: 0.9178\n","Epoch 00030: val_val_accuracy did not improve from 0.86103\n","11/11 [==============================] - 18s 2s/step - loss: 103.2630 - accuracy: 0.9178 - val_loss_val: 170.3106 - val_val_accuracy: 0.8605\n","Epoch 31/50\n","11/11 [==============================] - ETA: 0s - loss: 100.5167 - accuracy: 0.9192\n","Epoch 00031: val_val_accuracy did not improve from 0.86103\n","11/11 [==============================] - 18s 2s/step - loss: 100.5167 - accuracy: 0.9192 - val_loss_val: 169.3535 - val_val_accuracy: 0.8604\n","Epoch 32/50\n","11/11 [==============================] - ETA: 0s - loss: 97.8038 - accuracy: 0.9217\n","Epoch 00032: val_val_accuracy did not improve from 0.86103\n","11/11 [==============================] - 18s 2s/step - loss: 97.8038 - accuracy: 0.9217 - val_loss_val: 169.1068 - val_val_accuracy: 0.8574\n","Epoch 33/50\n","11/11 [==============================] - ETA: 0s - loss: 95.1800 - accuracy: 0.9234\n","Epoch 00033: val_val_accuracy did not improve from 0.86103\n","11/11 [==============================] - 18s 2s/step - loss: 95.1800 - accuracy: 0.9234 - val_loss_val: 167.7753 - val_val_accuracy: 0.8601\n","Epoch 34/50\n","11/11 [==============================] - ETA: 0s - loss: 92.6479 - accuracy: 0.9264\n","Epoch 00034: val_val_accuracy did not improve from 0.86103\n","11/11 [==============================] - 18s 2s/step - loss: 92.6479 - accuracy: 0.9264 - val_loss_val: 168.0924 - val_val_accuracy: 0.8596\n","Epoch 35/50\n","11/11 [==============================] - ETA: 0s - loss: 90.2592 - accuracy: 0.9273\n","Epoch 00035: val_val_accuracy did not improve from 0.86103\n","11/11 [==============================] - 18s 2s/step - loss: 90.2592 - accuracy: 0.9273 - val_loss_val: 167.4370 - val_val_accuracy: 0.8589\n","Epoch 36/50\n","11/11 [==============================] - ETA: 0s - loss: 87.7203 - accuracy: 0.9304\n","Epoch 00036: val_val_accuracy did not improve from 0.86103\n","11/11 [==============================] - 18s 2s/step - loss: 87.7203 - accuracy: 0.9304 - val_loss_val: 167.9478 - val_val_accuracy: 0.8578\n","Epoch 37/50\n","11/11 [==============================] - ETA: 0s - loss: 85.5144 - accuracy: 0.9315\n","Epoch 00037: val_val_accuracy did not improve from 0.86103\n","11/11 [==============================] - 18s 2s/step - loss: 85.5144 - accuracy: 0.9315 - val_loss_val: 167.0571 - val_val_accuracy: 0.8586\n","Epoch 38/50\n","11/11 [==============================] - ETA: 0s - loss: 83.4391 - accuracy: 0.9335\n","Epoch 00038: val_val_accuracy did not improve from 0.86103\n","11/11 [==============================] - 18s 2s/step - loss: 83.4391 - accuracy: 0.9335 - val_loss_val: 167.6400 - val_val_accuracy: 0.8592\n","Epoch 39/50\n","11/11 [==============================] - ETA: 0s - loss: 81.1359 - accuracy: 0.9352\n","Epoch 00039: val_val_accuracy did not improve from 0.86103\n","11/11 [==============================] - 18s 2s/step - loss: 81.1359 - accuracy: 0.9352 - val_loss_val: 164.4895 - val_val_accuracy: 0.8604\n","Epoch 40/50\n","11/11 [==============================] - ETA: 0s - loss: 79.3323 - accuracy: 0.9368\n","Epoch 00040: val_val_accuracy did not improve from 0.86103\n","11/11 [==============================] - 18s 2s/step - loss: 79.3323 - accuracy: 0.9368 - val_loss_val: 166.2983 - val_val_accuracy: 0.8581\n","Epoch 41/50\n","11/11 [==============================] - ETA: 0s - loss: 77.3066 - accuracy: 0.9391\n","Epoch 00041: val_val_accuracy did not improve from 0.86103\n","11/11 [==============================] - 18s 2s/step - loss: 77.3066 - accuracy: 0.9391 - val_loss_val: 164.2630 - val_val_accuracy: 0.8587\n","Epoch 42/50\n","11/11 [==============================] - ETA: 0s - loss: 75.0338 - accuracy: 0.9406\n","Epoch 00042: val_val_accuracy did not improve from 0.86103\n","11/11 [==============================] - 18s 2s/step - loss: 75.0338 - accuracy: 0.9406 - val_loss_val: 164.6913 - val_val_accuracy: 0.8605\n","Epoch 43/50\n","11/11 [==============================] - ETA: 0s - loss: 73.3967 - accuracy: 0.9418\n","Epoch 00043: val_val_accuracy did not improve from 0.86103\n","11/11 [==============================] - 18s 2s/step - loss: 73.3967 - accuracy: 0.9418 - val_loss_val: 165.5882 - val_val_accuracy: 0.8545\n","Epoch 44/50\n","11/11 [==============================] - ETA: 0s - loss: 71.7665 - accuracy: 0.9434\n","Epoch 00044: val_val_accuracy did not improve from 0.86103\n","11/11 [==============================] - 18s 2s/step - loss: 71.7665 - accuracy: 0.9434 - val_loss_val: 164.9940 - val_val_accuracy: 0.8585\n"]},{"output_type":"stream","name":"stderr","text":["\r1it [13:23, 803.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","11/11 [==============================] - ETA: 0s - loss: 871.5256 - accuracy: 0.3423\n","Epoch 00001: val_val_accuracy improved from -inf to 0.43830, saving model to ./models/Bilstm_crf/best_model_bilstm1.hdf5\n","11/11 [==============================] - 30s 2s/step - loss: 871.5256 - accuracy: 0.3423 - val_loss_val: 789.3542 - val_val_accuracy: 0.4383\n","Epoch 2/50\n","11/11 [==============================] - ETA: 0s - loss: 660.3671 - accuracy: 0.7099\n","Epoch 00002: val_val_accuracy improved from 0.43830 to 0.84510, saving model to ./models/Bilstm_crf/best_model_bilstm1.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 660.3671 - accuracy: 0.7099 - val_loss_val: 592.8926 - val_val_accuracy: 0.8451\n","Epoch 3/50\n","11/11 [==============================] - ETA: 0s - loss: 458.4522 - accuracy: 0.8601\n","Epoch 00003: val_val_accuracy improved from 0.84510 to 0.85052, saving model to ./models/Bilstm_crf/best_model_bilstm1.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 458.4522 - accuracy: 0.8601 - val_loss_val: 430.0552 - val_val_accuracy: 0.8505\n","Epoch 4/50\n","11/11 [==============================] - ETA: 0s - loss: 324.2320 - accuracy: 0.8622\n","Epoch 00004: val_val_accuracy improved from 0.85052 to 0.85371, saving model to ./models/Bilstm_crf/best_model_bilstm1.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 324.2320 - accuracy: 0.8622 - val_loss_val: 314.1929 - val_val_accuracy: 0.8537\n","Epoch 5/50\n","11/11 [==============================] - ETA: 0s - loss: 249.9963 - accuracy: 0.8647\n","Epoch 00005: val_val_accuracy improved from 0.85371 to 0.85448, saving model to ./models/Bilstm_crf/best_model_bilstm1.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 249.9963 - accuracy: 0.8647 - val_loss_val: 260.6068 - val_val_accuracy: 0.8545\n","Epoch 6/50\n","11/11 [==============================] - ETA: 0s - loss: 224.1168 - accuracy: 0.8647\n","Epoch 00006: val_val_accuracy did not improve from 0.85448\n","11/11 [==============================] - 18s 2s/step - loss: 224.1168 - accuracy: 0.8647 - val_loss_val: 243.7290 - val_val_accuracy: 0.8544\n","Epoch 7/50\n","11/11 [==============================] - ETA: 0s - loss: 209.6086 - accuracy: 0.8647\n","Epoch 00007: val_val_accuracy did not improve from 0.85448\n","11/11 [==============================] - 18s 2s/step - loss: 209.6086 - accuracy: 0.8647 - val_loss_val: 233.7501 - val_val_accuracy: 0.8544\n","Epoch 8/50\n","11/11 [==============================] - ETA: 0s - loss: 197.6908 - accuracy: 0.8649\n","Epoch 00008: val_val_accuracy did not improve from 0.85448\n","11/11 [==============================] - 18s 2s/step - loss: 197.6908 - accuracy: 0.8649 - val_loss_val: 227.7679 - val_val_accuracy: 0.8545\n","Epoch 9/50\n","11/11 [==============================] - ETA: 0s - loss: 188.9259 - accuracy: 0.8657\n","Epoch 00009: val_val_accuracy improved from 0.85448 to 0.85479, saving model to ./models/Bilstm_crf/best_model_bilstm1.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 188.9259 - accuracy: 0.8657 - val_loss_val: 222.6234 - val_val_accuracy: 0.8548\n","Epoch 10/50\n","11/11 [==============================] - ETA: 0s - loss: 181.3447 - accuracy: 0.8673\n","Epoch 00010: val_val_accuracy improved from 0.85479 to 0.85495, saving model to ./models/Bilstm_crf/best_model_bilstm1.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 181.3447 - accuracy: 0.8673 - val_loss_val: 217.6735 - val_val_accuracy: 0.8549\n","Epoch 11/50\n","11/11 [==============================] - ETA: 0s - loss: 174.6202 - accuracy: 0.8696\n","Epoch 00011: val_val_accuracy improved from 0.85495 to 0.85536, saving model to ./models/Bilstm_crf/best_model_bilstm1.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 174.6202 - accuracy: 0.8696 - val_loss_val: 213.6043 - val_val_accuracy: 0.8554\n","Epoch 12/50\n","11/11 [==============================] - ETA: 0s - loss: 168.3243 - accuracy: 0.8731\n","Epoch 00012: val_val_accuracy improved from 0.85536 to 0.85567, saving model to ./models/Bilstm_crf/best_model_bilstm1.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 168.3243 - accuracy: 0.8731 - val_loss_val: 210.2421 - val_val_accuracy: 0.8557\n","Epoch 13/50\n","11/11 [==============================] - ETA: 0s - loss: 162.4570 - accuracy: 0.8775\n","Epoch 00013: val_val_accuracy did not improve from 0.85567\n","11/11 [==============================] - 18s 2s/step - loss: 162.4570 - accuracy: 0.8775 - val_loss_val: 207.6731 - val_val_accuracy: 0.8549\n","Epoch 14/50\n","11/11 [==============================] - ETA: 0s - loss: 156.8461 - accuracy: 0.8813\n","Epoch 00014: val_val_accuracy did not improve from 0.85567\n","11/11 [==============================] - 18s 2s/step - loss: 156.8461 - accuracy: 0.8813 - val_loss_val: 205.1167 - val_val_accuracy: 0.8546\n","Epoch 15/50\n","11/11 [==============================] - ETA: 0s - loss: 151.3116 - accuracy: 0.8837\n","Epoch 00015: val_val_accuracy did not improve from 0.85567\n","11/11 [==============================] - 18s 2s/step - loss: 151.3116 - accuracy: 0.8837 - val_loss_val: 202.6045 - val_val_accuracy: 0.8540\n","Epoch 16/50\n","11/11 [==============================] - ETA: 0s - loss: 145.8942 - accuracy: 0.8864\n","Epoch 00016: val_val_accuracy did not improve from 0.85567\n","11/11 [==============================] - 18s 2s/step - loss: 145.8942 - accuracy: 0.8864 - val_loss_val: 200.5817 - val_val_accuracy: 0.8543\n","Epoch 17/50\n","11/11 [==============================] - ETA: 0s - loss: 140.4946 - accuracy: 0.8901\n","Epoch 00017: val_val_accuracy did not improve from 0.85567\n","11/11 [==============================] - 18s 2s/step - loss: 140.4946 - accuracy: 0.8901 - val_loss_val: 198.7187 - val_val_accuracy: 0.8544\n","Epoch 18/50\n","11/11 [==============================] - ETA: 0s - loss: 135.2005 - accuracy: 0.8944\n","Epoch 00018: val_val_accuracy did not improve from 0.85567\n","11/11 [==============================] - 18s 2s/step - loss: 135.2005 - accuracy: 0.8944 - val_loss_val: 196.6920 - val_val_accuracy: 0.8549\n","Epoch 19/50\n","11/11 [==============================] - ETA: 0s - loss: 129.9295 - accuracy: 0.8992\n","Epoch 00019: val_val_accuracy improved from 0.85567 to 0.85577, saving model to ./models/Bilstm_crf/best_model_bilstm1.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 129.9295 - accuracy: 0.8992 - val_loss_val: 195.2861 - val_val_accuracy: 0.8558\n","Epoch 20/50\n","11/11 [==============================] - ETA: 0s - loss: 124.6647 - accuracy: 0.9048\n","Epoch 00020: val_val_accuracy did not improve from 0.85577\n","11/11 [==============================] - 18s 2s/step - loss: 124.6647 - accuracy: 0.9048 - val_loss_val: 194.1591 - val_val_accuracy: 0.8551\n","Epoch 21/50\n","11/11 [==============================] - ETA: 0s - loss: 119.7257 - accuracy: 0.9099\n","Epoch 00021: val_val_accuracy improved from 0.85577 to 0.85634, saving model to ./models/Bilstm_crf/best_model_bilstm1.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 119.7257 - accuracy: 0.9099 - val_loss_val: 192.6086 - val_val_accuracy: 0.8563\n","Epoch 22/50\n","11/11 [==============================] - ETA: 0s - loss: 114.8866 - accuracy: 0.9142\n","Epoch 00022: val_val_accuracy did not improve from 0.85634\n","11/11 [==============================] - 18s 2s/step - loss: 114.8866 - accuracy: 0.9142 - val_loss_val: 192.1039 - val_val_accuracy: 0.8563\n","Epoch 23/50\n","11/11 [==============================] - ETA: 0s - loss: 110.2162 - accuracy: 0.9183\n","Epoch 00023: val_val_accuracy improved from 0.85634 to 0.85660, saving model to ./models/Bilstm_crf/best_model_bilstm1.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 110.2162 - accuracy: 0.9183 - val_loss_val: 191.4704 - val_val_accuracy: 0.8566\n","Epoch 24/50\n","11/11 [==============================] - ETA: 0s - loss: 105.9025 - accuracy: 0.9223\n","Epoch 00024: val_val_accuracy improved from 0.85660 to 0.85665, saving model to ./models/Bilstm_crf/best_model_bilstm1.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 105.9025 - accuracy: 0.9223 - val_loss_val: 190.7731 - val_val_accuracy: 0.8566\n","Epoch 25/50\n","11/11 [==============================] - ETA: 0s - loss: 101.5757 - accuracy: 0.9265\n","Epoch 00025: val_val_accuracy improved from 0.85665 to 0.85861, saving model to ./models/Bilstm_crf/best_model_bilstm1.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 101.5757 - accuracy: 0.9265 - val_loss_val: 190.6421 - val_val_accuracy: 0.8586\n","Epoch 26/50\n","11/11 [==============================] - ETA: 0s - loss: 97.3890 - accuracy: 0.9298\n","Epoch 00026: val_val_accuracy did not improve from 0.85861\n","11/11 [==============================] - 18s 2s/step - loss: 97.3890 - accuracy: 0.9298 - val_loss_val: 190.2722 - val_val_accuracy: 0.8574\n","Epoch 27/50\n","11/11 [==============================] - ETA: 0s - loss: 93.6885 - accuracy: 0.9334\n","Epoch 00027: val_val_accuracy did not improve from 0.85861\n","11/11 [==============================] - 18s 2s/step - loss: 93.6885 - accuracy: 0.9334 - val_loss_val: 189.9235 - val_val_accuracy: 0.8563\n","Epoch 28/50\n","11/11 [==============================] - ETA: 0s - loss: 89.8938 - accuracy: 0.9357\n","Epoch 00028: val_val_accuracy did not improve from 0.85861\n","11/11 [==============================] - 18s 2s/step - loss: 89.8938 - accuracy: 0.9357 - val_loss_val: 190.6770 - val_val_accuracy: 0.8561\n","Epoch 29/50\n","11/11 [==============================] - ETA: 0s - loss: 86.3524 - accuracy: 0.9390\n","Epoch 00029: val_val_accuracy did not improve from 0.85861\n","11/11 [==============================] - 18s 2s/step - loss: 86.3524 - accuracy: 0.9390 - val_loss_val: 190.2195 - val_val_accuracy: 0.8564\n","Epoch 30/50\n","11/11 [==============================] - ETA: 0s - loss: 83.1249 - accuracy: 0.9413\n","Epoch 00030: val_val_accuracy did not improve from 0.85861\n","11/11 [==============================] - 18s 2s/step - loss: 83.1249 - accuracy: 0.9413 - val_loss_val: 190.4482 - val_val_accuracy: 0.8572\n"]},{"output_type":"stream","name":"stderr","text":["\r2it [23:32, 688.83s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","11/11 [==============================] - ETA: 0s - loss: 1000.4501 - accuracy: 0.2004\n","Epoch 00001: val_val_accuracy improved from -inf to 0.49113, saving model to ./models/Bilstm_crf/best_model_bilstm2.hdf5\n","11/11 [==============================] - 29s 2s/step - loss: 1000.4501 - accuracy: 0.2004 - val_loss_val: 859.6066 - val_val_accuracy: 0.4911\n","Epoch 2/50\n","11/11 [==============================] - ETA: 0s - loss: 795.4606 - accuracy: 0.4976\n","Epoch 00002: val_val_accuracy improved from 0.49113 to 0.57567, saving model to ./models/Bilstm_crf/best_model_bilstm2.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 795.4606 - accuracy: 0.4976 - val_loss_val: 658.5172 - val_val_accuracy: 0.5757\n","Epoch 3/50\n","11/11 [==============================] - ETA: 0s - loss: 603.5964 - accuracy: 0.7917\n","Epoch 00003: val_val_accuracy improved from 0.57567 to 0.85103, saving model to ./models/Bilstm_crf/best_model_bilstm2.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 603.5964 - accuracy: 0.7917 - val_loss_val: 484.3324 - val_val_accuracy: 0.8510\n","Epoch 4/50\n","11/11 [==============================] - ETA: 0s - loss: 436.3708 - accuracy: 0.8612\n","Epoch 00004: val_val_accuracy did not improve from 0.85103\n","11/11 [==============================] - 18s 2s/step - loss: 436.3708 - accuracy: 0.8612 - val_loss_val: 366.4328 - val_val_accuracy: 0.8508\n","Epoch 5/50\n","11/11 [==============================] - ETA: 0s - loss: 330.2451 - accuracy: 0.8596\n","Epoch 00005: val_val_accuracy did not improve from 0.85103\n","11/11 [==============================] - 18s 2s/step - loss: 330.2451 - accuracy: 0.8596 - val_loss_val: 305.0280 - val_val_accuracy: 0.8480\n","Epoch 6/50\n","11/11 [==============================] - ETA: 0s - loss: 272.1318 - accuracy: 0.8576\n","Epoch 00006: val_val_accuracy did not improve from 0.85103\n","11/11 [==============================] - 18s 2s/step - loss: 272.1318 - accuracy: 0.8576 - val_loss_val: 270.3025 - val_val_accuracy: 0.8461\n","Epoch 7/50\n","11/11 [==============================] - ETA: 0s - loss: 239.1347 - accuracy: 0.8576\n","Epoch 00007: val_val_accuracy did not improve from 0.85103\n","11/11 [==============================] - 18s 2s/step - loss: 239.1347 - accuracy: 0.8576 - val_loss_val: 250.2759 - val_val_accuracy: 0.8469\n","Epoch 8/50\n","11/11 [==============================] - ETA: 0s - loss: 219.7594 - accuracy: 0.8585\n","Epoch 00008: val_val_accuracy did not improve from 0.85103\n","11/11 [==============================] - 18s 2s/step - loss: 219.7594 - accuracy: 0.8585 - val_loss_val: 236.8110 - val_val_accuracy: 0.8487\n","Epoch 9/50\n","11/11 [==============================] - ETA: 0s - loss: 206.4988 - accuracy: 0.8600\n","Epoch 00009: val_val_accuracy did not improve from 0.85103\n","11/11 [==============================] - 18s 2s/step - loss: 206.4988 - accuracy: 0.8600 - val_loss_val: 226.8684 - val_val_accuracy: 0.8498\n","Epoch 10/50\n","11/11 [==============================] - ETA: 0s - loss: 196.7215 - accuracy: 0.8619\n","Epoch 00010: val_val_accuracy improved from 0.85103 to 0.85222, saving model to ./models/Bilstm_crf/best_model_bilstm2.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 196.7215 - accuracy: 0.8619 - val_loss_val: 219.1879 - val_val_accuracy: 0.8522\n","Epoch 11/50\n","11/11 [==============================] - ETA: 0s - loss: 188.8311 - accuracy: 0.8637\n","Epoch 00011: val_val_accuracy improved from 0.85222 to 0.85325, saving model to ./models/Bilstm_crf/best_model_bilstm2.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 188.8311 - accuracy: 0.8637 - val_loss_val: 213.4052 - val_val_accuracy: 0.8532\n","Epoch 12/50\n","11/11 [==============================] - ETA: 0s - loss: 182.6172 - accuracy: 0.8644\n","Epoch 00012: val_val_accuracy improved from 0.85325 to 0.85412, saving model to ./models/Bilstm_crf/best_model_bilstm2.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 182.6172 - accuracy: 0.8644 - val_loss_val: 208.5168 - val_val_accuracy: 0.8541\n","Epoch 13/50\n","11/11 [==============================] - ETA: 0s - loss: 176.8681 - accuracy: 0.8647\n","Epoch 00013: val_val_accuracy improved from 0.85412 to 0.85433, saving model to ./models/Bilstm_crf/best_model_bilstm2.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 176.8681 - accuracy: 0.8647 - val_loss_val: 204.8638 - val_val_accuracy: 0.8543\n","Epoch 14/50\n","11/11 [==============================] - ETA: 0s - loss: 171.9780 - accuracy: 0.8648\n","Epoch 00014: val_val_accuracy improved from 0.85433 to 0.85443, saving model to ./models/Bilstm_crf/best_model_bilstm2.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 171.9780 - accuracy: 0.8648 - val_loss_val: 201.2421 - val_val_accuracy: 0.8544\n","Epoch 15/50\n","11/11 [==============================] - ETA: 0s - loss: 167.3855 - accuracy: 0.8649\n","Epoch 00015: val_val_accuracy did not improve from 0.85443\n","11/11 [==============================] - 18s 2s/step - loss: 167.3855 - accuracy: 0.8649 - val_loss_val: 198.2790 - val_val_accuracy: 0.8543\n","Epoch 16/50\n","11/11 [==============================] - ETA: 0s - loss: 163.3582 - accuracy: 0.8650\n","Epoch 00016: val_val_accuracy improved from 0.85443 to 0.85459, saving model to ./models/Bilstm_crf/best_model_bilstm2.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 163.3582 - accuracy: 0.8650 - val_loss_val: 195.6873 - val_val_accuracy: 0.8546\n","Epoch 17/50\n","11/11 [==============================] - ETA: 0s - loss: 159.4690 - accuracy: 0.8653\n","Epoch 00017: val_val_accuracy improved from 0.85459 to 0.85495, saving model to ./models/Bilstm_crf/best_model_bilstm2.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 159.4690 - accuracy: 0.8653 - val_loss_val: 193.1258 - val_val_accuracy: 0.8549\n","Epoch 18/50\n","11/11 [==============================] - ETA: 0s - loss: 155.8446 - accuracy: 0.8659\n","Epoch 00018: val_val_accuracy improved from 0.85495 to 0.85500, saving model to ./models/Bilstm_crf/best_model_bilstm2.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 155.8446 - accuracy: 0.8659 - val_loss_val: 191.4008 - val_val_accuracy: 0.8550\n","Epoch 19/50\n","11/11 [==============================] - ETA: 0s - loss: 152.2604 - accuracy: 0.8665\n","Epoch 00019: val_val_accuracy improved from 0.85500 to 0.85536, saving model to ./models/Bilstm_crf/best_model_bilstm2.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 152.2604 - accuracy: 0.8665 - val_loss_val: 189.7997 - val_val_accuracy: 0.8554\n","Epoch 20/50\n","11/11 [==============================] - ETA: 0s - loss: 148.9368 - accuracy: 0.8680\n","Epoch 00020: val_val_accuracy improved from 0.85536 to 0.85582, saving model to ./models/Bilstm_crf/best_model_bilstm2.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 148.9368 - accuracy: 0.8680 - val_loss_val: 187.8717 - val_val_accuracy: 0.8558\n","Epoch 21/50\n","11/11 [==============================] - ETA: 0s - loss: 145.7547 - accuracy: 0.8692\n","Epoch 00021: val_val_accuracy improved from 0.85582 to 0.85649, saving model to ./models/Bilstm_crf/best_model_bilstm2.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 145.7547 - accuracy: 0.8692 - val_loss_val: 186.6556 - val_val_accuracy: 0.8565\n","Epoch 22/50\n","11/11 [==============================] - ETA: 0s - loss: 142.7641 - accuracy: 0.8710\n","Epoch 00022: val_val_accuracy improved from 0.85649 to 0.85716, saving model to ./models/Bilstm_crf/best_model_bilstm2.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 142.7641 - accuracy: 0.8710 - val_loss_val: 184.5565 - val_val_accuracy: 0.8572\n","Epoch 23/50\n","11/11 [==============================] - ETA: 0s - loss: 139.4743 - accuracy: 0.8728\n","Epoch 00023: val_val_accuracy improved from 0.85716 to 0.85742, saving model to ./models/Bilstm_crf/best_model_bilstm2.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 139.4743 - accuracy: 0.8728 - val_loss_val: 183.4504 - val_val_accuracy: 0.8574\n","Epoch 24/50\n","11/11 [==============================] - ETA: 0s - loss: 136.5791 - accuracy: 0.8750\n","Epoch 00024: val_val_accuracy improved from 0.85742 to 0.85758, saving model to ./models/Bilstm_crf/best_model_bilstm2.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 136.5791 - accuracy: 0.8750 - val_loss_val: 181.6310 - val_val_accuracy: 0.8576\n","Epoch 25/50\n","11/11 [==============================] - ETA: 0s - loss: 133.7520 - accuracy: 0.8783\n","Epoch 00025: val_val_accuracy improved from 0.85758 to 0.85763, saving model to ./models/Bilstm_crf/best_model_bilstm2.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 133.7520 - accuracy: 0.8783 - val_loss_val: 180.3508 - val_val_accuracy: 0.8576\n","Epoch 26/50\n","11/11 [==============================] - ETA: 0s - loss: 130.8305 - accuracy: 0.8810\n","Epoch 00026: val_val_accuracy improved from 0.85763 to 0.85948, saving model to ./models/Bilstm_crf/best_model_bilstm2.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 130.8305 - accuracy: 0.8810 - val_loss_val: 179.9841 - val_val_accuracy: 0.8595\n","Epoch 27/50\n","11/11 [==============================] - ETA: 0s - loss: 127.7480 - accuracy: 0.8833\n","Epoch 00027: val_val_accuracy did not improve from 0.85948\n","11/11 [==============================] - 18s 2s/step - loss: 127.7480 - accuracy: 0.8833 - val_loss_val: 178.1965 - val_val_accuracy: 0.8588\n","Epoch 28/50\n","11/11 [==============================] - ETA: 0s - loss: 124.8237 - accuracy: 0.8866\n","Epoch 00028: val_val_accuracy improved from 0.85948 to 0.85959, saving model to ./models/Bilstm_crf/best_model_bilstm2.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 124.8237 - accuracy: 0.8866 - val_loss_val: 177.6303 - val_val_accuracy: 0.8596\n","Epoch 29/50\n","11/11 [==============================] - ETA: 0s - loss: 122.1869 - accuracy: 0.8903\n","Epoch 00029: val_val_accuracy improved from 0.85959 to 0.86015, saving model to ./models/Bilstm_crf/best_model_bilstm2.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 122.1869 - accuracy: 0.8903 - val_loss_val: 177.1898 - val_val_accuracy: 0.8602\n","Epoch 30/50\n","11/11 [==============================] - ETA: 0s - loss: 119.1689 - accuracy: 0.8920\n","Epoch 00030: val_val_accuracy did not improve from 0.86015\n","11/11 [==============================] - 18s 2s/step - loss: 119.1689 - accuracy: 0.8920 - val_loss_val: 175.5113 - val_val_accuracy: 0.8589\n","Epoch 31/50\n","11/11 [==============================] - ETA: 0s - loss: 116.6416 - accuracy: 0.8959\n","Epoch 00031: val_val_accuracy improved from 0.86015 to 0.86052, saving model to ./models/Bilstm_crf/best_model_bilstm2.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 116.6416 - accuracy: 0.8959 - val_loss_val: 174.5383 - val_val_accuracy: 0.8605\n","Epoch 32/50\n","11/11 [==============================] - ETA: 0s - loss: 114.2859 - accuracy: 0.8991\n","Epoch 00032: val_val_accuracy did not improve from 0.86052\n","11/11 [==============================] - 18s 2s/step - loss: 114.2859 - accuracy: 0.8991 - val_loss_val: 173.8984 - val_val_accuracy: 0.8596\n","Epoch 33/50\n","11/11 [==============================] - ETA: 0s - loss: 111.8299 - accuracy: 0.9021\n","Epoch 00033: val_val_accuracy did not improve from 0.86052\n","11/11 [==============================] - 18s 2s/step - loss: 111.8299 - accuracy: 0.9021 - val_loss_val: 173.7524 - val_val_accuracy: 0.8593\n","Epoch 34/50\n","11/11 [==============================] - ETA: 0s - loss: 108.8355 - accuracy: 0.9044\n","Epoch 00034: val_val_accuracy did not improve from 0.86052\n","11/11 [==============================] - 18s 2s/step - loss: 108.8355 - accuracy: 0.9044 - val_loss_val: 171.6891 - val_val_accuracy: 0.8595\n","Epoch 35/50\n","11/11 [==============================] - ETA: 0s - loss: 106.0489 - accuracy: 0.9069\n","Epoch 00035: val_val_accuracy did not improve from 0.86052\n","11/11 [==============================] - 18s 2s/step - loss: 106.0489 - accuracy: 0.9069 - val_loss_val: 172.9250 - val_val_accuracy: 0.8579\n","Epoch 36/50\n","11/11 [==============================] - ETA: 0s - loss: 103.8723 - accuracy: 0.9099\n","Epoch 00036: val_val_accuracy did not improve from 0.86052\n","11/11 [==============================] - 18s 2s/step - loss: 103.8723 - accuracy: 0.9099 - val_loss_val: 172.0298 - val_val_accuracy: 0.8602\n","Epoch 37/50\n","11/11 [==============================] - ETA: 0s - loss: 100.9214 - accuracy: 0.9127\n","Epoch 00037: val_val_accuracy did not improve from 0.86052\n","11/11 [==============================] - 18s 2s/step - loss: 100.9214 - accuracy: 0.9127 - val_loss_val: 171.2651 - val_val_accuracy: 0.8593\n","Epoch 38/50\n","11/11 [==============================] - ETA: 0s - loss: 99.0146 - accuracy: 0.9151\n","Epoch 00038: val_val_accuracy did not improve from 0.86052\n","11/11 [==============================] - 18s 2s/step - loss: 99.0146 - accuracy: 0.9151 - val_loss_val: 170.6615 - val_val_accuracy: 0.8598\n","Epoch 39/50\n","11/11 [==============================] - ETA: 0s - loss: 96.4071 - accuracy: 0.9164\n","Epoch 00039: val_val_accuracy did not improve from 0.86052\n","11/11 [==============================] - 18s 2s/step - loss: 96.4071 - accuracy: 0.9164 - val_loss_val: 170.9069 - val_val_accuracy: 0.8558\n","Epoch 40/50\n","11/11 [==============================] - ETA: 0s - loss: 94.1100 - accuracy: 0.9188\n","Epoch 00040: val_val_accuracy did not improve from 0.86052\n","11/11 [==============================] - 18s 2s/step - loss: 94.1100 - accuracy: 0.9188 - val_loss_val: 170.0628 - val_val_accuracy: 0.8584\n","Epoch 41/50\n","11/11 [==============================] - ETA: 0s - loss: 92.0098 - accuracy: 0.9216\n","Epoch 00041: val_val_accuracy did not improve from 0.86052\n","11/11 [==============================] - 18s 2s/step - loss: 92.0098 - accuracy: 0.9216 - val_loss_val: 171.0191 - val_val_accuracy: 0.8553\n","Epoch 42/50\n","11/11 [==============================] - ETA: 0s - loss: 89.9254 - accuracy: 0.9236\n","Epoch 00042: val_val_accuracy did not improve from 0.86052\n","11/11 [==============================] - 18s 2s/step - loss: 89.9254 - accuracy: 0.9236 - val_loss_val: 168.2649 - val_val_accuracy: 0.8580\n","Epoch 43/50\n","11/11 [==============================] - ETA: 0s - loss: 87.7833 - accuracy: 0.9252\n","Epoch 00043: val_val_accuracy did not improve from 0.86052\n","11/11 [==============================] - 18s 2s/step - loss: 87.7833 - accuracy: 0.9252 - val_loss_val: 168.2213 - val_val_accuracy: 0.8575\n","Epoch 44/50\n","11/11 [==============================] - ETA: 0s - loss: 85.5751 - accuracy: 0.9275\n","Epoch 00044: val_val_accuracy did not improve from 0.86052\n","11/11 [==============================] - 18s 2s/step - loss: 85.5751 - accuracy: 0.9275 - val_loss_val: 169.6542 - val_val_accuracy: 0.8578\n","Epoch 45/50\n","11/11 [==============================] - ETA: 0s - loss: 83.2842 - accuracy: 0.9292\n","Epoch 00045: val_val_accuracy did not improve from 0.86052\n","11/11 [==============================] - 18s 2s/step - loss: 83.2842 - accuracy: 0.9292 - val_loss_val: 168.3815 - val_val_accuracy: 0.8580\n","Epoch 46/50\n","11/11 [==============================] - ETA: 0s - loss: 81.4998 - accuracy: 0.9319\n","Epoch 00046: val_val_accuracy did not improve from 0.86052\n","11/11 [==============================] - 18s 2s/step - loss: 81.4998 - accuracy: 0.9319 - val_loss_val: 167.8617 - val_val_accuracy: 0.8593\n","Epoch 47/50\n","11/11 [==============================] - ETA: 0s - loss: 79.5027 - accuracy: 0.9330\n","Epoch 00047: val_val_accuracy did not improve from 0.86052\n","11/11 [==============================] - 18s 2s/step - loss: 79.5027 - accuracy: 0.9330 - val_loss_val: 166.0894 - val_val_accuracy: 0.8578\n","Epoch 48/50\n","11/11 [==============================] - ETA: 0s - loss: 77.5573 - accuracy: 0.9352\n","Epoch 00048: val_val_accuracy did not improve from 0.86052\n","11/11 [==============================] - 18s 2s/step - loss: 77.5573 - accuracy: 0.9352 - val_loss_val: 166.8419 - val_val_accuracy: 0.8580\n","Epoch 49/50\n","11/11 [==============================] - ETA: 0s - loss: 75.5265 - accuracy: 0.9377\n","Epoch 00049: val_val_accuracy did not improve from 0.86052\n","11/11 [==============================] - 18s 2s/step - loss: 75.5265 - accuracy: 0.9377 - val_loss_val: 167.4304 - val_val_accuracy: 0.8581\n","Epoch 50/50\n","11/11 [==============================] - ETA: 0s - loss: 73.5888 - accuracy: 0.9394\n","Epoch 00050: val_val_accuracy did not improve from 0.86052\n","11/11 [==============================] - 18s 2s/step - loss: 73.5888 - accuracy: 0.9394 - val_loss_val: 167.3110 - val_val_accuracy: 0.8581\n"]},{"output_type":"stream","name":"stderr","text":["\r3it [39:07, 801.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","11/11 [==============================] - ETA: 0s - loss: 1037.2866 - accuracy: 0.0334\n","Epoch 00001: val_val_accuracy improved from -inf to 0.33866, saving model to ./models/Bilstm_crf/best_model_bilstm3.hdf5\n","11/11 [==============================] - 28s 2s/step - loss: 1037.2866 - accuracy: 0.0334 - val_loss_val: 953.7915 - val_val_accuracy: 0.3387\n","Epoch 2/50\n","11/11 [==============================] - ETA: 0s - loss: 830.5369 - accuracy: 0.5674\n","Epoch 00002: val_val_accuracy improved from 0.33866 to 0.61902, saving model to ./models/Bilstm_crf/best_model_bilstm3.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 830.5369 - accuracy: 0.5674 - val_loss_val: 746.5834 - val_val_accuracy: 0.6190\n","Epoch 3/50\n","11/11 [==============================] - ETA: 0s - loss: 596.9462 - accuracy: 0.7110\n","Epoch 00003: val_val_accuracy improved from 0.61902 to 0.72948, saving model to ./models/Bilstm_crf/best_model_bilstm3.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 596.9462 - accuracy: 0.7110 - val_loss_val: 544.5524 - val_val_accuracy: 0.7295\n","Epoch 4/50\n","11/11 [==============================] - ETA: 0s - loss: 419.9850 - accuracy: 0.8385\n","Epoch 00004: val_val_accuracy improved from 0.72948 to 0.84376, saving model to ./models/Bilstm_crf/best_model_bilstm3.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 419.9850 - accuracy: 0.8385 - val_loss_val: 412.3529 - val_val_accuracy: 0.8438\n","Epoch 5/50\n","11/11 [==============================] - ETA: 0s - loss: 324.5074 - accuracy: 0.8562\n","Epoch 00005: val_val_accuracy improved from 0.84376 to 0.84665, saving model to ./models/Bilstm_crf/best_model_bilstm3.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 324.5074 - accuracy: 0.8562 - val_loss_val: 331.4955 - val_val_accuracy: 0.8466\n","Epoch 6/50\n","11/11 [==============================] - ETA: 0s - loss: 271.8617 - accuracy: 0.8583\n","Epoch 00006: val_val_accuracy improved from 0.84665 to 0.84840, saving model to ./models/Bilstm_crf/best_model_bilstm3.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 271.8617 - accuracy: 0.8583 - val_loss_val: 285.2889 - val_val_accuracy: 0.8484\n","Epoch 7/50\n","11/11 [==============================] - ETA: 0s - loss: 245.2692 - accuracy: 0.8600\n","Epoch 00007: val_val_accuracy improved from 0.84840 to 0.84959, saving model to ./models/Bilstm_crf/best_model_bilstm3.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 245.2692 - accuracy: 0.8600 - val_loss_val: 264.3402 - val_val_accuracy: 0.8496\n","Epoch 8/50\n","11/11 [==============================] - ETA: 0s - loss: 232.0424 - accuracy: 0.8611\n","Epoch 00008: val_val_accuracy improved from 0.84959 to 0.85046, saving model to ./models/Bilstm_crf/best_model_bilstm3.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 232.0424 - accuracy: 0.8611 - val_loss_val: 253.7720 - val_val_accuracy: 0.8505\n","Epoch 9/50\n","11/11 [==============================] - ETA: 0s - loss: 222.5263 - accuracy: 0.8615\n","Epoch 00009: val_val_accuracy improved from 0.85046 to 0.85088, saving model to ./models/Bilstm_crf/best_model_bilstm3.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 222.5263 - accuracy: 0.8615 - val_loss_val: 246.1937 - val_val_accuracy: 0.8509\n","Epoch 10/50\n","11/11 [==============================] - ETA: 0s - loss: 213.9485 - accuracy: 0.8616\n","Epoch 00010: val_val_accuracy improved from 0.85088 to 0.85093, saving model to ./models/Bilstm_crf/best_model_bilstm3.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 213.9485 - accuracy: 0.8616 - val_loss_val: 239.8134 - val_val_accuracy: 0.8509\n","Epoch 11/50\n","11/11 [==============================] - ETA: 0s - loss: 206.3823 - accuracy: 0.8618\n","Epoch 00011: val_val_accuracy improved from 0.85093 to 0.85119, saving model to ./models/Bilstm_crf/best_model_bilstm3.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 206.3823 - accuracy: 0.8618 - val_loss_val: 234.5891 - val_val_accuracy: 0.8512\n","Epoch 12/50\n","11/11 [==============================] - ETA: 0s - loss: 199.7957 - accuracy: 0.8622\n","Epoch 00012: val_val_accuracy improved from 0.85119 to 0.85155, saving model to ./models/Bilstm_crf/best_model_bilstm3.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 199.7957 - accuracy: 0.8622 - val_loss_val: 229.6388 - val_val_accuracy: 0.8515\n","Epoch 13/50\n","11/11 [==============================] - ETA: 0s - loss: 194.0285 - accuracy: 0.8631\n","Epoch 00013: val_val_accuracy improved from 0.85155 to 0.85273, saving model to ./models/Bilstm_crf/best_model_bilstm3.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 194.0285 - accuracy: 0.8631 - val_loss_val: 225.2984 - val_val_accuracy: 0.8527\n","Epoch 14/50\n","11/11 [==============================] - ETA: 0s - loss: 188.7492 - accuracy: 0.8642\n","Epoch 00014: val_val_accuracy improved from 0.85273 to 0.85351, saving model to ./models/Bilstm_crf/best_model_bilstm3.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 188.7492 - accuracy: 0.8642 - val_loss_val: 221.7331 - val_val_accuracy: 0.8535\n","Epoch 15/50\n","11/11 [==============================] - ETA: 0s - loss: 184.0856 - accuracy: 0.8647\n","Epoch 00015: val_val_accuracy improved from 0.85351 to 0.85428, saving model to ./models/Bilstm_crf/best_model_bilstm3.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 184.0856 - accuracy: 0.8647 - val_loss_val: 218.7234 - val_val_accuracy: 0.8543\n","Epoch 16/50\n","11/11 [==============================] - ETA: 0s - loss: 179.6953 - accuracy: 0.8652\n","Epoch 00016: val_val_accuracy improved from 0.85428 to 0.85454, saving model to ./models/Bilstm_crf/best_model_bilstm3.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 179.6953 - accuracy: 0.8652 - val_loss_val: 216.0714 - val_val_accuracy: 0.8545\n","Epoch 17/50\n","11/11 [==============================] - ETA: 0s - loss: 175.6617 - accuracy: 0.8659\n","Epoch 00017: val_val_accuracy improved from 0.85454 to 0.85490, saving model to ./models/Bilstm_crf/best_model_bilstm3.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 175.6617 - accuracy: 0.8659 - val_loss_val: 213.6754 - val_val_accuracy: 0.8549\n","Epoch 18/50\n","11/11 [==============================] - ETA: 0s - loss: 171.9500 - accuracy: 0.8669\n","Epoch 00018: val_val_accuracy improved from 0.85490 to 0.85500, saving model to ./models/Bilstm_crf/best_model_bilstm3.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 171.9500 - accuracy: 0.8669 - val_loss_val: 211.5154 - val_val_accuracy: 0.8550\n","Epoch 19/50\n","11/11 [==============================] - ETA: 0s - loss: 168.2829 - accuracy: 0.8680\n","Epoch 00019: val_val_accuracy improved from 0.85500 to 0.85505, saving model to ./models/Bilstm_crf/best_model_bilstm3.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 168.2829 - accuracy: 0.8680 - val_loss_val: 209.4816 - val_val_accuracy: 0.8551\n","Epoch 20/50\n","11/11 [==============================] - ETA: 0s - loss: 164.7503 - accuracy: 0.8694\n","Epoch 00020: val_val_accuracy improved from 0.85505 to 0.85552, saving model to ./models/Bilstm_crf/best_model_bilstm3.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 164.7503 - accuracy: 0.8694 - val_loss_val: 207.8281 - val_val_accuracy: 0.8555\n","Epoch 21/50\n","11/11 [==============================] - ETA: 0s - loss: 161.2651 - accuracy: 0.8707\n","Epoch 00021: val_val_accuracy did not improve from 0.85552\n","11/11 [==============================] - 18s 2s/step - loss: 161.2651 - accuracy: 0.8707 - val_loss_val: 205.7540 - val_val_accuracy: 0.8551\n","Epoch 22/50\n","11/11 [==============================] - ETA: 0s - loss: 157.8397 - accuracy: 0.8722\n","Epoch 00022: val_val_accuracy improved from 0.85552 to 0.85557, saving model to ./models/Bilstm_crf/best_model_bilstm3.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 157.8397 - accuracy: 0.8722 - val_loss_val: 203.9187 - val_val_accuracy: 0.8556\n","Epoch 23/50\n","11/11 [==============================] - ETA: 0s - loss: 154.2894 - accuracy: 0.8734\n","Epoch 00023: val_val_accuracy improved from 0.85557 to 0.85562, saving model to ./models/Bilstm_crf/best_model_bilstm3.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 154.2894 - accuracy: 0.8734 - val_loss_val: 202.2564 - val_val_accuracy: 0.8556\n","Epoch 24/50\n","11/11 [==============================] - ETA: 0s - loss: 150.6281 - accuracy: 0.8752\n","Epoch 00024: val_val_accuracy did not improve from 0.85562\n","11/11 [==============================] - 18s 2s/step - loss: 150.6281 - accuracy: 0.8752 - val_loss_val: 200.4837 - val_val_accuracy: 0.8556\n","Epoch 25/50\n","11/11 [==============================] - ETA: 0s - loss: 146.7316 - accuracy: 0.8773\n","Epoch 00025: val_val_accuracy improved from 0.85562 to 0.85572, saving model to ./models/Bilstm_crf/best_model_bilstm3.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 146.7316 - accuracy: 0.8773 - val_loss_val: 199.0683 - val_val_accuracy: 0.8557\n","Epoch 26/50\n","11/11 [==============================] - ETA: 0s - loss: 142.7421 - accuracy: 0.8797\n","Epoch 00026: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 142.7421 - accuracy: 0.8797 - val_loss_val: 197.4565 - val_val_accuracy: 0.8553\n","Epoch 27/50\n","11/11 [==============================] - ETA: 0s - loss: 138.7747 - accuracy: 0.8836\n","Epoch 00027: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 138.7747 - accuracy: 0.8836 - val_loss_val: 195.8799 - val_val_accuracy: 0.8551\n","Epoch 28/50\n","11/11 [==============================] - ETA: 0s - loss: 134.5864 - accuracy: 0.8857\n","Epoch 00028: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 134.5864 - accuracy: 0.8857 - val_loss_val: 194.9100 - val_val_accuracy: 0.8552\n","Epoch 29/50\n","11/11 [==============================] - ETA: 0s - loss: 130.5572 - accuracy: 0.8887\n","Epoch 00029: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 130.5572 - accuracy: 0.8887 - val_loss_val: 193.2687 - val_val_accuracy: 0.8540\n","Epoch 30/50\n","11/11 [==============================] - ETA: 0s - loss: 126.5577 - accuracy: 0.8929\n","Epoch 00030: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 126.5577 - accuracy: 0.8929 - val_loss_val: 192.1401 - val_val_accuracy: 0.8547\n","Epoch 31/50\n","11/11 [==============================] - ETA: 0s - loss: 122.4981 - accuracy: 0.8959\n","Epoch 00031: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 122.4981 - accuracy: 0.8959 - val_loss_val: 191.0064 - val_val_accuracy: 0.8543\n","Epoch 32/50\n","11/11 [==============================] - ETA: 0s - loss: 118.7879 - accuracy: 0.9001\n","Epoch 00032: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 118.7879 - accuracy: 0.9001 - val_loss_val: 189.9629 - val_val_accuracy: 0.8548\n","Epoch 33/50\n","11/11 [==============================] - ETA: 0s - loss: 115.1863 - accuracy: 0.9043\n","Epoch 00033: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 115.1863 - accuracy: 0.9043 - val_loss_val: 189.6470 - val_val_accuracy: 0.8551\n","Epoch 34/50\n","11/11 [==============================] - ETA: 0s - loss: 111.5758 - accuracy: 0.9075\n","Epoch 00034: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 111.5758 - accuracy: 0.9075 - val_loss_val: 188.1612 - val_val_accuracy: 0.8551\n","Epoch 35/50\n","11/11 [==============================] - ETA: 0s - loss: 108.0923 - accuracy: 0.9118\n","Epoch 00035: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 108.0923 - accuracy: 0.9118 - val_loss_val: 188.3520 - val_val_accuracy: 0.8543\n","Epoch 36/50\n","11/11 [==============================] - ETA: 0s - loss: 104.8971 - accuracy: 0.9152\n","Epoch 00036: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 104.8971 - accuracy: 0.9152 - val_loss_val: 187.3990 - val_val_accuracy: 0.8533\n","Epoch 37/50\n","11/11 [==============================] - ETA: 0s - loss: 101.8638 - accuracy: 0.9177\n","Epoch 00037: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 101.8638 - accuracy: 0.9177 - val_loss_val: 186.7913 - val_val_accuracy: 0.8533\n","Epoch 38/50\n","11/11 [==============================] - ETA: 0s - loss: 98.6825 - accuracy: 0.9209\n","Epoch 00038: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 98.6825 - accuracy: 0.9209 - val_loss_val: 186.2752 - val_val_accuracy: 0.8525\n","Epoch 39/50\n","11/11 [==============================] - ETA: 0s - loss: 95.8951 - accuracy: 0.9232\n","Epoch 00039: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 95.8951 - accuracy: 0.9232 - val_loss_val: 185.8512 - val_val_accuracy: 0.8528\n","Epoch 40/50\n","11/11 [==============================] - ETA: 0s - loss: 93.0262 - accuracy: 0.9262\n","Epoch 00040: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 93.0262 - accuracy: 0.9262 - val_loss_val: 185.2719 - val_val_accuracy: 0.8529\n","Epoch 41/50\n","11/11 [==============================] - ETA: 0s - loss: 90.4176 - accuracy: 0.9278\n","Epoch 00041: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 90.4176 - accuracy: 0.9278 - val_loss_val: 184.7023 - val_val_accuracy: 0.8519\n","Epoch 42/50\n","11/11 [==============================] - ETA: 0s - loss: 87.7708 - accuracy: 0.9300\n","Epoch 00042: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 87.7708 - accuracy: 0.9300 - val_loss_val: 184.3493 - val_val_accuracy: 0.8530\n","Epoch 43/50\n","11/11 [==============================] - ETA: 0s - loss: 85.1647 - accuracy: 0.9321\n","Epoch 00043: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 85.1647 - accuracy: 0.9321 - val_loss_val: 184.2959 - val_val_accuracy: 0.8543\n","Epoch 44/50\n","11/11 [==============================] - ETA: 0s - loss: 82.8496 - accuracy: 0.9342\n","Epoch 00044: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 82.8496 - accuracy: 0.9342 - val_loss_val: 183.7374 - val_val_accuracy: 0.8546\n","Epoch 45/50\n","11/11 [==============================] - ETA: 0s - loss: 80.5893 - accuracy: 0.9353\n","Epoch 00045: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 80.5893 - accuracy: 0.9353 - val_loss_val: 184.2998 - val_val_accuracy: 0.8529\n","Epoch 46/50\n","11/11 [==============================] - ETA: 0s - loss: 78.2256 - accuracy: 0.9378\n","Epoch 00046: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 78.2256 - accuracy: 0.9378 - val_loss_val: 183.2210 - val_val_accuracy: 0.8518\n","Epoch 47/50\n","11/11 [==============================] - ETA: 0s - loss: 76.1089 - accuracy: 0.9393\n","Epoch 00047: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 76.1089 - accuracy: 0.9393 - val_loss_val: 182.8904 - val_val_accuracy: 0.8519\n","Epoch 48/50\n","11/11 [==============================] - ETA: 0s - loss: 74.0875 - accuracy: 0.9413\n","Epoch 00048: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 74.0875 - accuracy: 0.9413 - val_loss_val: 183.1541 - val_val_accuracy: 0.8539\n","Epoch 49/50\n","11/11 [==============================] - ETA: 0s - loss: 72.0150 - accuracy: 0.9427\n","Epoch 00049: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 72.0150 - accuracy: 0.9427 - val_loss_val: 184.1532 - val_val_accuracy: 0.8524\n","Epoch 50/50\n","11/11 [==============================] - ETA: 0s - loss: 69.9955 - accuracy: 0.9452\n","Epoch 00050: val_val_accuracy did not improve from 0.85572\n","11/11 [==============================] - 18s 2s/step - loss: 69.9955 - accuracy: 0.9452 - val_loss_val: 184.1506 - val_val_accuracy: 0.8534\n"]},{"output_type":"stream","name":"stderr","text":["\r4it [54:35, 851.35s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","11/11 [==============================] - ETA: 0s - loss: 743.4224 - accuracy: 0.5356\n","Epoch 00001: val_val_accuracy improved from -inf to 0.84830, saving model to ./models/Bilstm_crf/best_model_bilstm4.hdf5\n","11/11 [==============================] - 27s 2s/step - loss: 743.4224 - accuracy: 0.5356 - val_loss_val: 562.0449 - val_val_accuracy: 0.8483\n","Epoch 2/50\n","11/11 [==============================] - ETA: 0s - loss: 470.6226 - accuracy: 0.8608\n","Epoch 00002: val_val_accuracy improved from 0.84830 to 0.85423, saving model to ./models/Bilstm_crf/best_model_bilstm4.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 470.6226 - accuracy: 0.8608 - val_loss_val: 285.1451 - val_val_accuracy: 0.8542\n","Epoch 3/50\n","11/11 [==============================] - ETA: 0s - loss: 216.0299 - accuracy: 0.8630\n","Epoch 00003: val_val_accuracy did not improve from 0.85423\n","11/11 [==============================] - 18s 2s/step - loss: 216.0299 - accuracy: 0.8630 - val_loss_val: 224.8109 - val_val_accuracy: 0.8517\n","Epoch 4/50\n","11/11 [==============================] - ETA: 0s - loss: 188.0731 - accuracy: 0.8650\n","Epoch 00004: val_val_accuracy improved from 0.85423 to 0.85485, saving model to ./models/Bilstm_crf/best_model_bilstm4.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 188.0731 - accuracy: 0.8650 - val_loss_val: 207.9885 - val_val_accuracy: 0.8548\n","Epoch 5/50\n","11/11 [==============================] - ETA: 0s - loss: 177.1631 - accuracy: 0.8677\n","Epoch 00005: val_val_accuracy improved from 0.85485 to 0.85686, saving model to ./models/Bilstm_crf/best_model_bilstm4.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 177.1631 - accuracy: 0.8677 - val_loss_val: 199.9566 - val_val_accuracy: 0.8569\n","Epoch 6/50\n","11/11 [==============================] - ETA: 0s - loss: 168.4441 - accuracy: 0.8700\n","Epoch 00006: val_val_accuracy improved from 0.85686 to 0.85804, saving model to ./models/Bilstm_crf/best_model_bilstm4.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 168.4441 - accuracy: 0.8700 - val_loss_val: 191.5057 - val_val_accuracy: 0.8580\n","Epoch 7/50\n","11/11 [==============================] - ETA: 0s - loss: 160.3216 - accuracy: 0.8727\n","Epoch 00007: val_val_accuracy improved from 0.85804 to 0.86026, saving model to ./models/Bilstm_crf/best_model_bilstm4.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 160.3216 - accuracy: 0.8727 - val_loss_val: 185.5939 - val_val_accuracy: 0.8603\n","Epoch 8/50\n","11/11 [==============================] - ETA: 0s - loss: 152.8682 - accuracy: 0.8771\n","Epoch 00008: val_val_accuracy improved from 0.86026 to 0.86206, saving model to ./models/Bilstm_crf/best_model_bilstm4.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 152.8682 - accuracy: 0.8771 - val_loss_val: 178.0694 - val_val_accuracy: 0.8621\n","Epoch 9/50\n","11/11 [==============================] - ETA: 0s - loss: 145.8248 - accuracy: 0.8794\n","Epoch 00009: val_val_accuracy improved from 0.86206 to 0.86273, saving model to ./models/Bilstm_crf/best_model_bilstm4.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 145.8248 - accuracy: 0.8794 - val_loss_val: 173.9385 - val_val_accuracy: 0.8627\n","Epoch 10/50\n","11/11 [==============================] - ETA: 0s - loss: 138.7645 - accuracy: 0.8842\n","Epoch 00010: val_val_accuracy improved from 0.86273 to 0.86515, saving model to ./models/Bilstm_crf/best_model_bilstm4.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 138.7645 - accuracy: 0.8842 - val_loss_val: 169.0667 - val_val_accuracy: 0.8652\n","Epoch 11/50\n","11/11 [==============================] - ETA: 0s - loss: 131.5433 - accuracy: 0.8886\n","Epoch 00011: val_val_accuracy improved from 0.86515 to 0.86552, saving model to ./models/Bilstm_crf/best_model_bilstm4.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 131.5433 - accuracy: 0.8886 - val_loss_val: 165.6495 - val_val_accuracy: 0.8655\n","Epoch 12/50\n","11/11 [==============================] - ETA: 0s - loss: 124.3978 - accuracy: 0.8942\n","Epoch 00012: val_val_accuracy improved from 0.86552 to 0.86577, saving model to ./models/Bilstm_crf/best_model_bilstm4.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 124.3978 - accuracy: 0.8942 - val_loss_val: 162.2051 - val_val_accuracy: 0.8658\n","Epoch 13/50\n","11/11 [==============================] - ETA: 0s - loss: 117.2854 - accuracy: 0.8991\n","Epoch 00013: val_val_accuracy improved from 0.86577 to 0.86691, saving model to ./models/Bilstm_crf/best_model_bilstm4.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 117.2854 - accuracy: 0.8991 - val_loss_val: 160.2056 - val_val_accuracy: 0.8669\n","Epoch 14/50\n","11/11 [==============================] - ETA: 0s - loss: 110.3157 - accuracy: 0.9036\n","Epoch 00014: val_val_accuracy improved from 0.86691 to 0.86825, saving model to ./models/Bilstm_crf/best_model_bilstm4.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 110.3157 - accuracy: 0.9036 - val_loss_val: 157.1483 - val_val_accuracy: 0.8682\n","Epoch 15/50\n","11/11 [==============================] - ETA: 0s - loss: 102.8738 - accuracy: 0.9104\n","Epoch 00015: val_val_accuracy did not improve from 0.86825\n","11/11 [==============================] - 18s 2s/step - loss: 102.8738 - accuracy: 0.9104 - val_loss_val: 156.7975 - val_val_accuracy: 0.8673\n","Epoch 16/50\n","11/11 [==============================] - ETA: 0s - loss: 96.4286 - accuracy: 0.9162\n","Epoch 00016: val_val_accuracy did not improve from 0.86825\n","11/11 [==============================] - 18s 2s/step - loss: 96.4286 - accuracy: 0.9162 - val_loss_val: 157.4594 - val_val_accuracy: 0.8669\n","Epoch 17/50\n","11/11 [==============================] - ETA: 0s - loss: 90.0859 - accuracy: 0.9219\n","Epoch 00017: val_val_accuracy did not improve from 0.86825\n","11/11 [==============================] - 18s 2s/step - loss: 90.0859 - accuracy: 0.9219 - val_loss_val: 155.8476 - val_val_accuracy: 0.8671\n","Epoch 18/50\n","11/11 [==============================] - ETA: 0s - loss: 84.2307 - accuracy: 0.9273\n","Epoch 00018: val_val_accuracy did not improve from 0.86825\n","11/11 [==============================] - 18s 2s/step - loss: 84.2307 - accuracy: 0.9273 - val_loss_val: 158.5376 - val_val_accuracy: 0.8657\n","Epoch 19/50\n","11/11 [==============================] - ETA: 0s - loss: 78.0186 - accuracy: 0.9322\n","Epoch 00019: val_val_accuracy did not improve from 0.86825\n","11/11 [==============================] - 18s 2s/step - loss: 78.0186 - accuracy: 0.9322 - val_loss_val: 155.8327 - val_val_accuracy: 0.8669\n","Epoch 20/50\n","11/11 [==============================] - ETA: 0s - loss: 72.5234 - accuracy: 0.9378\n","Epoch 00020: val_val_accuracy did not improve from 0.86825\n","11/11 [==============================] - 18s 2s/step - loss: 72.5234 - accuracy: 0.9378 - val_loss_val: 158.8300 - val_val_accuracy: 0.8626\n","Epoch 21/50\n","11/11 [==============================] - ETA: 0s - loss: 67.8043 - accuracy: 0.9415\n","Epoch 00021: val_val_accuracy did not improve from 0.86825\n","11/11 [==============================] - 18s 2s/step - loss: 67.8043 - accuracy: 0.9415 - val_loss_val: 158.5603 - val_val_accuracy: 0.8657\n","Epoch 22/50\n","11/11 [==============================] - ETA: 0s - loss: 63.1150 - accuracy: 0.9461\n","Epoch 00022: val_val_accuracy did not improve from 0.86825\n","11/11 [==============================] - 18s 2s/step - loss: 63.1150 - accuracy: 0.9461 - val_loss_val: 161.6916 - val_val_accuracy: 0.8632\n"]},{"output_type":"stream","name":"stderr","text":["\r5it [1:01:42, 698.37s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","11/11 [==============================] - ETA: 0s - loss: 869.6631 - accuracy: 0.4574\n","Epoch 00001: val_val_accuracy improved from -inf to 0.83598, saving model to ./models/Bilstm_crf/best_model_bilstm5.hdf5\n","11/11 [==============================] - 27s 2s/step - loss: 869.6631 - accuracy: 0.4574 - val_loss_val: 624.7947 - val_val_accuracy: 0.8360\n","Epoch 2/50\n","11/11 [==============================] - ETA: 0s - loss: 413.4493 - accuracy: 0.8498\n","Epoch 00002: val_val_accuracy improved from 0.83598 to 0.84129, saving model to ./models/Bilstm_crf/best_model_bilstm5.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 413.4493 - accuracy: 0.8498 - val_loss_val: 368.8108 - val_val_accuracy: 0.8413\n","Epoch 3/50\n","11/11 [==============================] - ETA: 0s - loss: 253.4279 - accuracy: 0.8569\n","Epoch 00003: val_val_accuracy improved from 0.84129 to 0.85072, saving model to ./models/Bilstm_crf/best_model_bilstm5.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 253.4279 - accuracy: 0.8569 - val_loss_val: 242.6870 - val_val_accuracy: 0.8507\n","Epoch 4/50\n","11/11 [==============================] - ETA: 0s - loss: 205.0997 - accuracy: 0.8641\n","Epoch 00004: val_val_accuracy improved from 0.85072 to 0.85448, saving model to ./models/Bilstm_crf/best_model_bilstm5.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 205.0997 - accuracy: 0.8641 - val_loss_val: 230.2374 - val_val_accuracy: 0.8545\n","Epoch 5/50\n","11/11 [==============================] - ETA: 0s - loss: 187.0229 - accuracy: 0.8647\n","Epoch 00005: val_val_accuracy improved from 0.85448 to 0.85459, saving model to ./models/Bilstm_crf/best_model_bilstm5.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 187.0229 - accuracy: 0.8647 - val_loss_val: 210.9201 - val_val_accuracy: 0.8546\n","Epoch 6/50\n","11/11 [==============================] - ETA: 0s - loss: 172.9125 - accuracy: 0.8654\n","Epoch 00006: val_val_accuracy improved from 0.85459 to 0.85469, saving model to ./models/Bilstm_crf/best_model_bilstm5.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 172.9125 - accuracy: 0.8654 - val_loss_val: 204.3195 - val_val_accuracy: 0.8547\n","Epoch 7/50\n","11/11 [==============================] - ETA: 0s - loss: 162.0789 - accuracy: 0.8674\n","Epoch 00007: val_val_accuracy improved from 0.85469 to 0.85515, saving model to ./models/Bilstm_crf/best_model_bilstm5.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 162.0789 - accuracy: 0.8674 - val_loss_val: 196.5636 - val_val_accuracy: 0.8552\n","Epoch 8/50\n","11/11 [==============================] - ETA: 0s - loss: 152.6302 - accuracy: 0.8751\n","Epoch 00008: val_val_accuracy improved from 0.85515 to 0.85577, saving model to ./models/Bilstm_crf/best_model_bilstm5.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 152.6302 - accuracy: 0.8751 - val_loss_val: 192.3775 - val_val_accuracy: 0.8558\n","Epoch 9/50\n","11/11 [==============================] - ETA: 0s - loss: 143.8112 - accuracy: 0.8818\n","Epoch 00009: val_val_accuracy did not improve from 0.85577\n","11/11 [==============================] - 18s 2s/step - loss: 143.8112 - accuracy: 0.8818 - val_loss_val: 190.5510 - val_val_accuracy: 0.8539\n","Epoch 10/50\n","11/11 [==============================] - ETA: 0s - loss: 134.6599 - accuracy: 0.8894\n","Epoch 00010: val_val_accuracy did not improve from 0.85577\n","11/11 [==============================] - 18s 2s/step - loss: 134.6599 - accuracy: 0.8894 - val_loss_val: 188.4390 - val_val_accuracy: 0.8535\n","Epoch 11/50\n","11/11 [==============================] - ETA: 0s - loss: 124.9362 - accuracy: 0.8979\n","Epoch 00011: val_val_accuracy did not improve from 0.85577\n","11/11 [==============================] - 18s 2s/step - loss: 124.9362 - accuracy: 0.8979 - val_loss_val: 188.7616 - val_val_accuracy: 0.8529\n","Epoch 12/50\n","11/11 [==============================] - ETA: 0s - loss: 114.8958 - accuracy: 0.9068\n","Epoch 00012: val_val_accuracy did not improve from 0.85577\n","11/11 [==============================] - 18s 2s/step - loss: 114.8958 - accuracy: 0.9068 - val_loss_val: 188.8639 - val_val_accuracy: 0.8514\n","Epoch 13/50\n","11/11 [==============================] - ETA: 0s - loss: 105.0866 - accuracy: 0.9148\n","Epoch 00013: val_val_accuracy did not improve from 0.85577\n","11/11 [==============================] - 18s 2s/step - loss: 105.0866 - accuracy: 0.9148 - val_loss_val: 188.3698 - val_val_accuracy: 0.8517\n","Epoch 14/50\n","11/11 [==============================] - ETA: 0s - loss: 95.5014 - accuracy: 0.9222\n","Epoch 00014: val_val_accuracy did not improve from 0.85577\n","11/11 [==============================] - 18s 2s/step - loss: 95.5014 - accuracy: 0.9222 - val_loss_val: 191.1317 - val_val_accuracy: 0.8493\n","Epoch 15/50\n","11/11 [==============================] - ETA: 0s - loss: 86.5684 - accuracy: 0.9310\n","Epoch 00015: val_val_accuracy did not improve from 0.85577\n","11/11 [==============================] - 18s 2s/step - loss: 86.5684 - accuracy: 0.9310 - val_loss_val: 192.2762 - val_val_accuracy: 0.8519\n","Epoch 16/50\n","11/11 [==============================] - ETA: 0s - loss: 78.4896 - accuracy: 0.9384\n","Epoch 00016: val_val_accuracy did not improve from 0.85577\n","11/11 [==============================] - 18s 2s/step - loss: 78.4896 - accuracy: 0.9384 - val_loss_val: 196.6446 - val_val_accuracy: 0.8467\n"]},{"output_type":"stream","name":"stderr","text":["\r6it [1:07:19, 575.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","11/11 [==============================] - ETA: 0s - loss: 795.4254 - accuracy: 0.3729\n","Epoch 00001: val_val_accuracy improved from -inf to 0.83649, saving model to ./models/Bilstm_crf/best_model_bilstm6.hdf5\n","11/11 [==============================] - 28s 2s/step - loss: 795.4254 - accuracy: 0.3729 - val_loss_val: 551.2900 - val_val_accuracy: 0.8365\n","Epoch 2/50\n","11/11 [==============================] - ETA: 0s - loss: 397.7016 - accuracy: 0.8570\n","Epoch 00002: val_val_accuracy improved from 0.83649 to 0.85072, saving model to ./models/Bilstm_crf/best_model_bilstm6.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 397.7016 - accuracy: 0.8570 - val_loss_val: 259.1644 - val_val_accuracy: 0.8507\n","Epoch 3/50\n","11/11 [==============================] - ETA: 0s - loss: 224.5286 - accuracy: 0.8580\n","Epoch 00003: val_val_accuracy did not improve from 0.85072\n","11/11 [==============================] - 18s 2s/step - loss: 224.5286 - accuracy: 0.8580 - val_loss_val: 246.6926 - val_val_accuracy: 0.8475\n","Epoch 4/50\n","11/11 [==============================] - ETA: 0s - loss: 209.0459 - accuracy: 0.8622\n","Epoch 00004: val_val_accuracy improved from 0.85072 to 0.85402, saving model to ./models/Bilstm_crf/best_model_bilstm6.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 209.0459 - accuracy: 0.8622 - val_loss_val: 229.0817 - val_val_accuracy: 0.8540\n","Epoch 5/50\n","11/11 [==============================] - ETA: 0s - loss: 198.8728 - accuracy: 0.8648\n","Epoch 00005: val_val_accuracy improved from 0.85402 to 0.85438, saving model to ./models/Bilstm_crf/best_model_bilstm6.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 198.8728 - accuracy: 0.8648 - val_loss_val: 220.2141 - val_val_accuracy: 0.8544\n","Epoch 6/50\n","11/11 [==============================] - ETA: 0s - loss: 190.2677 - accuracy: 0.8654\n","Epoch 00006: val_val_accuracy improved from 0.85438 to 0.85515, saving model to ./models/Bilstm_crf/best_model_bilstm6.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 190.2677 - accuracy: 0.8654 - val_loss_val: 212.9374 - val_val_accuracy: 0.8552\n","Epoch 7/50\n","11/11 [==============================] - ETA: 0s - loss: 182.1038 - accuracy: 0.8671\n","Epoch 00007: val_val_accuracy improved from 0.85515 to 0.85649, saving model to ./models/Bilstm_crf/best_model_bilstm6.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 182.1038 - accuracy: 0.8671 - val_loss_val: 205.7927 - val_val_accuracy: 0.8565\n","Epoch 8/50\n","11/11 [==============================] - ETA: 0s - loss: 174.8570 - accuracy: 0.8701\n","Epoch 00008: val_val_accuracy improved from 0.85649 to 0.85887, saving model to ./models/Bilstm_crf/best_model_bilstm6.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 174.8570 - accuracy: 0.8701 - val_loss_val: 198.9313 - val_val_accuracy: 0.8589\n","Epoch 9/50\n","11/11 [==============================] - ETA: 0s - loss: 167.6190 - accuracy: 0.8735\n","Epoch 00009: val_val_accuracy improved from 0.85887 to 0.86005, saving model to ./models/Bilstm_crf/best_model_bilstm6.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 167.6190 - accuracy: 0.8735 - val_loss_val: 193.4623 - val_val_accuracy: 0.8601\n","Epoch 10/50\n","11/11 [==============================] - ETA: 0s - loss: 160.3369 - accuracy: 0.8759\n","Epoch 00010: val_val_accuracy improved from 0.86005 to 0.86284, saving model to ./models/Bilstm_crf/best_model_bilstm6.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 160.3369 - accuracy: 0.8759 - val_loss_val: 188.5827 - val_val_accuracy: 0.8628\n","Epoch 11/50\n","11/11 [==============================] - ETA: 0s - loss: 152.9917 - accuracy: 0.8796\n","Epoch 00011: val_val_accuracy improved from 0.86284 to 0.86335, saving model to ./models/Bilstm_crf/best_model_bilstm6.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 152.9917 - accuracy: 0.8796 - val_loss_val: 182.5237 - val_val_accuracy: 0.8634\n","Epoch 12/50\n","11/11 [==============================] - ETA: 0s - loss: 145.0149 - accuracy: 0.8834\n","Epoch 00012: val_val_accuracy improved from 0.86335 to 0.86366, saving model to ./models/Bilstm_crf/best_model_bilstm6.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 145.0149 - accuracy: 0.8834 - val_loss_val: 177.2776 - val_val_accuracy: 0.8637\n","Epoch 13/50\n","11/11 [==============================] - ETA: 0s - loss: 137.0461 - accuracy: 0.8882\n","Epoch 00013: val_val_accuracy did not improve from 0.86366\n","11/11 [==============================] - 18s 2s/step - loss: 137.0461 - accuracy: 0.8882 - val_loss_val: 174.9149 - val_val_accuracy: 0.8611\n","Epoch 14/50\n","11/11 [==============================] - ETA: 0s - loss: 129.8146 - accuracy: 0.8937\n","Epoch 00014: val_val_accuracy improved from 0.86366 to 0.86376, saving model to ./models/Bilstm_crf/best_model_bilstm6.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 129.8146 - accuracy: 0.8937 - val_loss_val: 170.5806 - val_val_accuracy: 0.8638\n","Epoch 15/50\n","11/11 [==============================] - ETA: 0s - loss: 121.7186 - accuracy: 0.8983\n","Epoch 00015: val_val_accuracy improved from 0.86376 to 0.86464, saving model to ./models/Bilstm_crf/best_model_bilstm6.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 121.7186 - accuracy: 0.8983 - val_loss_val: 168.3234 - val_val_accuracy: 0.8646\n","Epoch 16/50\n","11/11 [==============================] - ETA: 0s - loss: 113.7512 - accuracy: 0.9046\n","Epoch 00016: val_val_accuracy did not improve from 0.86464\n","11/11 [==============================] - 18s 2s/step - loss: 113.7512 - accuracy: 0.9046 - val_loss_val: 167.9857 - val_val_accuracy: 0.8630\n","Epoch 17/50\n","11/11 [==============================] - ETA: 0s - loss: 105.3593 - accuracy: 0.9123\n","Epoch 00017: val_val_accuracy did not improve from 0.86464\n","11/11 [==============================] - 18s 2s/step - loss: 105.3593 - accuracy: 0.9123 - val_loss_val: 167.0740 - val_val_accuracy: 0.8641\n","Epoch 18/50\n","11/11 [==============================] - ETA: 0s - loss: 98.7250 - accuracy: 0.9168\n","Epoch 00018: val_val_accuracy did not improve from 0.86464\n","11/11 [==============================] - 18s 2s/step - loss: 98.7250 - accuracy: 0.9168 - val_loss_val: 165.8655 - val_val_accuracy: 0.8625\n","Epoch 19/50\n","11/11 [==============================] - ETA: 0s - loss: 92.0058 - accuracy: 0.9229\n","Epoch 00019: val_val_accuracy did not improve from 0.86464\n","11/11 [==============================] - 18s 2s/step - loss: 92.0058 - accuracy: 0.9229 - val_loss_val: 166.8245 - val_val_accuracy: 0.8637\n","Epoch 20/50\n","11/11 [==============================] - ETA: 0s - loss: 85.6248 - accuracy: 0.9278\n","Epoch 00020: val_val_accuracy did not improve from 0.86464\n","11/11 [==============================] - 18s 2s/step - loss: 85.6248 - accuracy: 0.9278 - val_loss_val: 167.9448 - val_val_accuracy: 0.8592\n","Epoch 21/50\n","11/11 [==============================] - ETA: 0s - loss: 79.6531 - accuracy: 0.9332\n","Epoch 00021: val_val_accuracy improved from 0.86464 to 0.86500, saving model to ./models/Bilstm_crf/best_model_bilstm6.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 79.6531 - accuracy: 0.9332 - val_loss_val: 168.5295 - val_val_accuracy: 0.8650\n"]},{"output_type":"stream","name":"stderr","text":["\r7it [1:14:23, 525.80s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","11/11 [==============================] - ETA: 0s - loss: 823.4320 - accuracy: 0.4440\n","Epoch 00001: val_val_accuracy improved from -inf to 0.53670, saving model to ./models/Bilstm_crf/best_model_bilstm7.hdf5\n","11/11 [==============================] - 27s 2s/step - loss: 823.4320 - accuracy: 0.4440 - val_loss_val: 621.2227 - val_val_accuracy: 0.5367\n","Epoch 2/50\n","11/11 [==============================] - ETA: 0s - loss: 457.6543 - accuracy: 0.6558\n","Epoch 00002: val_val_accuracy improved from 0.53670 to 0.76371, saving model to ./models/Bilstm_crf/best_model_bilstm7.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 457.6543 - accuracy: 0.6558 - val_loss_val: 417.7347 - val_val_accuracy: 0.7637\n","Epoch 3/50\n","11/11 [==============================] - ETA: 0s - loss: 288.0335 - accuracy: 0.8502\n","Epoch 00003: val_val_accuracy improved from 0.76371 to 0.85330, saving model to ./models/Bilstm_crf/best_model_bilstm7.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 288.0335 - accuracy: 0.8502 - val_loss_val: 287.4465 - val_val_accuracy: 0.8533\n","Epoch 4/50\n","11/11 [==============================] - ETA: 0s - loss: 243.9925 - accuracy: 0.8638\n","Epoch 00004: val_val_accuracy improved from 0.85330 to 0.85433, saving model to ./models/Bilstm_crf/best_model_bilstm7.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 243.9925 - accuracy: 0.8638 - val_loss_val: 264.7958 - val_val_accuracy: 0.8543\n","Epoch 5/50\n","11/11 [==============================] - ETA: 0s - loss: 226.7943 - accuracy: 0.8643\n","Epoch 00005: val_val_accuracy did not improve from 0.85433\n","11/11 [==============================] - 18s 2s/step - loss: 226.7943 - accuracy: 0.8643 - val_loss_val: 251.0077 - val_val_accuracy: 0.8536\n","Epoch 6/50\n","11/11 [==============================] - ETA: 0s - loss: 216.9130 - accuracy: 0.8642\n","Epoch 00006: val_val_accuracy did not improve from 0.85433\n","11/11 [==============================] - 18s 2s/step - loss: 216.9130 - accuracy: 0.8642 - val_loss_val: 245.3092 - val_val_accuracy: 0.8539\n","Epoch 7/50\n","11/11 [==============================] - ETA: 0s - loss: 209.0712 - accuracy: 0.8646\n","Epoch 00007: val_val_accuracy improved from 0.85433 to 0.85443, saving model to ./models/Bilstm_crf/best_model_bilstm7.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 209.0712 - accuracy: 0.8646 - val_loss_val: 237.7454 - val_val_accuracy: 0.8544\n","Epoch 8/50\n","11/11 [==============================] - ETA: 0s - loss: 200.9320 - accuracy: 0.8647\n","Epoch 00008: val_val_accuracy improved from 0.85443 to 0.85448, saving model to ./models/Bilstm_crf/best_model_bilstm7.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 200.9320 - accuracy: 0.8647 - val_loss_val: 230.1772 - val_val_accuracy: 0.8545\n","Epoch 9/50\n","11/11 [==============================] - ETA: 0s - loss: 192.6641 - accuracy: 0.8648\n","Epoch 00009: val_val_accuracy improved from 0.85448 to 0.85454, saving model to ./models/Bilstm_crf/best_model_bilstm7.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 192.6641 - accuracy: 0.8648 - val_loss_val: 222.7895 - val_val_accuracy: 0.8545\n","Epoch 10/50\n","11/11 [==============================] - ETA: 0s - loss: 184.0183 - accuracy: 0.8648\n","Epoch 00010: val_val_accuracy improved from 0.85454 to 0.85459, saving model to ./models/Bilstm_crf/best_model_bilstm7.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 184.0183 - accuracy: 0.8648 - val_loss_val: 216.1654 - val_val_accuracy: 0.8546\n","Epoch 11/50\n","11/11 [==============================] - ETA: 0s - loss: 175.4963 - accuracy: 0.8653\n","Epoch 00011: val_val_accuracy did not improve from 0.85459\n","11/11 [==============================] - 18s 2s/step - loss: 175.4963 - accuracy: 0.8653 - val_loss_val: 209.9115 - val_val_accuracy: 0.8546\n","Epoch 12/50\n","11/11 [==============================] - ETA: 0s - loss: 167.1657 - accuracy: 0.8663\n","Epoch 00012: val_val_accuracy improved from 0.85459 to 0.85552, saving model to ./models/Bilstm_crf/best_model_bilstm7.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 167.1657 - accuracy: 0.8663 - val_loss_val: 204.8066 - val_val_accuracy: 0.8555\n","Epoch 13/50\n","11/11 [==============================] - ETA: 0s - loss: 158.9868 - accuracy: 0.8689\n","Epoch 00013: val_val_accuracy improved from 0.85552 to 0.85603, saving model to ./models/Bilstm_crf/best_model_bilstm7.hdf5\n","11/11 [==============================] - 19s 2s/step - loss: 158.9868 - accuracy: 0.8689 - val_loss_val: 200.5554 - val_val_accuracy: 0.8560\n","Epoch 14/50\n","11/11 [==============================] - ETA: 0s - loss: 150.9649 - accuracy: 0.8735\n","Epoch 00014: val_val_accuracy improved from 0.85603 to 0.85716, saving model to ./models/Bilstm_crf/best_model_bilstm7.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 150.9649 - accuracy: 0.8735 - val_loss_val: 197.4810 - val_val_accuracy: 0.8572\n","Epoch 15/50\n","11/11 [==============================] - ETA: 0s - loss: 142.8816 - accuracy: 0.8788\n","Epoch 00015: val_val_accuracy did not improve from 0.85716\n","11/11 [==============================] - 18s 2s/step - loss: 142.8816 - accuracy: 0.8788 - val_loss_val: 194.6909 - val_val_accuracy: 0.8566\n","Epoch 16/50\n","11/11 [==============================] - ETA: 0s - loss: 134.5408 - accuracy: 0.8839\n","Epoch 00016: val_val_accuracy did not improve from 0.85716\n","11/11 [==============================] - 18s 2s/step - loss: 134.5408 - accuracy: 0.8839 - val_loss_val: 192.7493 - val_val_accuracy: 0.8560\n","Epoch 17/50\n","11/11 [==============================] - ETA: 0s - loss: 126.0100 - accuracy: 0.8910\n","Epoch 00017: val_val_accuracy did not improve from 0.85716\n","11/11 [==============================] - 18s 2s/step - loss: 126.0100 - accuracy: 0.8910 - val_loss_val: 192.7042 - val_val_accuracy: 0.8554\n","Epoch 18/50\n","11/11 [==============================] - ETA: 0s - loss: 117.3941 - accuracy: 0.8986\n","Epoch 00018: val_val_accuracy did not improve from 0.85716\n","11/11 [==============================] - 18s 2s/step - loss: 117.3941 - accuracy: 0.8986 - val_loss_val: 194.0066 - val_val_accuracy: 0.8539\n","Epoch 19/50\n","11/11 [==============================] - ETA: 0s - loss: 108.7834 - accuracy: 0.9065\n","Epoch 00019: val_val_accuracy did not improve from 0.85716\n","11/11 [==============================] - 18s 2s/step - loss: 108.7834 - accuracy: 0.9065 - val_loss_val: 194.2857 - val_val_accuracy: 0.8550\n","Epoch 20/50\n","11/11 [==============================] - ETA: 0s - loss: 100.6973 - accuracy: 0.9158\n","Epoch 00020: val_val_accuracy improved from 0.85716 to 0.85722, saving model to ./models/Bilstm_crf/best_model_bilstm7.hdf5\n","11/11 [==============================] - 18s 2s/step - loss: 100.6973 - accuracy: 0.9158 - val_loss_val: 205.0542 - val_val_accuracy: 0.8572\n"]},{"output_type":"stream","name":"stderr","text":["8it [1:20:37, 604.69s/it]\n"]}]},{"cell_type":"markdown","metadata":{"id":"eSrGXfXnf-qk"},"source":["##### Evaluation Grid search"],"id":"eSrGXfXnf-qk"},{"cell_type":"code","metadata":{"id":"Qh5DEbg3GkSC"},"source":["val_true=np.argmax(val_tags_y_B, axis=-1)"],"id":"Qh5DEbg3GkSC","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GEUIDaU0LJ09","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638732997803,"user_tz":300,"elapsed":7729,"user":{"displayName":"lamia2 salhi2","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14440555040870778846"}},"outputId":"f8720d4f-10d1-44b0-d305-ac6fdde68f0a"},"source":["#We evaluate the different model based on their best perfomance\n","\n","grid_search_results = pd.DataFrame(columns=['model', 'f1 score micro','f1 score macro'])\n","grid_search_results['model'] = models_bilstm_crf_param\n","\n","f1_scores_micro=[]\n","f1_scores_macro=[]\n","\n","f1_score_selected=0\n","#model_selected=build_model()\n","for i in range(len(models_bilstm_crf)):\n","  # Restore the weights\n","  model_i = models_bilstm_crf[i]\n","  model_i.load_weights(\"./models/Bilstm_crf/best_model_bilstm\"+str(i)+\".hdf5\")\n","  val_pred_ner_i = model_i.predict(val_sentences_X_B)\n","  \n","  \n","  f1_score_micro = f1_score(val_true.flatten(), val_pred_ner_i.flatten(), average='micro')\n","  f1_score_macro = f1_score(val_true.flatten(), val_pred_ner_i.flatten(), average='macro')\n","  f1_scores_micro.append(f1_score_micro)\n","  f1_scores_macro.append(f1_score_macro)\n","  if f1_score_macro >= f1_score_selected:\n","    \n","    model_selected = model_i\n","    f1_score_selected = f1_score_macro\n","    print(model_selected)\n"],"id":"GEUIDaU0LJ09","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<tf2crf.model_wrapper.ModelWithCRFLossDSCLoss object at 0x7f17fc77b390>\n","<tf2crf.model_wrapper.ModelWithCRFLossDSCLoss object at 0x7f17a2acdf50>\n","<tf2crf.model_wrapper.ModelWithCRFLossDSCLoss object at 0x7f17d09a4310>\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"iQYrNWOIbys8","executionInfo":{"status":"ok","timestamp":1638732997807,"user_tz":300,"elapsed":28,"user":{"displayName":"lamia2 salhi2","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14440555040870778846"}},"outputId":"b3f1026d-302d-41f1-ae48-bc9b7e1a61c7"},"source":["grid_search_results['f1 score micro'] = f1_scores_micro\n","grid_search_results['f1 score macro'] = f1_scores_macro\n","grid_search_results"],"id":"iQYrNWOIbys8","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>f1 score micro</th>\n","      <th>f1 score macro</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[adam, Glove, GRU, 0.3, 10]</td>\n","      <td>0.862887</td>\n","      <td>0.277990</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[adam, None, GRU, 0.3, 10]</td>\n","      <td>0.856804</td>\n","      <td>0.250600</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[adam, Glove, LSTM, 0.3, 10]</td>\n","      <td>0.860412</td>\n","      <td>0.221649</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[adam, None, LSTM, 0.3, 10]</td>\n","      <td>0.855567</td>\n","      <td>0.155894</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[adam, Glove, GRU, 0.3, 64]</td>\n","      <td>0.870000</td>\n","      <td>0.326773</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>[adam, None, GRU, 0.3, 64]</td>\n","      <td>0.855361</td>\n","      <td>0.156574</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>[adam, Glove, LSTM, 0.3, 64]</td>\n","      <td>0.865619</td>\n","      <td>0.377952</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>[adam, None, LSTM, 0.3, 64]</td>\n","      <td>0.857113</td>\n","      <td>0.209945</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                          model  f1 score micro  f1 score macro\n","0   [adam, Glove, GRU, 0.3, 10]        0.862887        0.277990\n","1    [adam, None, GRU, 0.3, 10]        0.856804        0.250600\n","2  [adam, Glove, LSTM, 0.3, 10]        0.860412        0.221649\n","3   [adam, None, LSTM, 0.3, 10]        0.855567        0.155894\n","4   [adam, Glove, GRU, 0.3, 64]        0.870000        0.326773\n","5    [adam, None, GRU, 0.3, 64]        0.855361        0.156574\n","6  [adam, Glove, LSTM, 0.3, 64]        0.865619        0.377952\n","7   [adam, None, LSTM, 0.3, 64]        0.857113        0.209945"]},"metadata":{},"execution_count":83}]},{"cell_type":"markdown","metadata":{"id":"5oJrme4a2WZ1"},"source":["Tout d'abord on remarque que comparé à la tache A, les score en f1 macro sont plus faible. Cependendant on remarque également que ce sont les mêmes paramétres qui donne les meilleurs résultats c'est à dire: [adam, Glove, GRU, 0.3, 64] avec 0.87 de f1 score sur les données de validations. En soumissioin kaggle nous avons obtenu 73 % environ. Ces résultats ne sont pas satisfaisants car il ne dépasse pas un modèle simple qui ne prédirait que des 'O'."],"id":"5oJrme4a2WZ1"},{"cell_type":"markdown","metadata":{"id":"D3wHrhStSQ5p"},"source":["#### Evaluation du modèle (confusion matrix et autre metrics)"],"id":"D3wHrhStSQ5p"},{"cell_type":"code","metadata":{"id":"3L8rxWB6Ip8c"},"source":["# to not do the grid search again\n","model_selected = models_bilstm_crf[6]\n","callback2 = EarlyStopping(monitor='val_loss_val', patience=3)\n","checkpoint2 = ModelCheckpoint(\"./models/Bilstm_crf/best_model_bilstm_select.hdf5\", monitor='val_val_accuracy', verbose=1,save_best_only=True, mode='auto', save_weights_only=True)\n","model_selected.fit(train_sentences_X_B, train_tags_y_B, batch_size=32, epochs=50, validation_data=(val_sentences_X_B,val_tags_y_B),callbacks=[checkpoint2,callback2],verbose=1)"],"id":"3L8rxWB6Ip8c","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HzlWT1cMh9vd"},"source":["def show_confusion_bis(true, pred,labels):\n","  print(f\"f1 micro:{f1_score(true, pred, average='micro'):.3f}\")\n","  print(f\"f1 macro:{f1_score(true, pred, average='macro'):.3f}\")\n","  print(f\"recall macro:{recall_score(true, pred, average='macro'):.3f}\")\n","  print(\"Confusion Matrix:\")\n","  print(confusion_matrix(true, pred,labels=labels))"],"id":"HzlWT1cMh9vd","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bXapP7sYiBOO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638746305056,"user_tz":300,"elapsed":3534,"user":{"displayName":"lamia2 salhi2","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14440555040870778846"}},"outputId":"68bbc969-d7e7-4ba8-ff0a-f7d3b15f0cfd"},"source":["val_pred_ner = model_selected.predict(val_sentences_X_B)\n","labels_for_confusion_m= range(len(tags_B))\n","show_confusion_bis(val_true.flatten(), val_pred_ner.flatten(),labels=labels_for_confusion_m)"],"id":"bXapP7sYiBOO","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["f1 micro:0.865\n","f1 macro:0.371\n","recall macro:0.333\n","Confusion Matrix:\n","[[8239    0    0    0    0    0    0    0    0    0    0    0    0    0]\n"," [   0   89   10    0   31    0    3    0   96    0    1    4    0    3]\n"," [   0    7   28    0   17    1    2    0   61    0    2    8    0    0]\n"," [   0    0    0    0    1    0    0    0    8    0    0    0    0    0]\n"," [   0   11   17    0  145    2    0    0  142    2    5    5    0    2]\n"," [   0    1    1    0    2    9    2    1   63    5    1    1    2    0]\n"," [   4   11    2    0    4    0   48    1  158    8    6   10   11    1]\n"," [   0    1    0    0    0    0    1   20   76   14    3   10    1    0]\n"," [   3   38   25    0   90    0    6   16 7780   81  128  145   22    6]\n"," [   0    0    2    0    2    0    0    5  168  108   21   17    7    1]\n"," [   0    5    3    0   18    0    2    2  262   19   98   32    8    0]\n"," [   0    2   12    0    9    0    1    6  253   10   21  130    5    0]\n"," [   0    1    0    0    1    0    7    0  106   26   15   12   66    3]\n"," [   0   10    1    0    4    0    5    1   81    3   20   10   14   25]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"wzaiOiR23Aac"},"source":["***Remarque***\n","\n","on peut voir que les 'O' sont les tags les plus prédits et menant aux plus de faux-positifs du fait du déséquiliibre de nos données."],"id":"wzaiOiR23Aac"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ho-RDd8_PMs","executionInfo":{"status":"ok","timestamp":1638764625505,"user_tz":300,"elapsed":177,"user":{"displayName":"lamia2 salhi2","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14440555040870778846"}},"outputId":"5a35f631-d8cf-4636-ef4e-02c54923df41"},"source":["tags_names_ordered= ['-PAD-','L_M','L_T','U_T','L_P','U_P','U_M','B_T','O','B_P','I_P','I_T','B_M','I_M']\n","print(classification_report(val_true.flatten(), val_pred_ner.flatten(), target_names=tags_names_ordered))"],"id":"3ho-RDd8_PMs","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","       -PAD-       1.00      1.00      1.00      8239\n","         L_M       0.52      0.41      0.46       237\n","         L_T       0.34      0.16      0.22       126\n","         U_T       0.00      0.00      0.00         9\n","         L_P       0.40      0.44      0.42       331\n","         U_P       0.37      0.08      0.13        88\n","         U_M       0.63      0.14      0.23       264\n","         B_T       0.39      0.15      0.22       126\n","           O       0.84      0.94      0.88      8340\n","         B_P       0.34      0.28      0.31       331\n","         I_P       0.26      0.19      0.22       449\n","         I_T       0.41      0.26      0.32       449\n","         B_M       0.46      0.29      0.35       237\n","         I_M       0.45      0.14      0.22       174\n","\n","    accuracy                           0.86     19400\n","   macro avg       0.46      0.32      0.36     19400\n","weighted avg       0.84      0.86      0.85     19400\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"markdown","metadata":{"id":"tMV6MVq-R-pk"},"source":["***REMARQUE***\n","\n","On remarque 'B_P' obtient un bon score de 75% en f1 micro. C'est celui qui le \"mieux prédit\" après le 'O'. On remarque 'L_T' n'est jamais classifié correctement et ce parce que il est très rarement obtenu dans nos données (9 fois seulement). Il est donc difficilement prédictible. Le 'O' reste le tag ayant le meilleur f1 score car le plus présent dans notre dataset. En conclusion les résultats de notre algorithme dépend fortement de leur fréquences d'apparitions dans nos données. Finalement, ces résultats restent médiocre et ne nous offre que 0.73 sur nos données de tests.\n","\n"],"id":"tMV6MVq-R-pk"},{"cell_type":"markdown","metadata":{"id":"itX9_FIRf3u1"},"source":["#### création des fichiers de soumission de validation et de test"],"id":"itX9_FIRf3u1"},{"cell_type":"code","metadata":{"id":"5CPfLO5VTU2-"},"source":["def suppress_pad(test_tags_y_predict,test_sentences_X_no_pad):\n","  # we supress the 0s correponding to the additional padding\n","  #test_tags_y_predict_no_pad = np.delete(test_tags_y_predict[0], np.where(test_tags_y_predict[0] == 0))\n","  test_tags_y_predict_no_pad = test_tags_y_predict[0][:len(test_sentences_X_no_pad[0])]\n","  for i in range (1,len(test_tags_y_predict)):\n","    A_i=test_tags_y_predict[i][:len(test_sentences_X_no_pad[i])]\n","    test_tags_y_predict_no_pad = np.concatenate((test_tags_y_predict_no_pad ,A_i))\n","    \n","    if len(test_sentences_X_no_pad[i])!=len(A_i):\n","      print(len(A_i))\n","      print(len(test_sentences_X_no_pad[i]))\n","      print(\"FALSE:\",i)\n","  \n","  return test_tags_y_predict_no_pad"],"id":"5CPfLO5VTU2-","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cj7IL_yQUlSh","colab":{"base_uri":"https://localhost:8080/","height":441},"executionInfo":{"status":"ok","timestamp":1638746539990,"user_tz":300,"elapsed":14183,"user":{"displayName":"lamia2 salhi2","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14440555040870778846"}},"outputId":"9380336f-5a64-4daa-ceb1-57b0b7228ffd"},"source":["val_pred_ner = model_selected.predict(val_sentences_X_B)\n","val_pred_ner_no_pad = suppress_pad(val_pred_ner,val_sentences_X_no_pad_B)\n","submission_val_bilstm_crf = pd.DataFrame(columns = ['TokenID', 'Tag'])\n","submission_val_bilstm_crf['TokenID'] = val_B_df['TokenID']\n","\n","val_ids_concat = np.concatenate(val_ids_B)\n","for i in tqdm(range(len(val_B_df))):\n","  k = val_B_df[val_B_df[\"TokenID\"] == val_ids_concat[i]].index[0]\n","  submission_val_bilstm_crf.at[k, \"Tag\"] = val_pred_ner_no_pad[i]\n","\n","submission_val_bilstm_crf['Tag'] = submission_val_bilstm_crf['Tag'].map(index2tagB)\n","submission_val_bilstm_crf"],"id":"cj7IL_yQUlSh","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 11161/11161 [00:13<00:00, 830.46it/s]\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TokenID</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>S0301010415300355-0</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>S0301010415300355-1</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>S0301010415300355-2</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>S0301010415300355-3</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>S0301010415300355-4</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11156</th>\n","      <td>S1359028614000989-185</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>11157</th>\n","      <td>S1359028614000989-186</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>11158</th>\n","      <td>S1359028614000989-187</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>11159</th>\n","      <td>S1359028614000989-188</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>11160</th>\n","      <td>S1359028614000989-189</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11161 rows × 2 columns</p>\n","</div>"],"text/plain":["                     TokenID Tag\n","0        S0301010415300355-0   O\n","1        S0301010415300355-1   O\n","2        S0301010415300355-2   O\n","3        S0301010415300355-3   O\n","4        S0301010415300355-4   O\n","...                      ...  ..\n","11156  S1359028614000989-185   O\n","11157  S1359028614000989-186   O\n","11158  S1359028614000989-187   O\n","11159  S1359028614000989-188   O\n","11160  S1359028614000989-189   O\n","\n","[11161 rows x 2 columns]"]},"metadata":{},"execution_count":125}]},{"cell_type":"code","metadata":{"id":"vRAEFt52eJXe","colab":{"base_uri":"https://localhost:8080/","height":441},"executionInfo":{"status":"ok","timestamp":1638746607470,"user_tz":300,"elapsed":43919,"user":{"displayName":"lamia2 salhi2","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14440555040870778846"}},"outputId":"f2ea7f8a-26ff-42f3-f3ee-9f7252fa6780"},"source":["test_pred_ner = model_selected.predict(test_sentences_X_B)\n","test_pred_ner_no_pad = suppress_pad(test_pred_ner,test_sentences_X_no_pad_B)\n","submission_test_bilstm_crf = pd.DataFrame(columns = ['TokenID', 'Tag'])\n","submission_test_bilstm_crf['TokenID'] = test_df['TokenID']\n","\n","\n","test_ids_concat = np.concatenate(test_ids_B)\n","for i in tqdm(range(len(test_df))):\n","  k = test_df[test_df[\"TokenID\"] == test_ids_concat[i]].index[0]\n","  submission_test_bilstm_crf.at[k, \"Tag\"] = test_pred_ner_no_pad[i]\n","\n","\n","submission_test_bilstm_crf['Tag'] = submission_test_bilstm_crf['Tag'].map(index2tagB)\n","submission_test_bilstm_crf"],"id":"vRAEFt52eJXe","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 21711/21711 [00:42<00:00, 506.80it/s]\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TokenID</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>S0885230816301759-0</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>S0885230816301759-1</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>S0885230816301759-2</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>S0885230816301759-3</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>S0885230816301759-4</td>\n","      <td>I_T</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>21706</th>\n","      <td>S1877750313001269-211</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>21707</th>\n","      <td>S1877750313001269-212</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>21708</th>\n","      <td>S1877750313001269-213</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>21709</th>\n","      <td>S1877750313001269-214</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>21710</th>\n","      <td>S1877750313001269-215</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>21711 rows × 2 columns</p>\n","</div>"],"text/plain":["                     TokenID  Tag\n","0        S0885230816301759-0    O\n","1        S0885230816301759-1    O\n","2        S0885230816301759-2    O\n","3        S0885230816301759-3    O\n","4        S0885230816301759-4  I_T\n","...                      ...  ...\n","21706  S1877750313001269-211    O\n","21707  S1877750313001269-212    O\n","21708  S1877750313001269-213    O\n","21709  S1877750313001269-214    O\n","21710  S1877750313001269-215    O\n","\n","[21711 rows x 2 columns]"]},"metadata":{},"execution_count":127}]},{"cell_type":"code","metadata":{"id":"fnED0DchYm3d"},"source":["submission_test_bilstm_crf.to_csv('Test_submission_Glove_Bilstm_CRF_tache_B.csv',index=False)\n","submission_val_bilstm_crf.to_csv('Val_submission_Glove_Bilstm_CRF_tache_B.csv',index=False)"],"id":"fnED0DchYm3d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5wIOsqXjGHwB"},"source":["### BERT"],"id":"5wIOsqXjGHwB"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qKO_DqPjJGlE","executionInfo":{"status":"ok","timestamp":1638742058305,"user_tz":300,"elapsed":1526,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"30401efc-702b-463d-9552-01bc5481559f"},"source":["from nltk.tokenize import sent_tokenize\n","print(\"==== Train dataset ====\")\n","all_sentences_train, all_tags_train_B = get_doc_sentences(get_text_docs('train'), 'train', train_sentences, train_tags_B)\n","print()\n","print()\n","print(\"==== Validation dataset ====\")\n","all_sentences_val, all_tags_val_B = get_doc_sentences(get_text_docs('val'), 'val', val_sentences, val_tags_B)"],"id":"qKO_DqPjJGlE","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==== Train dataset ====\n","Loading docs...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 350/350 [00:00<00:00, 777.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["   DONE.\n","\n","Constructing sentences...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 350/350 [00:00<00:00, 441.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Doc S0370269304009359 invalid\n","['Table', '1', 'lists', '8', 'pairs', 'of', 'B', 'decays', '.', 'In', 'fact', ',', 'there', 'are', 'more', 'decay', 'pairs', ',', 'since', 'many', 'of', 'the', 'particles', 'in', 'the', 'final', 'states', 'can', 'be', 'observed', 'as', 'either', 'pseudoscalar', '(', 'P', ')', 'or', 'vector', '(', 'V', ')', 'mesons', '.', 'Note', 'that', 'certain', 'decays', 'are', 'written', 'in', 'terms', 'of', 'VV', 'final', 'states', ',', 'while', 'others', 'are', 'have', 'PP', 'states', '.', 'There', 'are', 'three', 'reasons', 'for', 'this', '.', 'First', ',', 'some', 'decays', 'involve', 'a', 'final-state', 'π0', '.', 'However', ',', 'experimentally', 'it', 'will', 'be', 'necessary', 'to', 'find', 'the', 'decay', 'vertices', 'of', 'the', 'final', 'particles', '.', 'This', 'is', 'virtually', 'impossible', 'for', 'a', 'π0', ',', 'and', 'so', 'we', 'always', 'use', 'a', 'ρ0', '.', 'Second', ',', 'some', 'pairs', 'of', 'decays', 'are', 'related', 'by', 'SU', '(', '3', ')', 'in', 'the', 'SM', 'only', 'if', 'an', '(', 'ss¯', ')', 'quark', 'pair', 'is', 'used', '.', 'However', ',', 'there', 'are', 'no', 'P', \"'s\", 'which', 'are', 'pure', '(', 'ss¯', ')', '.', 'The', 'mesons', 'η', 'and', 'η′', 'have', 'an', '(', 'ss¯', ')', 'component', ',', 'but', 'they', 'also', 'have', 'significant', '(', 'uu¯', ')', 'and', '(', 'dd¯', ')', 'pieces', '.', 'As', 'a', 'result', 'the', 'b¯→s¯', 'and', 'b¯→d¯', 'decays', 'are', 'not', 'really', 'related', 'by', 'SU', '(', '3', ')', 'in', 'the', 'SM', 'if', 'the', 'final', 'state', 'involves', 'an', 'η', 'or', 'η′', '.', 'We', 'therefore', 'consider', 'instead', 'the', 'vector', 'meson', 'ϕ', 'which', 'is', 'essentially', 'a', 'pure', '(', 'ss¯', ')', 'quark', 'state', '.', 'Finally', ',', 'we', 'require', 'that', 'both', 'B0', 'and', 'B¯0', 'be', 'able', 'to', 'decay', 'to', 'the', 'final', 'state', '.', 'This', 'can', 'not', 'happen', 'if', 'the', 'final', 'state', 'contains', 'a', 'single', 'K0', '(', 'or', 'K¯0', ')', 'meson', '.', 'However', ',', 'it', 'can', 'occur', 'if', 'this', 'final-state', 'particle', 'is', 'an', 'excited', 'neutral', 'kaon', '.', 'In', 'this', 'case', 'one', 'decay', 'involves', 'K*0', ',', 'while', 'the', 'other', 'has', 'K¯*0', '.', 'Assuming', 'that', 'the', 'vector', 'meson', 'is', 'detected', 'via', 'its', 'decay', 'to', 'ψKsπ0', '(', 'as', 'in', 'the', 'measurement', 'of', 'sin2β', 'via', 'Bd0', '(', 't', ')', '→J/ψK*', ')', ',', 'then', 'both', 'B0', 'and', 'B¯0', 'can', 'decay', 'to', 'the', 'same', 'final', 'state', '.', '.']\n","['Table', '1', 'lists', '8', 'pairs', 'of', 'B', 'decays', '.', 'In', 'fact', ',', 'there', 'are', 'more', 'decay', 'pairs', ',', 'since', 'many', 'of', 'the', 'particles', 'in', 'the', 'final', 'states', 'can', 'be', 'observed', 'as', 'either', 'pseudoscalar', '(', 'P', ')', 'or', 'vector', '(', 'V', ')', 'mesons', '.', 'Note', 'that', 'certain', 'decays', 'are', 'written', 'in', 'terms', 'of', 'VV', 'final', 'states', ',', 'while', 'others', 'are', 'have', 'PP', 'states', '.', 'There', 'are', 'three', 'reasons', 'for', 'this', '.', 'First', ',', 'some', 'decays', 'involve', 'a', 'final-state', 'π0', '.', 'However', ',', 'experimentally', 'it', 'will', 'be', 'necessary', 'to', 'find', 'the', 'decay', 'vertices', 'of', 'the', 'final', 'particles', '.', 'This', 'is', 'virtually', 'impossible', 'for', 'a', 'π0', ',', 'and', 'so', 'we', 'always', 'use', 'a', 'ρ0', '.', 'Second', ',', 'some', 'pairs', 'of', 'decays', 'are', 'related', 'by', 'SU', '(', '3', ')', 'in', 'the', 'SM', 'only', 'if', 'an', '(', 'ss¯', ')', 'quark', 'pair', 'is', 'used', '.', 'However', ',', 'there', 'are', 'no', 'P', \"'s\", 'which', 'are', 'pure', '(', 'ss¯', ')', '.', 'The', 'mesons', 'η', 'and', 'η′', 'have', 'an', '(', 'ss¯', ')', 'component', ',', 'but', 'they', 'also', 'have', 'significant', '(', 'uu¯', ')', 'and', '(', 'dd¯', ')', 'pieces', '.', 'As', 'a', 'result', 'the', 'b¯→s¯', 'and', 'b¯→d¯', 'decays', 'are', 'not', 'really', 'related', 'by', 'SU', '(', '3', ')', 'in', 'the', 'SM', 'if', 'the', 'final', 'state', 'involves', 'an', 'η', 'or', 'η′', '.', 'We', 'therefore', 'consider', 'instead', 'the', 'vector', 'meson', 'ϕ', 'which', 'is', 'essentially', 'a', 'pure', '(', 'ss¯', ')', 'quark', 'state', '.', 'Finally', ',', 'we', 'require', 'that', 'both', 'B0', 'and', 'B¯0', 'be', 'able', 'to', 'decay', 'to', 'the', 'final', 'state', '.', 'This', 'can', 'not', 'happen', 'if', 'the', 'final', 'state', 'contains', 'a', 'single', 'K0', '(', 'or', 'K¯0', ')', 'meson', '.', 'However', ',', 'it', 'can', 'occur', 'if', 'this', 'final-state', 'particle', 'is', 'an', 'excited', 'neutral', 'kaon', '.', 'In', 'this', 'case', 'one', 'decay', 'involves', 'K', '*', '0', ',', 'while', 'the', 'other', 'has', 'K¯', '*', '0', '.', 'Assuming', 'that', 'the', 'vector', 'meson', 'is', 'detected', 'via', 'its', 'decay', 'to', 'ψKsπ0', '(', 'as', 'in', 'the', 'measurement', 'of', 'sin2β', 'via', 'Bd0', '(', 't', ')', '→J/ψK', '*', ')', ',', 'then', 'both', 'B0', 'and', 'B¯0', 'can', 'decay', 'to', 'the', 'same', 'final', 'state', '.']\n","\n","   DONE.\n","\n","Same number of sentences and tag sequences: True\n","Size: 2402\n","Looking at some sentences randomly:\n","[('The', 'O'), ('effect', 'O'), ('of', 'O'), ('increasing', 'B_P'), ('direct', 'I_P'), ('solar', 'I_P'), ('radiation', 'I_P'), ('on', 'I_P'), ('both', 'I_P'), ('solar', 'I_P'), ('cells', 'I_P'), ('and', 'I_P'), ('water', 'I_P'), ('temperatures', 'L_P'), ('is', 'O'), ('shown', 'O'), ('in', 'O'), ('Fig', 'O'), ('.', 'O')]\n","[('The', 'O'), ('first', 'O'), ('of', 'O'), ('these', 'O'), ('systems', 'O'), (',', 'O'), ('a', 'O'), ('biopolymer', 'B_P'), ('gel', 'L_P'), (',', 'O'), ('involves', 'O'), ('the', 'O'), ('thermoreversible', 'B_T'), ('gelation', 'L_T'), ('of', 'O'), ('aqueous', 'O'), ('gelatin', 'O'), ('solutions', 'O'), ('to', 'O'), ('form', 'O'), ('a', 'O'), ('physical', 'O'), ('gel', 'O'), (',', 'O'), ('whereas', 'O'), ('the', 'O'), ('other', 'O'), ('systems', 'O'), ('considered', 'O'), ('herein', 'O'), ('involve', 'O'), ('the', 'O'), ('formation', 'O'), ('of', 'O'), ('chemical', 'O'), ('gels', 'O'), ('featuring', 'O'), ('permanent', 'O'), ('cross-linked', 'B_P'), ('branching', 'I_P'), ('networks', 'L_P'), ('.', 'O')]\n","[('The', 'O'), ('Schrödinger-electrostatic', 'B_P'), ('algorithm', 'L_P'), ('was', 'O'), ('propounded', 'O'), ('to', 'O'), ('further', 'O'), ('increase', 'O'), ('both', 'O'), ('the', 'O'), ('accuracy', 'O'), ('and', 'O'), ('alacrity', 'O'), ('of', 'O'), ('detecting', 'B_P'), ('natural', 'I_P'), ('phenomena', 'I_P'), ('.', 'L_P')]\n","[('The', 'O'), ('analysis', 'O'), ('relies', 'O'), ('essentially', 'O'), ('on', 'O'), ('some', 'O'), ('new', 'B_M'), ('regularity', 'I_M'), ('results', 'L_M'), ('of', 'O'), ('the', 'O'), ('multi-term', 'O'), ('time', 'O'), ('fractional', 'O'), ('diffusion', 'O'), ('equation', 'O'), ('.', 'O')]\n","[('We', 'O'), ('show', 'O'), ('that', 'O'), ('the', 'O'), ('method', 'O'), ('can', 'O'), ('be', 'O'), ('applied', 'O'), ('to', 'O'), ('isogeometric', 'B_T'), ('analysis', 'L_T'), ('with', 'O'), ('little', 'O'), ('effort', 'O'), (',', 'O'), ('once', 'O'), ('the', 'O'), ('framework', 'B_M'), ('of', 'I_M'), ('NURBS', 'I_M'), ('based', 'I_M'), ('shape', 'I_M'), ('functions', 'L_M'), ('has', 'O'), ('been', 'O'), ('implemented', 'O'), ('.', 'O')]\n","\n","\n","==== Validation dataset ====\n","Loading docs...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 50/50 [00:00<00:00, 683.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["   DONE.\n","\n","Constructing sentences...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 50/50 [00:00<00:00, 368.20it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","   DONE.\n","\n","Same number of sentences and tag sequences: True\n","Size: 413\n","Looking at some sentences randomly:\n","[('The', 'O'), ('four', 'O'), ('bounding', 'B_M'), ('PCM', 'I_M'), ('wastes', 'L_M'), (',', 'O'), ('given', 'O'), ('in', 'O'), ('Table', 'O'), ('1', 'O'), (',', 'O'), ('were', 'O'), ('simulated', 'O'), ('using', 'O'), ('the', 'O'), ('most', 'O'), ('appropriate', 'O'), ('materials', 'O'), ('and', 'O'), ('geometries', 'O'), ('.', 'O')]\n","[('Between', 'O'), ('iterations', 'O'), (',', 'O'), ('the', 'O'), ('1000', 'O'), ('values', 'O'), ('acquired', 'O'), ('were', 'O'), ('averaged', 'O'), ('to', 'O'), ('obtain', 'B_T'), ('a', 'I_T'), ('single', 'I_T'), ('value', 'I_T'), ('of', 'I_T'), ('potential', 'L_T'), (',', 'O'), ('subsequently', 'O'), ('saved', 'O'), ('to', 'O'), ('the', 'O'), ('file', 'O'), ('used', 'O'), ('for', 'O'), ('later', 'O'), ('processing', 'O'), ('.', 'O')]\n","[('A', 'O'), ('frequently', 'O'), ('employed', 'O'), ('surfactant', 'U_M'), ('was', 'O'), ('N-dodecylpyridinium', 'B_M'), ('bromide', 'L_M'), ('(', 'O'), ('DDPB', 'U_M'), (')', 'O'), ('[', 'O'), ('9,60,61,108,109', 'O'), (']', 'O'), ('.', 'O')]\n","[('This', 'O'), ('is', 'O'), ('a', 'O'), ('further', 'O'), ('reason', 'O'), ('to', 'O'), ('adopt', 'O'), ('the', 'O'), ('CLSVOF', 'B_P'), ('method', 'L_P'), (',', 'O'), ('which', 'O'), ('has', 'O'), ('been', 'O'), ('used', 'O'), ('for', 'O'), ('all', 'O'), ('the', 'O'), ('following', 'O'), ('simulations', 'B_T'), ('of', 'I_T'), ('liquid', 'I_T'), ('jet', 'I_T'), ('primary', 'I_T'), ('breakup', 'L_T'), ('.', 'O')]\n","[('Such', 'O'), ('outcome', 'O'), ('is', 'O'), ('probably', 'O'), ('related', 'O'), ('to', 'O'), ('the', 'O'), ('simplified', 'B_P'), ('contact', 'I_P'), ('model', 'L_P'), ('used', 'O'), ('by', 'O'), ('FM', 'U_P'), (',', 'O'), ('which', 'O'), ('makes', 'O'), ('the', 'O'), ('stent-graft', 'B_P'), ('expansion', 'L_P'), ('terminate', 'O'), ('once', 'O'), ('the', 'O'), ('nodes', 'U_M'), ('come', 'O'), ('in', 'O'), ('contact', 'O'), ('with', 'O'), ('the', 'O'), ('vessel', 'B_M'), ('wall', 'L_M'), ('.', 'O')]\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VzzuNuBvKso6","executionInfo":{"status":"ok","timestamp":1638742060275,"user_tz":300,"elapsed":106,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"7c2fe36c-a91d-4dc9-ce6a-ad999232a3e4"},"source":["all_tags_train_B[10]"],"id":"VzzuNuBvKso6","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B_T', 'I_T', 'I_T', 'I_T',\n","       'L_T', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n","       'O'], dtype=object)"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-fhrjXPLKAFG","executionInfo":{"status":"ok","timestamp":1638742075158,"user_tz":300,"elapsed":13593,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"cd3d6bcb-a83f-4bc1-8719-0f7cec0cd6cc"},"source":["print(\"==== Train dataset ====\")\n","train_sentences_X, train_attention_masks, train_tags_tokenized_B = get_final_data(all_sentences_train, all_tags_train_B)\n","print()\n","print()\n","print(\"==== Validation dataset ====\")\n","val_sentences_X, val_attention_masks, val_tags_tokenized_B = get_final_data(all_sentences_val, all_tags_val_B)"],"id":"-fhrjXPLKAFG","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==== Train dataset ====\n","Adapting tags to BERT tokenizer...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2402/2402 [00:06<00:00, 369.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["   DONE.\n","Applying BERT tokenizer to the sentences...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2402/2402 [00:03<00:00, 735.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["   DONE.\n","\n","Looking at some sentences randomly:\n","[('CLS', '-N-', 1), ('conversely', 'O', 1), (',', 'O', 1), ('the', 'O', 1), ('versa', 'B_P', 1), ('##tility', '-N-', 1), ('of', 'I_P', 1), ('simplified', 'I_P', 1), ('(', 'I_P', 1), ('approximate', 'I_P', 1), (')', 'I_P', 1), ('methods', 'L_P', 1), (',', 'O', 1), ('such', 'O', 1), ('as', 'O', 1), ('the', 'O', 1), ('interaction', 'B_P', 1), ('factor', 'I_P', 1), ('approach', 'L_P', 1), ('that', 'O', 1), ('allows', 'O', 1), ('capturing', 'B_P', 1), ('the', 'I_P', 1), ('(', 'I_P', 1), ('e', 'I_P', 1), ('.', '-N-', 1), ('g', '-N-', 1), ('.', '-N-', 1), (',', 'I_P', 1), ('vertical', 'I_P', 1), (')', 'I_P', 1), ('displacement', 'I_P', 1), ('##s', '-N-', 1), ('of', 'I_P', 1), ('any', 'I_P', 1), ('general', 'I_P', 1), ('pile', 'I_P', 1), ('group', 'L_P', 1), ('by', 'O', 1), ('the', 'O', 1), ('analysis', 'B_P', 1), ('of', 'I_P', 1), ('the', 'I_P', 1), ('displacement', 'I_P', 1), ('interaction', 'I_P', 1), ('between', 'I_P', 1), ('two', 'I_P', 1), ('identical', 'I_P', 1), ('piles', 'L_P', 1), ('and', 'O', 1), ('by', 'O', 1), ('the', 'O', 1), ('use', 'O', 1), ('of', 'O', 1), ('the', 'O', 1), ('elastic', 'B_P', 1), ('principle', 'I_P', 1), ('of', 'I_P', 1), ('super', 'I_P', 1), ('##position', '-N-', 1), ('of', 'I_P', 1), ('effects', 'L_P', 1), (',', 'O', 1), ('makes', 'O', 1), ('them', 'O', 1), ('attractive', 'O', 1), ('as', 'O', 1), ('design', 'B_M', 1), ('tools', 'L_M', 1), ('because', 'O', 1), ('they', 'O', 1), ('allow', 'O', 1), ('for', 'O', 1), ('the', 'O', 1), ('use', 'O', 1), ('of', 'O', 1), ('ex', 'B_T', 1), ('##ped', '-N-', 1), ('##ient', '-N-', 1), ('para', 'I_T', 1), ('##metric', '-N-', 1), ('studies', 'L_T', 1), ('under', 'O', 1), ('various', 'O', 1), ('design', 'O', 1), ('conditions', 'O', 1), ('.', 'O', 1), ('SEP', '-N-', 1), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0)]\n","[('CLS', '-N-', 1), ('for', 'O', 1), ('d', 'U_M', 1), ('-', '-N-', 1), ('me', '-N-', 1), ('##sons', '-N-', 1), (',', 'O', 1), ('we', 'O', 1), ('found', 'O', 1), ('that', 'O', 1), ('the', 'O', 1), ('difference', 'B_P', 1), ('in', 'I_P', 1), ('the', 'I_P', 1), ('slope', 'I_P', 1), ('parameters', 'L_P', 1), ('of', 'O', 1), ('the', 'O', 1), ('pt', 'U_T', 1), ('-', '-N-', 1), ('spectra', '-N-', 1), ('in', 'O', 1), ('the', 'O', 1), ('two', 'O', 1), ('scenarios', 'O', 1), ('is', 'O', 1), ('less', 'O', 1), ('pronounced', 'O', 1), (',', 'O', 1), ('but', 'O', 1), ('their', 'O', 1), ('elliptic', 'B_P', 1), ('flow', 'L_P', 1), ('is', 'O', 1), ('about', 'O', 1), ('a', 'O', 1), ('factor', 'O', 1), ('of', 'O', 1), ('2', 'O', 1), ('larger', 'O', 1), ('for', 'O', 1), ('[UNK]', 'O', 1), ('.', '-N-', 1), ('5', '-N-', 1), ('ge', 'O', 1), ('##v', '-N-', 1), ('in', 'O', 1), ('the', 'O', 1), ('thermal', 'O', 1), ('##ized', '-N-', 1), ('case', 'O', 1), ('.', 'O', 1), ('SEP', '-N-', 1), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0)]\n","[('CLS', '-N-', 1), ('the', 'O', 1), ('algorithm', 'O', 1), ('used', 'O', 1), ('by', 'O', 1), ('us', 'O', 1), ('to', 'O', 1), ('evaluate', 'O', 1), ('the', 'O', 1), ('k', 'O', 1), ('##rat', '-N-', 1), ('##ky', '-N-', 1), ('plots', 'O', 1), ('by', 'O', 1), ('sets', 'O', 1), ('of', 'O', 1), ('parallel', 'O', 1), ('polymer', 'O', 1), ('stems', 'O', 1), ('is', 'O', 1), ('very', 'O', 1), ('simplified', 'O', 1), ('.', 'O', 1), ('SEP', '-N-', 1), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0)]\n","[('CLS', '-N-', 1), ('the', 'O', 1), ('patterned', 'B_M', 1), ('hydro', 'I_M', 1), ('##gel', '-N-', 1), ('films', 'L_M', 1), ('can', 'O', 1), ('be', 'O', 1), ('triggered', 'B_P', 1), ('consecutive', 'L_P', 1), ('##ly', '-N-', 1), ('allowing', 'O', 1), ('for', 'O', 1), ('successive', 'O', 1), ('rolling', 'U_P', 1), ('and', 'O', 1), ('un', 'U_P', 1), ('##roll', '-N-', 1), ('##ing', '-N-', 1), ('depending', 'O', 1), ('on', 'O', 1), ('the', 'O', 1), ('a', 'B_M', 1), ('##que', '-N-', 1), ('##ous', '-N-', 1), ('ph', 'L_M', 1), ('.', 'O', 1), ('SEP', '-N-', 1), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0)]\n","[('CLS', '-N-', 1), ('the', 'O', 1), ('measurements', 'O', 1), ('were', 'O', 1), ('carried', 'O', 1), ('out', 'O', 1), ('under', 'O', 1), ('an', 'O', 1), ('ar', 'U_M', 1), ('##gon', '-N-', 1), ('atmosphere', 'O', 1), ('(', 'O', 1), ('with', 'O', 1), ('an', 'O', 1), ('oxygen', 'U_M', 1), ('content', 'O', 1), ('of', 'O', 1), ('7', 'O', 1), ('pp', 'O', 1), ('##m', '-N-', 1), (')', 'O', 1), (',', 'O', 1), ('using', 'O', 1), ('pure', 'O', 1), ('platinum', 'B_M', 1), ('ing', 'L_M', 1), ('##ots', '-N-', 1), ('(', 'O', 1), ('64', 'O', 1), ('–', '-N-', 1), ('144', '-N-', 1), ('mg', 'O', 1), (')', 'O', 1), ('of', 'O', 1), ('99', 'O', 1), ('.', '-N-', 1), ('95', '-N-', 1), ('at', 'O', 1), ('%', 'O', 1), ('purity', 'O', 1), ('as', 'O', 1), ('a', 'O', 1), ('reference', 'B_M', 1), ('material', 'L_M', 1), ('.', 'O', 1), ('SEP', '-N-', 1), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0)]\n","\n","\n","==== Validation dataset ====\n","Adapting tags to BERT tokenizer...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 413/413 [00:01<00:00, 376.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["   DONE.\n","Applying BERT tokenizer to the sentences...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 413/413 [00:00<00:00, 791.49it/s]"]},{"output_type":"stream","name":"stdout","text":["   DONE.\n","\n","Looking at some sentences randomly:\n","[('CLS', '-N-', 1), ('this', 'O', 1), ('approach', 'O', 1), ('is', 'O', 1), ('helpful', 'O', 1), (',', 'O', 1), ('but', 'O', 1), ('it', 'O', 1), ('might', 'O', 1), ('be', 'O', 1), ('extended', 'O', 1), ('further', 'O', 1), ('by', 'O', 1), ('modelling', 'B_P', 1), ('proximity', 'I_P', 1), ('across', 'I_P', 1), ('flows', 'L_P', 1), ('through', 'O', 1), ('a', 'O', 1), ('distance', 'O', 1), ('that', 'O', 1), ('would', 'O', 1), ('relate', 'O', 1), ('to', 'O', 1), ('the', 'O', 1), ('flow', 'U_P', 1), ('characteristics', 'O', 1), ('in', 'O', 1), ('order', 'O', 1), ('to', 'O', 1), ('borrow', 'O', 1), ('strength', 'O', 1), ('across', 'O', 1), ('cal', 'O', 1), ('##ib', '-N-', 1), ('##rations', '-N-', 1), ('instead', 'O', 1), ('of', 'O', 1), ('splitting', 'B_P', 1), ('the', 'I_P', 1), ('cal', 'I_P', 1), ('##ib', '-N-', 1), ('##rations', '-N-', 1), ('and', 'I_P', 1), ('then', 'I_P', 1), ('merging', 'I_P', 1), ('the', 'I_P', 1), ('outcomes', 'L_P', 1), ('afterwards', 'O', 1), ('.', 'O', 1), ('SEP', '-N-', 1), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0)]\n","[('CLS', '-N-', 1), ('this', 'O', 1), ('method', 'O', 1), ('leads', 'O', 1), ('to', 'O', 1), ('solution', 'O', 1), ('of', 'O', 1), ('perhaps', 'O', 1), ('the', 'O', 1), ('most', 'O', 1), ('known', 'O', 1), ('test', 'O', 1), ('-', '-N-', 1), ('case', '-N-', 1), ('that', 'O', 1), ('exhibits', 'O', 1), ('a', 'O', 1), ('first', 'B_P', 1), ('order', 'I_P', 1), ('phase', 'I_P', 1), ('transition', 'L_P', 1), ('(', 'O', 1), ('semi', 'O', 1), ('-', '-N-', 1), ('he', '-N-', 1), ('##uri', '-N-', 1), ('##stic', '-N-', 1), ('##ally', '-N-', 1), ('described', 'O', 1), (')', 'O', 1), ('such', 'O', 1), ('as', 'O', 1), ('the', 'O', 1), ('van', 'B_P', 1), ('der', 'I_P', 1), ('wa', 'I_P', 1), ('##als', '-N-', 1), ('model', 'L_P', 1), ('.', 'O', 1), ('SEP', '-N-', 1), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0)]\n","[('CLS', '-N-', 1), ('the', 'O', 1), ('major', 'O', 1), ('deficiency', 'O', 1), ('of', 'O', 1), ('the', 'O', 1), ('mor', 'B_P', 1), ('##l', '-N-', 1), ('potential', 'L_P', 1), ('is', 'O', 1), ('that', 'O', 1), ('the', 'O', 1), ('cat', 'U_M', 1), ('##ion', '-N-', 1), ('defect', 'O', 1), ('energies', 'O', 1), ('are', 'O', 1), ('high', 'O', 1), (',', 'O', 1), ('and', 'O', 1), ('hence', 'O', 1), ('the', 'O', 1), ('number', 'O', 1), ('of', 'O', 1), ('cat', 'U_M', 1), ('##ion', '-N-', 1), ('defects', 'O', 1), ('will', 'O', 1), ('be', 'O', 1), ('under', 'O', 1), ('##est', '-N-', 1), ('##imated', '-N-', 1), ('.', 'O', 1), ('SEP', '-N-', 1), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0)]\n","[('CLS', '-N-', 1), ('also', 'O', 1), (',', 'O', 1), ('{', 'O', 1), ('φ', 'O', 1), ('##n', '-N-', 1), ('}', 'O', 1), ('n', 'O', 1), ('##∈', '-N-', 1), ('##in', '-N-', 1), ('is', 'O', 1), ('constructed', 'O', 1), ('to', 'O', 1), ('be', 'O', 1), ('orthogonal', 'O', 1), ('with', 'O', 1), ('respect', 'O', 1), ('to', 'O', 1), ('l', 'O', 1), ('##r', '-N-', 1), (',', 'O', 1), ('not', 'O', 1), ('l', 'O', 1), ('##q', '-N-', 1), ('.', 'O', 1), ('SEP', '-N-', 1), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0)]\n","[('CLS', '-N-', 1), ('a', 'O', 1), ('significant', 'O', 1), ('draw', 'O', 1), ('##back', '-N-', 1), (',', 'O', 1), ('for', 'O', 1), ('either', 'O', 1), ('approach', 'O', 1), (',', 'O', 1), ('is', 'O', 1), ('a', 'O', 1), ('dependence', 'O', 1), ('on', 'O', 1), ('a', 'O', 1), ('supply', 'O', 1), ('of', 'O', 1), ('en', 'B_M', 1), ('##ant', '-N-', 1), ('##io', '-N-', 1), ('##pur', '-N-', 1), ('##e', '-N-', 1), ('re', 'I_M', 1), ('##age', '-N-', 1), ('##nts', '-N-', 1), ('or', 'I_M', 1), ('substrates', 'L_M', 1), ('–', 'O', 1), ('synthesis', 'B_P', 1), ('routes', 'L_P', 1), ('generally', 'O', 1), ('ut', 'O', 1), ('##ilis', '-N-', 1), ('##e', '-N-', 1), ('chi', 'B_M', 1), ('##ral', '-N-', 1), ('building', 'I_M', 1), ('blocks', 'L_M', 1), ('or', 'O', 1), ('en', 'B_M', 1), ('##ant', '-N-', 1), ('##ios', '-N-', 1), ('##ele', '-N-', 1), ('##ctive', '-N-', 1), ('catalyst', 'L_M', 1), ('##s', '-N-', 1), ('[', 'O', 1), ('7', 'O', 1), (',', '-N-', 1), ('8', '-N-', 1), (']', 'O', 1), (',', 'O', 1), ('while', 'O', 1), ('en', 'B_P', 1), ('##ant', '-N-', 1), ('##iom', '-N-', 1), ('##er', '-N-', 1), ('separation', 'I_P', 1), ('techniques', 'L_P', 1), ('typically', 'O', 1), ('incorporate', 'O', 1), ('chi', 'B_M', 1), ('##ral', '-N-', 1), ('selector', 'I_M', 1), ('molecules', 'L_M', 1), ('to', 'O', 1), ('form', 'O', 1), ('chemical', 'O', 1), ('##ly', '-N-', 1), ('distinct', 'O', 1), ('and', 'O', 1), ('distinguish', 'O', 1), ('##able', '-N-', 1), ('dia', 'B_M', 1), ('##ster', '-N-', 1), ('##eo', '-N-', 1), ('##meric', '-N-', 1), ('complexes', 'L_M', 1), ('[', 'O', 1), ('8', 'O', 1), (',', '-N-', 1), ('9', '-N-', 1), (']', 'O', 1), ('.', 'O', 1), ('SEP', '-N-', 1), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0), ('PAD', '-N-', 0)]\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","metadata":{"id":"2aKSwE3KKQX4"},"source":["# Map each unique label to an integer.\n","# label_map = {'N':-100} # we don't keep that to be able to apply one_hot encoding for custom BERT\n","label_map_B = {}\n","\n","# For each label...\n","for (i, label) in enumerate(set([tag for L in train_tags_tokenized_B for tag in L])):\n","    \n","    # Map it to its integer.\n","    label_map_B[label] = i"],"id":"2aKSwE3KKQX4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xD0XFK1QKUQG","executionInfo":{"status":"ok","timestamp":1638742081531,"user_tz":300,"elapsed":96,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"29d65693-7b68-46cf-b9db-fe76b39ffcaf"},"source":["print(label_map_B)"],"id":"xD0XFK1QKUQG","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'U_P': 0, 'I_P': 1, 'U_T': 2, 'U_M': 3, 'B_P': 4, 'O': 5, 'I_T': 6, 'L_T': 7, 'I_M': 8, 'B_M': 9, 'B_T': 10, '-N-': 11, 'L_M': 12, 'L_P': 13}\n"]}]},{"cell_type":"code","metadata":{"id":"TCpKaS9aOeDQ"},"source":["train_new_labels_B = [[label_map_B[elt] for elt in L] for L in train_tags_tokenized_B]\n","val_new_labels_B = [[label_map_B[elt] for elt in L] for L in val_tags_tokenized_B]"],"id":"TCpKaS9aOeDQ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zX7_B72oOxlL"},"source":["train_dataloader_B = get_data_loader(train_sentences_X, train_attention_masks, train_new_labels_B, RandomSampler)\n","validation_dataloader_B = get_data_loader(val_sentences_X, val_attention_masks, val_new_labels_B, SequentialSampler)"],"id":"zX7_B72oOxlL","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"odFgkBIWPfhT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638744034316,"user_tz":300,"elapsed":10315,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"eef87783-4c0f-4219-92db-221da20a7701"},"source":["# model = get_BertForTokenClassification(label_map_B)\n","# model.cuda()\n","# train_model(model, train_dataloader_B, 5)\n","evaluate_model(model, validation_dataloader_B, label_map_B)"],"id":"odFgkBIWPfhT","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 26/26 [00:10<00:00,  2.58it/s]"]},{"output_type":"stream","name":"stdout","text":["f1 micro: 0.811\n","Accuracy: 0.811\n","[[  44    4    0   13    4   18    1    0    0    0    3    0    0    1]\n"," [   3  218    0    3   15  124   41    2   14    9    1    0    2   17]\n"," [   4    0    0    0    1    3    0    1    0    0    0    0    0    0]\n"," [   9    7    0  155   14   31    5    3    6   13    3    3   10    5]\n"," [   2   18    0    0  201   60   15    0    2   15   17    1    0    0]\n"," [  27  241    0   12  122 7457  111   28   60   56   42   14   50  120]\n"," [   0   43    0    0   23  145  192    8    1    6   20    0    3    8]\n"," [   1    1    0    1    0   15    7   60    0    0    0    0    5   36]\n"," [   0   20    0    0    1   19    4    0  106   15    0    1    7    1]\n"," [   0    5    0    8   23   25    6    0   10  160    0    0    0    0]\n"," [   0    2    0    0   28   43    3    0    0    1   49    0    0    0]\n"," [   0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n"," [   0    0    0    2    0   19    3    3    1    1    0    0  187   21]\n"," [   2    3    0    5    1   56    5   21    1    0    0    0   17  220]]\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"execute_result","data":{"text/plain":["0.810769644297106"]},"metadata":{},"execution_count":96}]},{"cell_type":"code","metadata":{"id":"mUcsxabPSwG6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638744056596,"user_tz":300,"elapsed":17463,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"238e5df3-cbda-4d5f-8b45-22113f7f249e"},"source":["all_sentences_test, _ = get_doc_sentences(get_text_docs('test'), 'test', test_sentences, None)"],"id":"mUcsxabPSwG6","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading docs...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:17<00:00,  5.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["   DONE.\n","\n","Constructing sentences...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:00<00:00, 373.35it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","   DONE.\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","metadata":{"id":"tnGAdpvBS3uQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638744062308,"user_tz":300,"elapsed":787,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"6e311f53-dbfb-46c1-9057-1f7d70143abe"},"source":["test_sentences_X, test_attention_masks, _ = get_final_data(all_sentences_test, None)"],"id":"tnGAdpvBS3uQ","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Applying BERT tokenizer to the sentences...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 829/829 [00:01<00:00, 771.36it/s]"]},{"output_type":"stream","name":"stdout","text":["   DONE.\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","metadata":{"id":"Y3N4vBt6TAwY"},"source":["test_dataloader = get_data_loader(test_sentences_X, test_attention_masks, None, SequentialSampler)"],"id":"Y3N4vBt6TAwY","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6oon1WD0TA1m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638744093480,"user_tz":300,"elapsed":20220,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"e5405994-cd0d-4cc7-96d3-7e70fa92b95b"},"source":["# Prediction on test set\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions_test , true_labels_test = [], []\n","\n","# Predict \n","for batch in tqdm(test_dataloader):\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","\n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      if isinstance(model,BertCustom):\n","        logits,loss = model(b_input_ids, b_input_mask, None)\n","      else:\n","        result = model(b_input_ids, \n","                      token_type_ids=None, \n","                      attention_mask=b_input_mask,\n","                      return_dict=True)\n","        logits = result.logits\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions_test.append(logits)\n","  true_labels_test.append(label_ids)\n","\n","print('    DONE.')"],"id":"6oon1WD0TA1m","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 52/52 [00:20<00:00,  2.57it/s]"]},{"output_type":"stream","name":"stdout","text":["    DONE.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","metadata":{"id":"R4iI5lRnTPUk"},"source":["all_predictions_test = np.concatenate(predictions_test, axis=0)\n","predicted_label_ids_test = np.argmax(all_predictions_test, axis=2)\n","\n","predicted_label_ids_test = np.concatenate(predicted_label_ids_test, axis=0)"],"id":"R4iI5lRnTPUk","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AWsZvaY2TXGP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638744093575,"user_tz":300,"elapsed":120,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"deeadbe2-caa7-491f-bf2a-164b80f8c563"},"source":["index_map_B = {value: key for key,value in label_map_B.items()}\n","index_map_B"],"id":"AWsZvaY2TXGP","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'U_P',\n"," 1: 'I_P',\n"," 2: 'U_T',\n"," 3: 'U_M',\n"," 4: 'B_P',\n"," 5: 'O',\n"," 6: 'I_T',\n"," 7: 'L_T',\n"," 8: 'I_M',\n"," 9: 'B_M',\n"," 10: 'B_T',\n"," 11: '-N-',\n"," 12: 'L_M',\n"," 13: 'L_P'}"]},"metadata":{},"execution_count":102}]},{"cell_type":"code","metadata":{"id":"Z2ukF6vQTho8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638744142658,"user_tz":300,"elapsed":49039,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"7ea1d3ce-429f-420c-f6d5-ed3e5860198e"},"source":["test_df_soumission = test_df.drop(['DocID', 'Token'], 1)\n","test_df_soumission = test_df_soumission.astype({'Tag': 'string'})\n","\n","clean_tokens = []\n","clean_predictions = []\n","for pred, token in zip(predicted_label_ids_test, np.concatenate(test_sentences_X)):\n","  if token not in (tokenizer.pad_token_id, tokenizer.cls_token_id, tokenizer.sep_token_id):\n","    clean_tokens.append(token)\n","    if index_map_B[pred] == null_tag:\n","      clean_predictions.append(\"O\")\n","    else:\n","      clean_predictions.append(index_map_B[pred])\n","\n","i = 0\n","for word, word_id in tqdm(list(zip(np.concatenate(all_sentences_test), np.concatenate(ids)))):\n","  k = test_df_soumission[test_df_soumission[\"TokenID\"] == word_id].index[0]\n","  test_df_soumission.at[k, \"Tag\"] = clean_predictions[i]\n","  i += len(tokenizer.tokenize(word))"],"id":"Z2ukF6vQTho8","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 21711/21711 [00:47<00:00, 452.89it/s]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"wj8qmN81pcR4","executionInfo":{"status":"ok","timestamp":1638744142661,"user_tz":300,"elapsed":27,"user":{"displayName":"Samuel Reyd","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00536899999376066322"}},"outputId":"912c18ca-f3a4-431b-c9a0-cbcc0965b86c"},"source":["test_df_soumission"],"id":"wj8qmN81pcR4","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TokenID</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>S0885230816301759-0</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>S0885230816301759-1</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>S0885230816301759-2</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>S0885230816301759-3</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>S0885230816301759-4</td>\n","      <td>B_P</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>21706</th>\n","      <td>S1877750313001269-211</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>21707</th>\n","      <td>S1877750313001269-212</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>21708</th>\n","      <td>S1877750313001269-213</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>21709</th>\n","      <td>S1877750313001269-214</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>21710</th>\n","      <td>S1877750313001269-215</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>21711 rows × 2 columns</p>\n","</div>"],"text/plain":["                     TokenID  Tag\n","0        S0885230816301759-0    O\n","1        S0885230816301759-1    O\n","2        S0885230816301759-2    O\n","3        S0885230816301759-3    O\n","4        S0885230816301759-4  B_P\n","...                      ...  ...\n","21706  S1877750313001269-211    O\n","21707  S1877750313001269-212    O\n","21708  S1877750313001269-213    O\n","21709  S1877750313001269-214    O\n","21710  S1877750313001269-215    O\n","\n","[21711 rows x 2 columns]"]},"metadata":{},"execution_count":104}]},{"cell_type":"code","metadata":{"id":"vlQc8GOET2PW"},"source":["test_df_soumission.to_csv('submission_test_B_bert.csv', index=False, encoding='utf-8')"],"id":"vlQc8GOET2PW","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0f0efac9-23f9-4aa8-a45b-28a84eee1932"},"source":["# 7.3.Conclusion (5%)\n","Indiquez, dans une cellule, vos conclusions sur la tâche : qu’est-ce qui fonctionne ? qu’est-ce qui ne fonctionne pas ? quel type de pré-traitement vous a donné les meilleurs résultats ? quelles architectures ?"],"id":"0f0efac9-23f9-4aa8-a45b-28a84eee1932"},{"cell_type":"markdown","metadata":{"id":"ifsPImVlGO0i"},"source":["Finalement, pour la tache A c'est le modèle BERT qui nous donnes les meilleurs résultats. Le modèle Bilstm-CRF et CRF nous des résultats corrected qui dépassent un simple algorithme qui prédirait que des 'O'. Pour la tache B c'est le Bilstm-CRF qui nous donne les meilleurs résultats.\n","\n","En ce qui concerne le CRF que ce soit pour la tache A ou B, ce modèle ne prend pas en compte le déséquilibre du jeu de donnée et résulte en des f1 score médiocres pour les autre labels.\n","\n","En ce qui concerne de Bilstm-CRF, pour la tache A et B, le modèle perfome mieux avec Glove, de plus utiliser un layer bilstm donne des meilleurs résultats pour la tache A mais pour la tache B c'est le GRU layer qui donne de meilleurs résultats.\n","\n","Enfin pour le BERT, ce dernier perfome trés bien pour la tache A  mais nous n'avons pas eu d'aussi bon résultat pour la tache B, en effet on a obtenu un score de 70% sur kaggle sur les données de test. Peut être est ce qu à une erreur d'implémentation.  \n","\n","Pour Bert, nous avons d'abord utiliser l'approche classique BertForTokenClassification. Puis nous avons tenté de trouver des méta-paramètres plus adaptés à notre problème en variant les méthodes d'aggregation des sous couches les plus hautes pour obtenir le vecteur à passer au réseau à ajuster. Malheuresement aucune structure que nous avons testé ne nous a permis d'obtenir de meilleurs résultats que le BertForTokenClassification.  \n","Cette architecture nous a permis d'obtenir de bons résultats sur la tache A avec plus de 82% en f1 score sur le validation set mais malheuresement de moins bon resultats sur la tache B, en effet on a obtenu un score de 70% sur kaggle sur les données de test. Nous aurions pu essayer d'autres approche pour palier au manque de données de certaines classes. Peut être est ce qu à une erreur d'implémentation.  \n","On aurait aussi pu essayer d'utiliser ds architectures de transformers plus puissantes telles que RoBERTa\n","\n","\n","Finalement, avec plus de temps nous aurions voulus tester d'autres embeddings pour le bilstm-CRF comme le Elmo embedding. Nous aurions voulu tester le state of the art CNN-Bistlm-CRF qui consistait en créer un embedding des caractéres à l'aide de CNN. Et enfin une autr idée était de combiner Bert et CRF."],"id":"ifsPImVlGO0i"},{"cell_type":"code","metadata":{"id":"OEHYbPTFCfoH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638766327779,"user_tz":300,"elapsed":2645,"user":{"displayName":"Lamia Salhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFMkAlJGmsSt7L5ATXPD56Fe9tUWtNv0S5tv4tCQ=s64","userId":"17072597845416678588"}},"outputId":"4b298f46-9b61-4474-af62-4eda4d5b8148"},"source":["%%shell\n","jupyter nbconvert --to html /content/BERT_SOUMISSION.ipynb\n","\n"],"id":"OEHYbPTFCfoH","execution_count":104,"outputs":[{"output_type":"stream","name":"stdout","text":["[NbConvertApp] Converting notebook /content/BERT_SOUMISSION.ipynb to html\n","[NbConvertApp] Writing 1178349 bytes to /content/BERT_SOUMISSION.html\n"]},{"output_type":"execute_result","data":{"text/plain":[""]},"metadata":{},"execution_count":104}]}]}